{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning: Tic Tac Toe et Alpha Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Jeu de morpion\n",
    "\n",
    "On va dans un premier temps créer la classe du jeu du morpion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Morpion_board():\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros([3, 3]).astype(str)\n",
    "        self.board[self.board == \"0.0\"] = ' '\n",
    "        self.player = 1\n",
    "\n",
    "    def draw_sign(self, move):\n",
    "        if hasattr(move, \"__len__\"):\n",
    "            row, column = move[0], move[1]\n",
    "        else:\n",
    "            row = move//3\n",
    "            column = move%3\n",
    "        if self.board[row, column] != ' ':\n",
    "            return ('Invalid position')\n",
    "        else:\n",
    "            if self.player == 0:\n",
    "                self.board[row, column] = 'O'\n",
    "                self.player = 1\n",
    "            elif self.player == 1:\n",
    "                self.board[row, column] = 'X'\n",
    "                self.player = 0\n",
    "\n",
    "    def check_winner(self):\n",
    "        if self.player == 0:\n",
    "            # on essaye les 6 points qui permettent de gagner (pour le joueur 0)\n",
    "            if self.board[0, 0] == 'O':\n",
    "                if (self.board[0, 0] == 'O' and self.board[0, 1] == 'O' and self.board[\n",
    "                    0, 2] == 'O'):\n",
    "                    return True\n",
    "                elif (self.board[0, 0] == 'O' and self.board[1, 0] == 'O' and self.board[\n",
    "                    2, 0] == 'O'):\n",
    "                    return True\n",
    "                elif (self.board[0, 0] == 'O' and self.board[1, 1] == 'O' and self.board[\n",
    "                    2, 2] == 'O'):\n",
    "                    return True\n",
    "            if self.board[1, 1] == 'O':\n",
    "                if (self.board[0, 1] == 'O' and self.board[1, 1] == 'O' and self.board[\n",
    "                    2, 1] == 'O'):\n",
    "                    return True\n",
    "                elif (self.board[1, 0] == 'O' and self.board[1, 1] == 'O' and self.board[\n",
    "                    1, 2] == 'O'):\n",
    "                    return True\n",
    "                elif (self.board[2, 0] == 'O' and self.board[1, 1] == 'O' and self.board[\n",
    "                    0, 2] == 'O'):\n",
    "                    return True\n",
    "            if self.board[2, 2] == 'O':\n",
    "                if (self.board[0, 2] == 'O' and self.board[1, 2] == 'O' and self.board[\n",
    "                    2, 2] == 'O'):\n",
    "                    return True\n",
    "                elif (self.board[2, 0] == 'O' and self.board[2, 1] == 'O' and self.board[\n",
    "                    2, 2] == 'O'):\n",
    "                    return True\n",
    "\n",
    "        if self.player == 1:\n",
    "            # on essaye les 6 points qui permettent de gagner (pour le joueur 1)\n",
    "            if self.board[0, 0] == 'X':\n",
    "                if (self.board[0, 0] == 'X' and self.board[0, 1] == 'X' and self.board[\n",
    "                    0, 2] == 'X'):\n",
    "                    return True\n",
    "                elif (self.board[0, 0] == 'X' and self.board[1, 0] == 'X' and self.board[\n",
    "                    2, 0] == 'X'):\n",
    "                    return True\n",
    "                elif (self.board[0, 0] == 'X' and self.board[1, 1] == 'X' and self.board[\n",
    "                    2, 2] == 'X'):\n",
    "                    return True\n",
    "            if self.board[1, 1] == 'X':\n",
    "                if (self.board[0, 1] == 'X' and self.board[1, 1] == 'X' and self.board[\n",
    "                    2, 1] == 'X'):\n",
    "                    return True\n",
    "                elif (self.board[1, 0] == 'X' and self.board[1, 1] == 'X' and self.board[\n",
    "                    1, 2] == 'X'):\n",
    "                    return True\n",
    "                elif (self.board[2, 0] == 'X' and self.board[1, 1] == 'X' and self.board[\n",
    "                    0, 2] == 'X'):\n",
    "                    return True\n",
    "            if self.board[2, 2] == 'X':\n",
    "                if (self.board[0, 2] == 'X' and self.board[1, 2] == 'X' and self.board[\n",
    "                    2, 2] == 'X'):\n",
    "                    return True\n",
    "                elif (self.board[2, 0] == 'X' and self.board[2, 1] == 'X' and self.board[\n",
    "                    2, 2] == 'X'):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def actions(self, as_1d=False):# returns all possible moves\n",
    "        acts = []\n",
    "        for row in range(3):\n",
    "            for col in range(3):\n",
    "                if self.board[row, col] == ' ':\n",
    "                    acts.append([row, col])\n",
    "        if as_1d:\n",
    "            for i in range(len(acts)):\n",
    "                acts[i] = acts[i][0]*3+acts[i][1]\n",
    "        return acts\n",
    "\n",
    "    def __repr__(self):\n",
    "        return(f\"\"\"|{self.board[0,0]}|{self.board[0,1]}|{self.board[0,2]}|\n",
    "|{self.board[1,0]}|{self.board[1,1]}|{self.board[1,2]}|\n",
    "|{self.board[2,0]}|{self.board[2,1]}|{self.board[2,2]}|\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant vérifier si ce dernier fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | |\n",
      "| | | |\n",
      "| | | |\n",
      "| | | |\n",
      "| | |X|\n",
      "| | | |\n",
      "[0, 1, 2, 3, 4, 6, 7, 8]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "M = Morpion_board()\n",
    "print(M)\n",
    "M.draw_sign([1,2])\n",
    "print(M)\n",
    "print(M.actions(True))\n",
    "print(M.check_winner())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les 3 éléments d'Alpha Zéro\n",
    "\n",
    "- Un réseau de neurone adapté au board qui renvoie la policy (distribution en proba des différents mouvements possibles) ainsi que du vainqueur \n",
    "- Monte-Carlo Tree Search (MCTS): comme Alpha Chess Zéro, nous avons implémenté la méthode MCTS plutôt que alpha beta.  On utilise la policy pour générer un ensemble de jeu.\n",
    "- Evaluation du réseau de neurone, basé sur une évaluation du gagnant du match à la prochaine itération"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le réseau de neurone \n",
    "\n",
    "On cherche dans un premier temps à avoir un plateau de jeu intelligible pour notre neural network. On va donc transformer notre plateau de jeu en 1 dimension en trois dimension. \n",
    "![baba](img/board_to_neural.png)\n",
    "\n",
    "On va donc d'abord définir la fonction encode board, pour passer d'un board à un array numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_board(board):\n",
    "    board_sate = board.board\n",
    "    encoded = np.zeros([3,3,3]).astype(int)\n",
    "    encoder_dict = {'O':1, 'X':0}\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            if board_sate[row,col] != ' ':\n",
    "                encoded[row, col, encoder_dict[board_sate[row, col]]] = 1\n",
    "    if board.player == 1:\n",
    "        encoded[:,:,2] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste si elle fonctionne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "The player that is about to play is 0\n"
     ]
    }
   ],
   "source": [
    "encoded_board = encode_board(M)\n",
    "print(encoded_board[:,:,0])\n",
    "print(encoded_board[:,:,1])\n",
    "print(encoded_board[:,:,2])\n",
    "print(f\"The player that is about to play is {M.player}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant aussi coder une fonction decode board, pour passer d'un array numpy à un board state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_board(encoded):\n",
    "    decoded = np.zeros([3,3]).astype(str)\n",
    "    decoded[decoded == '0.0'] = ' '\n",
    "    decoder_dict = {0:'O', 1:'X'}\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            for k in range(2):\n",
    "                if encoded[row,col,k] == 1:\n",
    "                    decoded[row,col] = decoder_dict[k]\n",
    "    mboard = Morpion_board()\n",
    "    mboard.board = decoded\n",
    "    mboard.player = encoded[0,0,2]\n",
    "    return mboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | |\n",
      "| | |O|\n",
      "| | | |\n"
     ]
    }
   ],
   "source": [
    "decoded_board = decode_board(encoded_board)\n",
    "print(decoded_board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve bien notre plateau initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture de notre réseau de neurone\n",
    "\n",
    "L'architecture d'Alpha Chess est assez simple. Il y a dans un premier temps un premier network de convolution, suivi d'un deuxième dit résiduel. Avant enfin d'obtenir le réseau d'output qui renvoie la policy, c'est à dire l'ensemble des probabilités de choix de prochain move et la value, c'est à dire le vainqueur. \n",
    "\n",
    "On a représenté le réseau de la façon suivante.\n",
    "![babi](img/neural_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nous inspirant de l'infrastructure réalisée pour AlphaChess Zero nous avons définit en pytorch le réseau suivant.\n",
    "\n",
    "### Convolutionnal block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock,self).__init__()\n",
    "        self.action_size = 3\n",
    "        self.conv1 = nn.Conv2d(3,128,3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = s.view(-1,3,3,3) #reshape the tensor\n",
    "        s = F.relu(self.bn1(self.conv1(s)))\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual block\n",
    "\n",
    "\n",
    "Nous n'avons ici détaillé qu'un seul des 19 residuals blocks utilisé dans le cas d'AlphaChess. Par ailleurs nous avons diminué leur nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inplanes=128, planes=128, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output block\n",
    "\n",
    "Notre block d'output nous donne notre policy $p$ et notre vainqueur $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OutBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(128, 3, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        self.fc1 = nn.Linear(3*3*3, 32)\n",
    "        self.fc2 = nn.Linear(32,1)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.fc = nn.Linear(3*3*32, 9)\n",
    "\n",
    "    def forward(self, s):\n",
    "        v = self.conv(s)\n",
    "        v = self.bn(v)\n",
    "        v = F.relu(v)\n",
    "        v = v.view(-1, 3*3*3)\n",
    "        v = self.fc1(v)\n",
    "        v = F.relu(v)\n",
    "        v = self.fc2(v)\n",
    "        v = F.tanh(v)\n",
    "\n",
    "        p = self.conv1(s)\n",
    "        p = self.bn1(p)\n",
    "        p = F.relu(p)\n",
    "        p = p.view(-1, 3*3*32)\n",
    "        p = self.fc(p)\n",
    "        p = self.logsoftmax(p).exp()\n",
    "        return p, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notre réseau complet\n",
    "\n",
    "On peut donc enfin définir notre réseau complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConnectNet,self).__init__()\n",
    "        self.conv = ConvBlock()\n",
    "        for block in range(5):\n",
    "            setattr(self, 'res%i' % block, ResBlock())\n",
    "        self.outblock = OutBlock()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = self.conv(s)\n",
    "        for block in range(5):\n",
    "            s = getattr(self, 'res%i' % block)(s)\n",
    "\n",
    "        s = self.outblock(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0528, 0.0789, 0.1466, 0.1009, 0.0611, 0.0642, 0.2066, 0.1446, 0.1443]],\n",
       "        grad_fn=<ExpBackward>), tensor([[0.0012]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = ConnectNet()\n",
    "m = torch.from_numpy(encoded_board).float()\n",
    "con(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient bien un array de la taille attendue. La policy est un array de taille $9$ et la valeur de taille $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir la loss\n",
    "\n",
    "On doit définir la loss de notre modèle à deux output. On va définir une loss personnalisée TicLoss comme une somme d'une erreur quadratique moyenne et d'une cross entropy, permettant de rendre compte des deux composantes de notre output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TicLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_value, value, y_policy, policy):\n",
    "        value_error = (value - y_value) ** 2\n",
    "        policy_error = torch.sum((-policy *\n",
    "                                  (1e-8 + y_policy.float()).float().log()), 1)\n",
    "        total_error = (value_error.view(-1).float() + policy_error).mean()\n",
    "        return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche avec Arbre Monte Carlo (Monte Carlo Tree Search)\n",
    "\n",
    "On va maintenant implémenter la partie MCTS.\n",
    "\n",
    "Un jeu est assimilé à un arbre dans lequel la racine est le plateau et les différentes branches tous les états possibles qui peuvent en résulter. Il est facile de calculer tous l'arbre complet du jeu dans le cas du morpion. Ce n'est pas le cas pour un jeu comme les échecs, car le nombre de possibilités augmente de façon exponentielle. \n",
    "\n",
    "On va donc utiliser l'algorithme Monte-Carlo Tree Search pour restreindre notre arbre à ses composantes intéressantes.\n",
    "\n",
    "On se retrouve dans un cas classique de bandit, dans lequel on doit maximiser notre compromis exploration-exploitation. Exploration car on veut observer la plupart des branches, exploitations car on veut se concentrer sur les seules branches intéressantes.\n",
    "\n",
    "Ceci est décrit succinctement dans une équation de l'algorithme MCTS qui définit la limite supérieure de confiance (Upper Confidence Bound) :\n",
    "\n",
    "$$Q + cP(s,a)\\times \\frac{\\sqrt{\\sum_{b}N(s,b)}}{1+N(s,a)}$$\n",
    "\n",
    "- $Q$ est la valeur moyenne de l'action\n",
    "- $c$ est une constante déterminant le niveau d'exploration (fixée à 1)\n",
    "- $P(s=état,a=action)$ est la probabilité préalable de choisir l'action a donnée par la sortie de la policy du neural network\n",
    "- $N(s,a)$ est le nombre de fois que la branche correspondant à l'action $a$ a été visitée\n",
    "- $\\sum_{b}N(s,b)$ le nombre de fois que le parent de $(s,a)$ a été visité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les différentes étapes de l'algorithme MCTS\n",
    "\n",
    "![babi](img/mcts.png)\n",
    "\n",
    "#### Sélection\n",
    "\n",
    "![babi](img/selection.png)\n",
    "\n",
    "On commence d'abord une première étape de sélection. En partant de l'état $s$, on choisit la branche suivante ayant le UCB le plus haut dans un noeud feuille (jamais exploré), ou noeud terminal (fin du jeu). Notre réseau de neurones guide la sélection en donnant la probabilité $P$, qui est pour l'instant aléatoire, notre réseau n'est pas entrainé.\n",
    "\n",
    "#### Expansion et évaluation\n",
    "\n",
    "![babi](img/expansion.png)\n",
    "\n",
    "\n",
    "Dans cette phase, la feuille est développée en évaluant les états associés aux noeuds enfants grâce au réseau et stocker $P$. De manière classique nous avons ajouté un bruit de Dirichler sur le noeud racine pour améliorer notre exploration. \n",
    "\n",
    "\n",
    "#### Backup\n",
    "\n",
    "![babi](img/backup.png)\n",
    "\n",
    "Maintenant, le noeud feuille est évalué par le réseau neuronal pour déterminer sa valeur $v$. Cette valeur est ensuite utilisée pour mettre à jour la moyenne $v$ de tous les nœuds parents au-dessus de lui. Si O gagne ($v = +1$ évalué pour le noeud feuille), alors dans le noeud parent direct de ce noeud feuille, ce sera au tour de O de jouer et nous mettrons à jour $v = +1$ pour ce noeud parent, puis nous mettrons à jour $v = -1$ pour tous les autres noeuds parents où X doit jouer pour indiquer que cette action est mauvaise pour X. Enfin, nous mettrons à jour $v = 0$ en cas d'égalité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe obtenue pour coder cet algorithme est très grande, mais on retrouve les différentes étapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import copy\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "class UCTNode():\n",
    "    def __init__(self, game, move, parent=None):\n",
    "        self.game = game  # state s\n",
    "        self.move = move  # action index\n",
    "        self.is_expanded = False\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.child_priors = np.zeros([9], dtype=np.float32)\n",
    "        self.child_total_value = np.zeros([9], dtype=np.float32)\n",
    "        self.child_number_visits = np.zeros([9], dtype=np.float32)\n",
    "        self.action_idxes = []\n",
    "\n",
    "    @property\n",
    "    def number_visits(self):\n",
    "        return self.parent.child_number_visits[self.move]\n",
    "\n",
    "    @number_visits.setter\n",
    "    def number_visits(self, value):\n",
    "        self.parent.child_number_visits[self.move] = value\n",
    "\n",
    "    @property\n",
    "    def total_value(self):\n",
    "        return self.parent.child_total_value[self.move]\n",
    "\n",
    "    @total_value.setter\n",
    "    def total_value(self, value):\n",
    "        self.parent.child_total_value[self.move] = value\n",
    "\n",
    "    def child_Q(self):\n",
    "        return self.child_total_value / (1 + self.child_number_visits)\n",
    "\n",
    "    def child_U(self):\n",
    "        return math.sqrt(self.number_visits) * (\n",
    "                abs(self.child_priors) / (1 + self.child_number_visits))\n",
    "\n",
    "    def best_child(self):\n",
    "        if self.action_idxes != []:\n",
    "            bestmove = self.child_Q() + self.child_U()\n",
    "            bestmove = self.action_idxes[np.argmax(bestmove[self.action_idxes])]\n",
    "        else:\n",
    "            bestmove = np.argmax(self.child_Q() + self.child_U())\n",
    "        return bestmove\n",
    "\n",
    "    def select_leaf(self):\n",
    "        current = self\n",
    "        while current.is_expanded:\n",
    "            best_move = current.best_child()\n",
    "            current = current.maybe_add_child(best_move)\n",
    "        return current\n",
    "\n",
    "    def add_dirichlet_noise(self, action_idxs, child_priors):\n",
    "        valid_child_priors = child_priors[action_idxs]  # select only legal moves entries in child_priors array\n",
    "        valid_child_priors = 0.75 * valid_child_priors + 0.25 * np.random.dirichlet(np.zeros([len(valid_child_priors)],dtype=np.float32) + 192)\n",
    "        child_priors[action_idxs] = valid_child_priors\n",
    "        return child_priors\n",
    "\n",
    "    def expand(self, child_priors):\n",
    "        self.is_expanded = True\n",
    "        action_idxs = self.game.actions(as_1d=True);\n",
    "        c_p = child_priors\n",
    "        if action_idxs == []:\n",
    "            self.is_expanded = False\n",
    "        self.action_idxes = action_idxs\n",
    "        c_p[[i for i in range(len(child_priors)) if i not in action_idxs]] = 0.000000000  # mask all illegal actions\n",
    "        if self.parent.parent == None:  # add dirichlet noise to child_priors in root node\n",
    "            c_p = self.add_dirichlet_noise(action_idxs, c_p)\n",
    "        self.child_priors = c_p\n",
    "\n",
    "    def decode_n_move_pieces(self, board, move):\n",
    "        board.draw_sign(move)\n",
    "        return board\n",
    "\n",
    "    def maybe_add_child(self, move):\n",
    "        if move not in self.children:\n",
    "            copy_board = copy.deepcopy(self.game)  # make copy of board\n",
    "            copy_board = self.decode_n_move_pieces(copy_board, move)\n",
    "            self.children[move] = UCTNode(\n",
    "                copy_board, move, parent=self)\n",
    "        return self.children[move]\n",
    "\n",
    "    def backup(self, value_estimate: float):\n",
    "        current = self\n",
    "        while current.parent is not None:\n",
    "            current.number_visits += 1\n",
    "            if current.game.player:  # same as current.parent.game.player = 0\n",
    "                current.total_value += (1 * value_estimate)  # value estimate +1 = O wins\n",
    "            elif not current.game.player:  # same as current.parent.game.player = 1\n",
    "                current.total_value += (-1 * value_estimate)\n",
    "            current = current.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyNode(object):\n",
    "    def __init__(self):\n",
    "        self.parent = None\n",
    "        self.child_total_value = collections.defaultdict(float)\n",
    "        self.child_number_visits = collections.defaultdict(float)\n",
    "        \n",
    "def do_decode_n_move_pieces(board,move):\n",
    "    if type(move)==int:\n",
    "        row = move//3\n",
    "        col = move%3\n",
    "        board.draw_sign([row, col])\n",
    "    else:\n",
    "        board.draw_sign(move)\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCT_search(game_state, num_reads,net,temp):\n",
    "    root = UCTNode(game_state, move=None, parent=DummyNode())\n",
    "    for i in range(num_reads):\n",
    "        leaf = root.select_leaf()\n",
    "        encoded_s = encode_board(leaf.game)\n",
    "        encoded_s = torch.from_numpy(encoded_s).float()\n",
    "        child_priors, value_estimate = net(encoded_s)\n",
    "        child_priors = child_priors.detach().cpu().numpy().reshape(-1)\n",
    "        value_estimate = value_estimate.item()\n",
    "        if leaf.game.check_winner() == True or leaf.game.actions() == []: # if somebody won or draw\n",
    "            leaf.backup(value_estimate); continue\n",
    "        leaf.expand(child_priors) # need to make sure valid moves\n",
    "        leaf.backup(value_estimate)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Le processus de sélection, d'expansion et d'évaluation et de sauvegarde représente une simulation pour chaque nœud racine de l'algorithme des SCTM. Dans AlphaChess Zero, 800 simulations de ce type sont effectuées. \n",
    "\n",
    "Pour notre implémentation du morpion, nous n'en avons effectué que 50 car c'est un jeu beaucoup plus simple. Après avoir exécuté 100 simulations pour ce nœud racine, nous formulerons la politique $p$ pour le nœud racine qui est définie comme étant proportionnelle au nombre de visites de ses nœuds enfants directs. Cette politique $p$ sera ensuite utilisée pour sélectionner le prochain coup vers le prochain état du plateau, et cet état du plateau sera alors traité comme le nœud racine pour les prochaines simulations de SCTM et ainsi de suite jusqu'à ce que le jeu se termine lorsque quelqu'un gagne ou fait match nul. \n",
    "\n",
    "Toute la procédure dans laquelle on exécute des simulations de SCTM pour chaque nœud racine au fur et à mesure des déplacements jusqu'à la fin du jeu est appelée <b>\"self-play MCTS\"</b>.\n",
    "    \n",
    "On va dans un premier temps créer des fonctions utilitaires pour enregistrer les différentes itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections\n",
    "\n",
    "def save_as_pickle(filename, data):\n",
    "    complete_name = os.path.join(\"./datasets/\", filename)\n",
    "    with open(complete_name, 'wb') as output:\n",
    "        pickle.dump(data, output)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    complete_name = os.path.join(\"./datasets/\", filename)\n",
    "    with open(complete_name, 'rb') as pkl_file:\n",
    "        data = pickle.load(pkl_file)\n",
    "    return data\n",
    "\n",
    "def get_policy(root, temp=1):\n",
    "    policy = ((root.child_number_visits) ** (1 / temp)) / sum(root.child_number_visits ** (1 / temp))\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCTS_self_play(connectnet,num_games, start_idx, cpu, args, iteration):\n",
    "    for idxx in range(0,num_games):\n",
    "        current_board = Morpion_board()\n",
    "        checkmate = False\n",
    "        dataset = [] # to store state, policy, value for neural network training\n",
    "        states = []; value = 0; move_count = 0\n",
    "        # play game against self\n",
    "        while checkmate == False and current_board.actions() != []:\n",
    "            # set temperature parameter\n",
    "            if move_count < 11:\n",
    "                t = 1\n",
    "            else:\n",
    "                t = 0.1\n",
    "            states.append(copy.deepcopy(current_board.board))\n",
    "            board_state = copy.deepcopy(encode_board(current_board))\n",
    "            root = UCT_search(current_board,50,connectnet,t) # run 777 MCTS simulations\n",
    "            policy = get_policy(root, t) # formulate policy based on results of MCTS simulations\n",
    "            current_board = do_decode_n_move_pieces(current_board,\\\n",
    "                                                    np.random.choice(np.array([0,1,2,3,4,5,6,7,8]), \\\n",
    "                                                                     p = policy)) # decode action and make a move\n",
    "            dataset.append([board_state,policy]) # stores s, p\n",
    "            if current_board.check_winner() == True: # if somebody won, update v\n",
    "                if current_board.player == 0: # X wins\n",
    "                    value = -1\n",
    "                elif current_board.player == 1: # O wins\n",
    "                    value = 1\n",
    "                checkmate = True\n",
    "            move_count += 1\n",
    "        dataset_p = []\n",
    "        # update v for all (s, p) except for starting board state s\n",
    "        for idx,data in enumerate(dataset):\n",
    "            s,p = data\n",
    "            if idx == 0:\n",
    "                dataset_p.append([s,p,0])\n",
    "            else:\n",
    "                dataset_p.append([s,p,value])\n",
    "        del dataset\n",
    "        # save (s,p,v) datasets for neural net training\n",
    "        if not os.path.isdir(\"./datasets/iter_%d/\" % iteration):\n",
    "            os.mkdir(\"./datasets/iter_%d/\" % iteration)\n",
    "        save_as_pickle(\"iter_%d/\" % iteration + \"dataset_cpu%i_%i_%s\" % (cpu,idxx, datetime.datetime.today().strftime(\"%Y-%m-%d\")),dataset_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logzero import logger\n",
    "\n",
    "def run_MCTS(args, start_idx=0, iteration=0):\n",
    "    net_to_play = f\"{args.neural_net_name}_iter{iteration}.pth.tar\"\n",
    "    net = ConnectNet()\n",
    "    cuda = torch.cuda.is_available()\n",
    "    \n",
    "    \n",
    "    logger.info('Preparing model for MCTS...')\n",
    "    net.eval()\n",
    "\n",
    "    current_net_filename = os.path.join('./model_data/', net_to_play)\n",
    "\n",
    "    if os.path.isfile(current_net_filename):\n",
    "        checkpoint = torch.load(current_net_filename)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "        logger.info('Loaded %s model.' % current_net_filename)\n",
    "    else:\n",
    "        torch.save({'state_dict': net.state_dict()}, os.path.join(\"./model_data/\",net_to_play))\n",
    "        logger.info('Initialized model.')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        MCTS_self_play(net, args.num_games_per_MCTS_process, start_idx, 0, args, iteration)\n",
    "    logger.info('Finished MCTS!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def train(net, dataset, optimizer, scheduler, start_epoch, cpu, args, iteration):\n",
    "    torch.manual_seed(cpu)\n",
    "    cuda = torch.cuda.is_available()\n",
    "    net.train()\n",
    "    criterion = TicLoss()\n",
    "    train_set = board_data(dataset)\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    losses_per_epoch = load_results(iteration + 1)\n",
    "\n",
    "    logger.info(\"Starting training process...\")\n",
    "    print(f\"the length of the train loader is{len(train_loader)}\")\n",
    "    update_size = max(len(train_loader)//10,1)\n",
    "    print(\"Update step size: %d\" % update_size)\n",
    "    for epoch in range(start_epoch, args.num_epochs):\n",
    "        total_loss = 0.0\n",
    "        losses_per_batch = []\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            state, policy, value = data\n",
    "            state = state.float()\n",
    "            policy = policy.float()\n",
    "            value = value.float()\n",
    "            if cuda:\n",
    "                state = state.cuda()\n",
    "                policy = policy.cuda()\n",
    "                value = value.cuda()\n",
    "            policy_pred, value_pred = net(state)\n",
    "            loss = criterion(value_pred[:,0], value, policy_pred, policy)\n",
    "            loss = loss/args.gradient_acc_steps\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(net.parameters(), args.max_norm)\n",
    "            if (epoch % args.gradient_acc_steps) == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            print(total_loss)\n",
    "            print(\"Update step size: %d\" % update_size)\n",
    "            if i % update_size == (update_size - 1):\n",
    "                losses_per_batch.append(args.gradient_acc_steps*total_loss/update_size)\n",
    "                print('[Iteration %d] Process ID: %d [Epoch: %d, %5d/ %d points] total loss per batch: %.3f' %\n",
    "                      (iteration, os.getpid(), epoch + 1, (i + 1) * args.batch_size, len(train_set),\n",
    "                       losses_per_batch[-1]))\n",
    "                print(\"Policy (actual, predicted):\", policy[0].argmax().item(), policy_pred[0].argmax().item())\n",
    "                print(\"Policy data:\", policy[0]);\n",
    "                print(\"Policy pred:\", policy_pred[0])\n",
    "                print(\"Value (actual, predicted):\", value[0].item(), value_pred[0, 0].item())\n",
    "                print(\" \")\n",
    "                total_loss = 0.0\n",
    "\n",
    "        scheduler.step()\n",
    "        if len(losses_per_batch) >=1:\n",
    "            losses_per_epoch.append(sum(losses_per_batch)/len(losses_per_batch))\n",
    "        if (epoch % 2) == 0:\n",
    "            save_as_pickle(\"losses_per_epoch_iter%d.pkl\" % (iteration + 1), losses_per_epoch)\n",
    "            torch.save({\n",
    "                'epoch' : epoch +1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'scheduler' : scheduler.state_dict(),\n",
    "                'loss':loss\n",
    "            }, os.path.join(f'./model_data/',f\"{args.neural_net_name}_iter{(iteration+1)}.pth.tar\"))\n",
    "\n",
    "    logger.info('Finished Training!')\n",
    "    fig = px.scatter([e for e in range(start_epoch, (len(losses_per_epoch) + start_epoch))], loss)\n",
    "    fig.update_layout(\n",
    "        title=\"loss vs Epoch\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Loss per batch\",\n",
    "        font=dict(\n",
    "            family=\"Courier New, monospace\",\n",
    "            size=18,\n",
    "            color=\"#7f7f7f\"\n",
    "            )\n",
    "        )\n",
    "    fig.write_image(os.path.join(\"./model_data/\", \"Loss_vs_Epoch_iter%d_%s.png\" % (\n",
    "    (iteration + 1), datetime.datetime.today().strftime(\"%Y-%m-%d\"))))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def train_connectnet(args, iteration, new_optim_state):\n",
    "    logger.info(\"Loading training data...\")\n",
    "    data_path = \"./datasets/iter_%d/\" % iteration\n",
    "    datasets = []\n",
    "    for idx, file in enumerate(os.listdir(data_path)):\n",
    "        filename = os.path.join(data_path, file)\n",
    "        with open(filename, 'rb') as fo:\n",
    "            datasets.extend(pickle.load(fo, encoding='bytes'))\n",
    "    datasets = np.array(datasets)\n",
    "    logger.info(\"Loaded data from %s.\" % data_path)\n",
    "    net = ConnectNet()\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args.lr, betas=(0.8, 0.999))\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,100,150, 200, 250, 300, 400], gamma=0.77)\n",
    "    start_epoch = load_state(net, optimizer, scheduler, args, iteration, new_optim_state)\n",
    "\n",
    "    train(net, datasets, optimizer, scheduler, start_epoch, 0, args, iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arena():\n",
    "    def __init__(self, current_cnet, best_cnet):\n",
    "        self.current = current_cnet\n",
    "        self.best = best_cnet\n",
    "\n",
    "\n",
    "    def play_round(self):\n",
    "        logger.info(\"starting game round...\")\n",
    "        if np.random.uniform(0,1) <= 0.5:\n",
    "            white = self.current\n",
    "            black = self.best\n",
    "            w = \"current\"\n",
    "            b = \"best\"\n",
    "        else:\n",
    "            white = self.best\n",
    "            black = self.current\n",
    "            w = 'best'\n",
    "            b= 'current'\n",
    "        current_board = Morpion_board()\n",
    "        checkmate = False\n",
    "        dataset = []\n",
    "        value = 0\n",
    "        t = 0.1\n",
    "        while checkmate == False and current_board.actions() != []:\n",
    "            dataset.append(copy.deepcopy(encode_board(current_board)))\n",
    "            if current_board.player == 0:\n",
    "                root = UCT_search(current_board, 50, white, t)\n",
    "                policy = get_policy(root, t)\n",
    "                print(\"Policy : \", policy, \"white = %s\" %(str(w)))\n",
    "            elif current_board.player == 1:\n",
    "                root = UCT_search(current_board, 50, black, t)\n",
    "                policy = get_policy(root, t)\n",
    "                print(\"Policy : \", policy, \"black = %s\" % (str(b)))\n",
    "            current_board = do_decode_n_move_pieces(current_board, np.random.choice(np.array([0,1,2,3,4,5,6,7,8]), p = policy))\n",
    "            if current_board.check_winner() == True:\n",
    "                if current_board.player == 0:\n",
    "                    value = -1\n",
    "                elif current_board.player == 1:\n",
    "                    value = 1\n",
    "                checkmate = True\n",
    "        dataset.append(encode_board(current_board))\n",
    "        if value == -1:\n",
    "            dataset.append(f\"{b} as circle wins\")\n",
    "            return b, dataset\n",
    "        elif value == 1:\n",
    "            dataset.append(f\"{w} as cross wins\")\n",
    "            return w, dataset\n",
    "        else:\n",
    "            dataset.append('Nobody wins')\n",
    "            return None, dataset\n",
    "    def evaluate(self, num_games, cpu):\n",
    "        current_wins = 0\n",
    "        logger.info(\"[CPU %d]: Starting games...\" % cpu)\n",
    "        for i in range(num_games):\n",
    "            with torch.no_grad():\n",
    "                winner, dataset = self.play_round()\n",
    "                print(\"%s wins!\" % winner)\n",
    "            if winner == 'current':\n",
    "                current_wins += 1\n",
    "            save_as_pickle(\"evaluate_net_dataset_cpu%i_%i_%s_%s\" % (cpu,i,datetime.datetime.today().strftime(\"%Y-%m-%d\"),\\\n",
    "                                                                     str(winner)),dataset)\n",
    "        print(\"Current_net wins ratio %.5f\" % (current_wins/num_games))\n",
    "        save_as_pickle(\"wins_cpu_%i\"  % (cpu),\\\n",
    "                                             {\"best_win_ratio\": current_wins/num_games, \"num_games\":num_games})\n",
    "        logger.info(\"[CPU %d]: Finished arena games!\" % cpu)\n",
    "\n",
    "def fork_process(arena_obj, num_games, cpu):\n",
    "    arena_obj.evaluate(num_games, cpu)\n",
    "\n",
    "def evaluate_nets(args, iteration_1, iteration_2):\n",
    "    logger.info('Loading nets...')\n",
    "    current_net = \"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration_2)\n",
    "    best_net = \"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration_1)\n",
    "    current_net_filename = os.path.join(\"./model_data/\", current_net)\n",
    "    best_net_filename = os.path.join(\"./model_data/\", best_net)\n",
    "\n",
    "    logger.info('Current net: %s' % current_net)\n",
    "    logger.info(\"Previous (Best net: %s\" %best_net)\n",
    "\n",
    "    current_cnet = ConnectNet()\n",
    "    best_cnet = ConnectNet()\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        current_cnet.cuda()\n",
    "        best_cnet.cuda()\n",
    "\n",
    "    if not os.path.isdir(\"./evaluator_data/\"):\n",
    "        os.mkdir(\"evaluator_data\")\n",
    "\n",
    "    if args.MCTS_num_processes > 1:\n",
    "        mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "        current_cnet.share_memory()\n",
    "        best_cnet.share_memory()\n",
    "        current_cnet.eval()\n",
    "        best_cnet.eval()\n",
    "\n",
    "        checkpoint = torch.load(current_net_filename)\n",
    "        current_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "        checkpoint = torch.load(best_net_filename)\n",
    "        best_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        processes = []\n",
    "        if args.MCTS_num_processes > mp.cpu_count():\n",
    "            num_processes = mp.cpu_count()\n",
    "            logger.info(\"Required number of processes exceed number of CPUs! Setting MCTS_num_processes to %d\" % num_processes)\n",
    "        else:\n",
    "            num_processes = args.MCTS_num_processes\n",
    "        logger.info(\"Spawning %d processes...\" % num_processes)\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_processes):\n",
    "                p = mp.Process(target=fork_process,args=(arena(current_cnet,best_cnet), args.num_evaluator_games, i))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "\n",
    "        wins_ratio = 0.0\n",
    "        for i in range(num_processes):\n",
    "            stats = load_pickle(\"wins_cpu_%i\" % (i))\n",
    "            wins_ratio += stats['best_win_ratio']\n",
    "        wins_ratio = wins_ratio/num_processes\n",
    "        if wins_ratio >= 0.55:\n",
    "            return iteration_2\n",
    "        else :\n",
    "            return iteration_1\n",
    "\n",
    "    elif args.MCTS_num_processes == 1:\n",
    "        current_cnet.eval()\n",
    "        best_cnet.eval()\n",
    "        checkpoint = torch.load(current_net_filename)\n",
    "        current_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "        checkpoint = torch.load(best_net_filename)\n",
    "        best_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "        arena1 = arena(current_cnet=current_cnet, best_cnet=best_cnet)\n",
    "        arena1.evaluate(num_games=args.num_evaluator_games, cpu=0)\n",
    "\n",
    "        stats = load_pickle('wins_cpu_%i' % (0))\n",
    "        if stats[\"best_win_ratio\"] >= 0.55:\n",
    "            return iteration_2\n",
    "        else:\n",
    "            return iteration_1\n",
    "        \n",
    "def load_state(net, optimizer, scheduler, args, iteration, new_optim_state=True):\n",
    "    base_path = \"./model-data/\"\n",
    "    checkpoint_path = os.path.join(base_path, \"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration))\n",
    "    start_epoch, checkpoint = 0, None\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    if checkpoint != None:\n",
    "        if (len(checkpoint) == 1) or (new_optim_state == True):\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            logger.info(\"Loaded checkpoint model %s.\" % checkpoint_path)\n",
    "        else:\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            logger.info('Loaded checkpoint model %s, and optimizer, scheduler.' % checkpoint_path)\n",
    "    return start_epoch\n",
    "\n",
    "def load_results(iteration):\n",
    "    losses_path = \"./model_data/losses_per_epoch_iter%d.pkl\" % iteration\n",
    "    if os.path.exists(losses_path):\n",
    "        losses_per_epoch = load_pickle(\"losses_per_epoch_iter%d.pkl\" % iteration)\n",
    "        logger.info(\"Loaded results buffer\")\n",
    "    else:\n",
    "        losses_per_epoch = []\n",
    "    return losses_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class board_data(Dataset):\n",
    "    def __init__(self, dataset):  # dataset = np.array of (s, p, v)\n",
    "        self.X = dataset[:, 0]\n",
    "        self.y_p, self.y_v = dataset[:, 1], dataset[:, 2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return np.int64(self.X[idx]), self.y_p[idx], self.y_v[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:37 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:37:37 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter5.pth.tar model.\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning:\n",
      "\n",
      "nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "\n",
      "[I 200412 12:37:40 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:37:40 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:37:40 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_5/.\n",
      "[I 200412 12:37:40 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "3.0447428226470947\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 1,    32/ 23 points] total loss per batch: 3.045\n",
      "Policy (actual, predicted): 8 1\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1429, 0.0000, 0.2041, 0.1224, 0.1429, 0.0000, 0.3878])\n",
      "Policy pred: tensor([0.1027, 0.1934, 0.1231, 0.1487, 0.1125, 0.0408, 0.1054, 0.0339, 0.1396],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 -0.13240526616573334\n",
      " \n",
      "2.5238564014434814\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 2,    32/ 23 points] total loss per batch: 2.524\n",
      "Policy (actual, predicted): 7 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0204, 0.0204, 0.1429, 0.0000, 0.8163, 0.0000])\n",
      "Policy pred: tensor([0.0416, 0.1014, 0.2810, 0.0338, 0.0387, 0.2393, 0.0401, 0.1623, 0.0618],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.17221017181873322\n",
      " \n",
      "2.525850296020508\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 3,    32/ 23 points] total loss per batch: 2.526\n",
      "Policy (actual, predicted): 3 1\n",
      "Policy data: tensor([0.0204, 0.0408, 0.2041, 0.4490, 0.0612, 0.1020, 0.1224, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0647, 0.2795, 0.0551, 0.1392, 0.0811, 0.0450, 0.0952, 0.0984, 0.1418],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.44846007227897644\n",
      " \n",
      "2.0548105239868164\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 4,    32/ 23 points] total loss per batch: 2.055\n",
      "Policy (actual, predicted): 5 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1633, 0.0000, 0.0000, 0.3673, 0.2041, 0.0000, 0.2653])\n",
      "Policy pred: tensor([0.0631, 0.0575, 0.1918, 0.1011, 0.0704, 0.1462, 0.1052, 0.1183, 0.1465],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.6264825463294983\n",
      " \n",
      "1.9258133172988892\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 5,    32/ 23 points] total loss per batch: 1.926\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0000, 0.0204, 0.6327, 0.0204, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0369, 0.1800, 0.2469, 0.0265, 0.0449, 0.2509, 0.0138, 0.1361, 0.0639],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.2332083284854889\n",
      " \n",
      "1.7310603857040405\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 6,    32/ 23 points] total loss per batch: 1.731\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.1633, 0.8367, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0478, 0.6884, 0.0828, 0.0093, 0.1007, 0.0322, 0.0139, 0.0086, 0.0164],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.1133299469947815\n",
      " \n",
      "1.6011533737182617\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 7,    32/ 23 points] total loss per batch: 1.601\n",
      "Policy (actual, predicted): 1 5\n",
      "Policy data: tensor([0.0408, 0.3878, 0.1020, 0.0612, 0.0612, 0.2245, 0.0204, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.0489, 0.0888, 0.1376, 0.0938, 0.0577, 0.2444, 0.0451, 0.1273, 0.1565],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.8139623403549194\n",
      " \n",
      "1.5300979614257812\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 8,    32/ 23 points] total loss per batch: 1.530\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0612, 0.0612, 0.5714, 0.0000, 0.0816, 0.0000, 0.2245, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1120, 0.1773, 0.3272, 0.0705, 0.1592, 0.0459, 0.0783, 0.0118, 0.0177],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.024133488535881042\n",
      " \n",
      "1.4560075998306274\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 9,    32/ 23 points] total loss per batch: 1.456\n",
      "Policy (actual, predicted): 5 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2653, 0.0000, 0.0000, 0.4082, 0.3265, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.5066e-02, 2.4033e-02, 4.2478e-01, 6.5625e-02, 1.2826e-02, 2.1103e-01,\n",
      "        1.8714e-01, 2.9956e-04, 5.9203e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9976639747619629\n",
      " \n",
      "1.3985285758972168\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 10,    32/ 23 points] total loss per batch: 1.399\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0408, 0.0000, 0.1224, 0.0612, 0.0000, 0.1429, 0.0204, 0.0000, 0.6122])\n",
      "Policy pred: tensor([0.0315, 0.0760, 0.1557, 0.0607, 0.0454, 0.2115, 0.0725, 0.0035, 0.3433],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9959788918495178\n",
      " \n",
      "1.3850393295288086\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 11,    32/ 23 points] total loss per batch: 1.385\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0816, 0.3673, 0.0000, 0.0000, 0.5510, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.2399e-01, 3.4755e-01, 8.8457e-02, 6.6311e-03, 3.6496e-01, 1.0095e-02,\n",
      "        5.7200e-02, 1.0493e-04, 1.0081e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.06520183384418488\n",
      " \n",
      "1.356229305267334\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 12,    32/ 23 points] total loss per batch: 1.356\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0000, 0.0204, 0.6327, 0.0204, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0961, 0.0223, 0.4058, 0.0130, 0.0189, 0.4186, 0.0115, 0.0097, 0.0040],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.07731994241476059\n",
      " \n",
      "1.327646255493164\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 13,    32/ 23 points] total loss per batch: 1.328\n",
      "Policy (actual, predicted): 4 1\n",
      "Policy data: tensor([0.0816, 0.3673, 0.0000, 0.0000, 0.5510, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([5.8165e-02, 7.4997e-01, 1.0035e-02, 8.3936e-04, 1.6492e-01, 2.0579e-03,\n",
      "        1.3882e-02, 8.4989e-06, 1.2233e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.002864804584532976\n",
      " \n",
      "1.3091415166854858\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 14,    32/ 23 points] total loss per batch: 1.309\n",
      "Policy (actual, predicted): 5 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2653, 0.0000, 0.0000, 0.4082, 0.3265, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.3369e-02, 2.3935e-03, 4.1651e-01, 1.6887e-02, 1.7772e-03, 2.5090e-01,\n",
      "        2.9677e-01, 5.7507e-06, 1.3889e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9997153282165527\n",
      " \n",
      "1.2801623344421387\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 15,    32/ 23 points] total loss per batch: 1.280\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0408, 0.3878, 0.1020, 0.0612, 0.0612, 0.2245, 0.0204, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.0451, 0.3738, 0.0907, 0.0728, 0.0893, 0.1816, 0.0169, 0.0093, 0.1205],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9862442016601562\n",
      " \n",
      "1.2602423429489136\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 16,    32/ 23 points] total loss per batch: 1.260\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.1633, 0.8367, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([5.9249e-02, 9.3161e-01, 7.0047e-04, 1.0058e-06, 8.0780e-03, 5.4384e-05,\n",
      "        3.0741e-04, 1.0891e-08, 1.2352e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.04607418179512024\n",
      " \n",
      "1.2365208864212036\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 17,    32/ 23 points] total loss per batch: 1.237\n",
      "Policy (actual, predicted): 3 2\n",
      "Policy data: tensor([0.0816, 0.0000, 0.3265, 0.4082, 0.0000, 0.1020, 0.0816, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1003, 0.0071, 0.2892, 0.2501, 0.0077, 0.1827, 0.1271, 0.0012, 0.0344],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9942687153816223\n",
      " \n",
      "1.2282177209854126\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 18,    32/ 23 points] total loss per batch: 1.228\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.6939, 0.0000, 0.0000, 0.0000, 0.3061, 0.0000, 0.0000])\n",
      "Policy pred: tensor([6.7255e-03, 6.7668e-05, 6.0435e-01, 3.7714e-04, 2.4169e-05, 7.0355e-02,\n",
      "        3.1790e-01, 1.9973e-08, 1.9996e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9999820590019226\n",
      " \n",
      "1.2248517274856567\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 19,    32/ 23 points] total loss per batch: 1.225\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0204, 0.0408, 0.2041, 0.4490, 0.0612, 0.1020, 0.1224, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0199, 0.0258, 0.2117, 0.4645, 0.0520, 0.0527, 0.1508, 0.0123, 0.0103],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0022539757192134857\n",
      " \n",
      "1.2051286697387695\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 20,    32/ 23 points] total loss per batch: 1.205\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0816, 0.0000, 0.3265, 0.4082, 0.0000, 0.1020, 0.0816, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0961, 0.0037, 0.2880, 0.3446, 0.0046, 0.1419, 0.0989, 0.0010, 0.0211],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9924002885818481\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2102251052856445\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 21,    32/ 23 points] total loss per batch: 1.210\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0816, 0.0000, 0.1429, 0.2041, 0.0612, 0.0816, 0.1633, 0.0000, 0.2653])\n",
      "Policy pred: tensor([0.0444, 0.0139, 0.1712, 0.1997, 0.0249, 0.0949, 0.1238, 0.0023, 0.3249],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9968484044075012\n",
      " \n",
      "1.1999138593673706\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 22,    32/ 23 points] total loss per batch: 1.200\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0408, 0.3878, 0.1020, 0.0612, 0.0612, 0.2245, 0.0204, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.0424, 0.3552, 0.0957, 0.0564, 0.0599, 0.2982, 0.0124, 0.0029, 0.0769],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9890971779823303\n",
      " \n",
      "1.1941745281219482\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 23,    32/ 23 points] total loss per batch: 1.194\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0204, 0.0204, 0.1429, 0.0000, 0.8163, 0.0000])\n",
      "Policy pred: tensor([0.0117, 0.0078, 0.0064, 0.0080, 0.0134, 0.1201, 0.0046, 0.8206, 0.0073],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.009552082046866417\n",
      " \n",
      "1.1940027475357056\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 24,    32/ 23 points] total loss per batch: 1.194\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0204, 0.0204, 0.1429, 0.0000, 0.8163, 0.0000])\n",
      "Policy pred: tensor([0.0108, 0.0068, 0.0055, 0.0082, 0.0143, 0.1241, 0.0047, 0.8174, 0.0082],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.009131583385169506\n",
      " \n",
      "1.1924469470977783\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 25,    32/ 23 points] total loss per batch: 1.192\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0000, 0.0204, 0.6327, 0.0204, 0.0000, 0.0000])\n",
      "Policy pred: tensor([6.5231e-02, 2.7028e-03, 2.2634e-01, 2.0629e-03, 2.1296e-02, 6.6869e-01,\n",
      "        1.3006e-02, 4.6896e-04, 2.0242e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.007160899695008993\n",
      " \n",
      "1.1912659406661987\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 26,    32/ 23 points] total loss per batch: 1.191\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.1224, 0.0000, 0.3673, 0.0000, 0.0000, 0.4490, 0.0612, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.1887e-01, 1.6313e-04, 2.6684e-01, 3.5147e-03, 2.6570e-04, 5.2844e-01,\n",
      "        7.7755e-02, 1.3639e-06, 4.1478e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9999082088470459\n",
      " \n",
      "1.1877670288085938\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 27,    32/ 23 points] total loss per batch: 1.188\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.4490, 0.0816, 0.1020, 0.0612, 0.0612, 0.1224, 0.0000, 0.1224])\n",
      "Policy pred: tensor([0.0068, 0.4424, 0.0942, 0.0918, 0.0769, 0.0451, 0.1165, 0.0016, 0.1247],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.983370304107666\n",
      " \n",
      "1.1872186660766602\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 28,    32/ 23 points] total loss per batch: 1.187\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0408, 0.3878, 0.1020, 0.0612, 0.0612, 0.2245, 0.0204, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.0328, 0.4561, 0.0733, 0.0490, 0.0467, 0.2567, 0.0141, 0.0019, 0.0694],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9910498261451721\n",
      " \n",
      "1.1805576086044312\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 29,    32/ 23 points] total loss per batch: 1.181\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.4490, 0.0816, 0.1020, 0.0612, 0.0612, 0.1224, 0.0000, 0.1224])\n",
      "Policy pred: tensor([0.0070, 0.3661, 0.0958, 0.1272, 0.0759, 0.0531, 0.1304, 0.0018, 0.1427],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9864873886108398\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:43 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1772958040237427\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 30,    32/ 23 points] total loss per batch: 1.177\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1429, 0.2653, 0.1837, 0.0612, 0.0612, 0.0000, 0.2857])\n",
      "Policy pred: tensor([3.4993e-03, 5.8596e-03, 1.5484e-01, 2.6661e-01, 1.7885e-01, 6.2104e-02,\n",
      "        5.9502e-02, 1.2115e-04, 2.6860e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9995348453521729\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:46 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:37:46 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter6.pth.tar\n",
      "[I 200412 12:37:46 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter5.pth.tar\n",
      "[I 200412 12:37:46 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:37:46 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [5.3697832e-08 5.3697832e-08 5.4986580e-05 9.5355142e-07 9.0937750e-13\n",
      " 8.8806401e-06 5.4986580e-05 8.8806401e-06 9.9987113e-01] black = current\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 9.536743e-17 9.536743e-17\n",
      " 2.693894e-08 0.000000e+00 1.000000e+00 0.000000e+00] white = best\n",
      "Policy :  [9.2540142e-10 5.3363312e-08 3.1510503e-03 9.9364221e-01 9.4761106e-07\n",
      " 3.1510503e-03 5.4644031e-05 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [1.2792907e-09 1.2200267e-15 3.1644347e-05 0.0000000e+00 1.2200267e-15\n",
      " 9.9996829e-01 1.2200267e-15 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [2.4923918e-09 1.4035534e-10 9.9999171e-01 0.0000000e+00 2.4923918e-09\n",
      " 0.0000000e+00 8.2878432e-06 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [1.7630699e-01 5.3020562e-05 0.0000000e+00 0.0000000e+00 5.0564348e-11\n",
      " 0.0000000e+00 8.2363993e-01 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.         0.47828844 0.         0.         0.2608558  0.\n",
      " 0.2608558  0.         0.        ] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7978726e-09\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:47 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [3.6250961e-16 3.7120984e-13 3.6250961e-16 3.7120984e-13 3.6250961e-16\n",
      " 3.6250961e-16 3.6250961e-16 1.0000000e+00 3.5401331e-09] black = best\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 2.7978726e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [3.5653745e-06 0.0000000e+00 9.6047414e-04 1.1855787e-02 2.0077877e-07\n",
      " 3.5653745e-06 3.6509435e-03 0.0000000e+00 9.8352551e-01] black = best\n",
      "Policy :  [0.         0.         0.         0.9897237  0.         0.01027631\n",
      " 0.         0.         0.        ] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:48 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [4.8093157e-06 0.0000000e+00 4.9247392e-03 0.0000000e+00 1.7171506e-03\n",
      " 5.2878988e-04 9.9282449e-01 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [0.         0.         0.02376161 0.         0.         0.97623837\n",
      " 0.         0.         0.        ] white = current\n",
      "current wins!\n",
      "Policy :  [5.3697832e-08 5.3697832e-08 5.4986580e-05 9.5355142e-07 9.0937750e-13\n",
      " 8.8806401e-06 5.4986580e-05 8.8806401e-06 9.9987113e-01] black = current\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 9.536743e-17 9.536743e-17\n",
      " 2.693894e-08 0.000000e+00 1.000000e+00 0.000000e+00] white = best\n",
      "Policy :  [9.2540142e-10 5.3363312e-08 3.1510503e-03 9.9364221e-01 9.4761106e-07\n",
      " 3.1510503e-03 5.4644031e-05 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [5.0793780e-10 4.8440724e-16 5.2012831e-07 0.0000000e+00 4.8440724e-16\n",
      " 9.9999946e-01 4.8440724e-16 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [2.4923918e-09 1.4035534e-10 9.9999171e-01 0.0000000e+00 2.4923918e-09\n",
      " 0.0000000e+00 8.2878432e-06 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [2.5851333e-01 7.7742363e-05 0.0000000e+00 0.0000000e+00 4.2753374e-09\n",
      " 0.0000000e+00 7.4140894e-01 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [1.5627434e-04 9.9083221e-01 0.0000000e+00 0.0000000e+00 9.0115666e-03\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [3.325256e-07 0.000000e+00 0.000000e+00 0.000000e+00 9.999997e-01\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:49 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [5.3697832e-08 5.3697832e-08 5.4986580e-05 9.5355142e-07 9.0937750e-13\n",
      " 8.8806401e-06 5.4986580e-05 8.8806401e-06 9.9987113e-01] black = current\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 9.536743e-17 9.536743e-17\n",
      " 2.693894e-08 0.000000e+00 1.000000e+00 0.000000e+00] white = best\n",
      "Policy :  [9.2540142e-10 5.3363312e-08 3.1510503e-03 9.9364221e-01 9.4761106e-07\n",
      " 3.1510503e-03 5.4644031e-05 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [1.2792907e-09 1.2200267e-15 3.1644347e-05 0.0000000e+00 1.2200267e-15\n",
      " 9.9996829e-01 1.2200267e-15 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [2.4924061e-09 2.4924061e-09 9.9999744e-01 0.0000000e+00 2.4924061e-09\n",
      " 0.0000000e+00 2.5522238e-06 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [1.1849943e-01 9.3750068e-06 0.0000000e+00 0.0000000e+00 3.3985304e-11\n",
      " 0.0000000e+00 8.8149118e-01 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [1.5627434e-04 9.9083221e-01 0.0000000e+00 0.0000000e+00 9.0115666e-03\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:50 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1.2284384e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9999875e-01\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [4.8440745e-16 4.9603323e-13 4.8440745e-16 4.9603323e-13 4.8440745e-16\n",
      " 4.9603323e-13 4.8440745e-16 1.0000000e+00 4.7305417e-09] black = best\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 2.7978726e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [7.3474780e-06 0.0000000e+00 1.9793327e-03 2.4432251e-02 4.1376231e-07\n",
      " 6.8428722e-05 7.5238175e-03 0.0000000e+00 9.6598840e-01] black = best\n",
      "Policy :  [0.         0.         0.         0.11426028 0.         0.8857397\n",
      " 0.         0.         0.        ] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:51 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:37:51 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:37:51 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:37:51 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter5.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1.9330501e-04 0.0000000e+00 3.1969115e-02 1.1146941e-02 3.1969115e-02\n",
      " 0.0000000e+00 9.2472154e-01 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.5402373e-17 1.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Current_net wins ratio 0.40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:55 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:37:55 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:37:55 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_5/.\n",
      "[I 200412 12:37:55 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.4542956352233887\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 1,    32/ 27 points] total loss per batch: 2.454\n",
      "Policy (actual, predicted): 7 3\n",
      "Policy data: tensor([0.0204, 0.0000, 0.0204, 0.0816, 0.0408, 0.0612, 0.0408, 0.7347, 0.0000])\n",
      "Policy pred: tensor([0.1419, 0.0872, 0.1716, 0.2062, 0.1078, 0.0725, 0.0876, 0.0770, 0.0482],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.2007826417684555\n",
      " \n",
      "2.3916728496551514\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 2,    32/ 27 points] total loss per batch: 2.392\n",
      "Policy (actual, predicted): 7 5\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0204, 0.0204, 0.0612, 0.0000, 0.6327, 0.2653])\n",
      "Policy pred: tensor([0.1119, 0.0271, 0.0220, 0.0298, 0.0137, 0.3404, 0.0252, 0.2605, 0.1694],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.4719117283821106\n",
      " \n",
      "2.520808219909668\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 3,    32/ 27 points] total loss per batch: 2.521\n",
      "Policy (actual, predicted): 8 0\n",
      "Policy data: tensor([0.0816, 0.0000, 0.1429, 0.1837, 0.0612, 0.1020, 0.1633, 0.0000, 0.2653])\n",
      "Policy pred: tensor([0.3709, 0.0468, 0.0758, 0.0527, 0.0198, 0.0233, 0.0536, 0.0673, 0.2897],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.021321209147572517\n",
      " \n",
      "1.9256772994995117\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 4,    32/ 27 points] total loss per batch: 1.926\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0612, 0.1837, 0.0000, 0.7347, 0.0204])\n",
      "Policy pred: tensor([0.0902, 0.1033, 0.1015, 0.0983, 0.0661, 0.1735, 0.0599, 0.1791, 0.1281],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0861792042851448\n",
      " \n",
      "1.719588279724121\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 5,    32/ 27 points] total loss per batch: 1.720\n",
      "Policy (actual, predicted): 3 7\n",
      "Policy data: tensor([0.0204, 0.0000, 0.3469, 0.4082, 0.0816, 0.1429, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0881, 0.0429, 0.0949, 0.1345, 0.0540, 0.1586, 0.0828, 0.2545, 0.0896],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.11963164806365967\n",
      " \n",
      "1.5323418378829956\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 6,    32/ 27 points] total loss per batch: 1.532\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.2041, 0.2449, 0.2857, 0.0000, 0.2653, 0.0000])\n",
      "Policy pred: tensor([0.0378, 0.0388, 0.0836, 0.1483, 0.0304, 0.3214, 0.0116, 0.2510, 0.0771],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.5400028228759766\n",
      " \n",
      "1.3334394693374634\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 7,    32/ 27 points] total loss per batch: 1.333\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0408, 0.0408, 0.0612, 0.1429, 0.0000, 0.0000, 0.6735])\n",
      "Policy pred: tensor([0.0583, 0.0798, 0.1946, 0.0874, 0.0451, 0.1946, 0.0132, 0.1118, 0.2153],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.04837452620267868\n",
      " \n",
      "1.2423917055130005\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 8,    32/ 27 points] total loss per batch: 1.242\n",
      "Policy (actual, predicted): 5 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.2041, 0.2449, 0.2857, 0.0000, 0.2653, 0.0000])\n",
      "Policy pred: tensor([0.0266, 0.0134, 0.0477, 0.3689, 0.0390, 0.1442, 0.0018, 0.3134, 0.0449],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.8316558003425598\n",
      " \n",
      "1.1415324211120605\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 9,    32/ 27 points] total loss per batch: 1.142\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0204, 0.0204, 0.0408, 0.0000, 0.0408, 0.1020, 0.0816, 0.0000, 0.6939])\n",
      "Policy pred: tensor([0.0632, 0.0438, 0.0594, 0.0278, 0.0340, 0.0376, 0.0971, 0.0761, 0.5610],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0018145794747397304\n",
      " \n",
      "1.1035268306732178\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 10,    32/ 27 points] total loss per batch: 1.104\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0612, 0.1837, 0.0000, 0.7347, 0.0204])\n",
      "Policy pred: tensor([0.0491, 0.0325, 0.0442, 0.0189, 0.0952, 0.1092, 0.0108, 0.4177, 0.2224],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.13449698686599731\n",
      " \n",
      "1.0386883020401\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 11,    32/ 27 points] total loss per batch: 1.039\n",
      "Policy (actual, predicted): 2 5\n",
      "Policy data: tensor([0.0612, 0.0000, 0.6122, 0.0000, 0.1020, 0.2245, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0670, 0.0170, 0.3569, 0.0296, 0.0538, 0.3641, 0.0023, 0.0031, 0.1062],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.01749858818948269\n",
      " \n",
      "1.0064975023269653\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 12,    32/ 27 points] total loss per batch: 1.006\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0204, 0.0204, 0.0408, 0.0000, 0.0408, 0.1020, 0.0816, 0.0000, 0.6939])\n",
      "Policy pred: tensor([0.0292, 0.0178, 0.0359, 0.0220, 0.0298, 0.0415, 0.0742, 0.0390, 0.7105],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.08264575153589249\n",
      " \n",
      "0.975513756275177\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 13,    32/ 27 points] total loss per batch: 0.976\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0204, 0.0000, 0.3469, 0.4082, 0.0816, 0.1429, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0437, 0.0036, 0.3280, 0.3789, 0.0448, 0.1652, 0.0080, 0.0060, 0.0218],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.02723691612482071\n",
      " \n",
      "0.9572522044181824\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 14,    32/ 27 points] total loss per batch: 0.957\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0408, 0.0408, 0.0612, 0.1429, 0.0000, 0.0000, 0.6735])\n",
      "Policy pred: tensor([0.0181, 0.0431, 0.0658, 0.0078, 0.0518, 0.0993, 0.0047, 0.0053, 0.7040],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.013559389859437943\n",
      " \n",
      "0.9456910490989685\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 15,    32/ 27 points] total loss per batch: 0.946\n",
      "Policy (actual, predicted): 2 5\n",
      "Policy data: tensor([0.1837, 0.0000, 0.4898, 0.0000, 0.0000, 0.3265, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.1592e-01, 1.3060e-04, 3.1748e-01, 6.3804e-03, 1.7619e-03, 3.5688e-01,\n",
      "        7.4051e-04, 6.4326e-04, 6.5735e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9854539036750793\n",
      " \n",
      "0.9651147127151489\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 16,    32/ 27 points] total loss per batch: 0.965\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0204, 0.0204, 0.0612, 0.0000, 0.6327, 0.2653])\n",
      "Policy pred: tensor([2.7087e-03, 1.8492e-02, 3.8054e-04, 3.8506e-03, 3.5117e-02, 6.8482e-02,\n",
      "        5.3608e-04, 7.3852e-01, 1.3191e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9407111406326294\n",
      " \n",
      "1.0139203071594238\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 17,    32/ 27 points] total loss per batch: 1.014\n",
      "Policy (actual, predicted): 7 3\n",
      "Policy data: tensor([0.0204, 0.0000, 0.0204, 0.0816, 0.0408, 0.0612, 0.0408, 0.7347, 0.0000])\n",
      "Policy pred: tensor([0.0458, 0.0075, 0.0674, 0.3888, 0.0556, 0.1152, 0.0164, 0.2877, 0.0156],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9431729316711426\n",
      " \n",
      "0.9575537443161011\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 18,    32/ 27 points] total loss per batch: 0.958\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0204, 0.0408, 0.0204, 0.0408, 0.0204, 0.0204, 0.0204, 0.7143, 0.1020])\n",
      "Policy pred: tensor([0.0096, 0.0183, 0.0043, 0.0140, 0.0116, 0.0252, 0.0099, 0.8482, 0.0590],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0091764060780406\n",
      " \n",
      "0.9429917335510254\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 19,    32/ 27 points] total loss per batch: 0.943\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0408, 0.3878, 0.0816, 0.0612, 0.0816, 0.2245, 0.0204, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.0610, 0.3867, 0.0444, 0.0421, 0.0906, 0.2167, 0.0253, 0.0341, 0.0990],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0169287770986557\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934633195400238\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 20,    32/ 27 points] total loss per batch: 0.935\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.3061, 0.0816, 0.5102, 0.0000, 0.0000, 0.0000, 0.1020, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.3210e-01, 5.6811e-02, 6.2125e-01, 5.9999e-05, 3.1572e-04, 6.7414e-03,\n",
      "        8.0129e-02, 6.8091e-06, 2.5780e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.02776207961142063\n",
      " \n",
      "0.9267706871032715\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 21,    32/ 27 points] total loss per batch: 0.927\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.3061, 0.0816, 0.5102, 0.0000, 0.0000, 0.0000, 0.1020, 0.0000, 0.0000])\n",
      "Policy pred: tensor([4.2402e-01, 4.3922e-02, 4.5911e-01, 4.7737e-05, 2.5013e-04, 5.6252e-03,\n",
      "        6.3912e-02, 4.7746e-06, 3.1075e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 2.993643283843994e-05\n",
      " \n",
      "0.9144534468650818\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 22,    32/ 27 points] total loss per batch: 0.914\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0612, 0.0000, 0.6122, 0.0000, 0.1020, 0.2245, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([8.9781e-02, 2.3106e-04, 6.3700e-01, 7.1212e-03, 6.9750e-02, 1.9454e-01,\n",
      "        4.6574e-04, 2.2928e-05, 1.0862e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.01301207672804594\n",
      " \n",
      "0.9176015257835388\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 23,    32/ 27 points] total loss per batch: 0.918\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0204, 0.0408, 0.0204, 0.0408, 0.0204, 0.0408, 0.0204, 0.6939, 0.1020])\n",
      "Policy pred: tensor([0.0133, 0.0341, 0.0118, 0.0354, 0.0273, 0.0453, 0.0168, 0.7442, 0.0717],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.01043898705393076\n",
      " \n",
      "0.9047869443893433\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 24,    32/ 27 points] total loss per batch: 0.905\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0408, 0.0408, 0.0612, 0.1429, 0.0000, 0.0000, 0.6735])\n",
      "Policy pred: tensor([0.0202, 0.0055, 0.0366, 0.0254, 0.0537, 0.1529, 0.0015, 0.0022, 0.7021],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.023532653227448463\n",
      " \n",
      "0.9072574377059937\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 25,    32/ 27 points] total loss per batch: 0.907\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.1020, 0.0000, 0.0000, 0.0000, 0.0000, 0.8980, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.3053e-01, 2.2767e-06, 1.0310e-02, 2.9388e-04, 6.6954e-04, 8.5819e-01,\n",
      "        5.8845e-06, 6.7345e-08, 1.3667e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.010795829817652702\n",
      " \n",
      "0.9026197791099548\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 26,    32/ 27 points] total loss per batch: 0.903\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([9.9875e-01, 2.6087e-08, 3.5353e-04, 2.2776e-10, 8.9120e-10, 8.9501e-04,\n",
      "        1.6026e-07, 1.4026e-10, 1.1965e-11], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9796029925346375\n",
      " \n",
      "0.8980411887168884\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 27,    32/ 27 points] total loss per batch: 0.898\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.1020, 0.0000, 0.0000, 0.0000, 0.0000, 0.8980, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([7.1937e-02, 1.2476e-06, 4.4507e-03, 1.2692e-04, 3.9621e-04, 9.2308e-01,\n",
      "        2.9918e-06, 3.7661e-08, 5.3969e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.009983163326978683\n",
      " \n",
      "0.8977240920066833\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 28,    32/ 27 points] total loss per batch: 0.898\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0204, 0.0204, 0.0612, 0.0000, 0.6327, 0.2653])\n",
      "Policy pred: tensor([1.1365e-03, 1.0607e-03, 1.4618e-04, 9.7261e-03, 2.7219e-02, 4.9864e-02,\n",
      "        2.6820e-04, 6.9640e-01, 2.1418e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9820292592048645\n",
      " \n",
      "0.895035982131958\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 29,    32/ 27 points] total loss per batch: 0.895\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0000, 0.0000, 0.0000, 0.9592, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([5.7889e-02, 8.0048e-08, 1.2589e-03, 2.8674e-04, 1.3278e-05, 9.4055e-01,\n",
      "        3.7630e-07, 5.7460e-07, 4.1454e-09], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.998533308506012\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:57 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8944973349571228\n",
      "Update step size: 1\n",
      "[Iteration 5] Process ID: 11022 [Epoch: 30,    32/ 27 points] total loss per batch: 0.894\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0204, 0.0204, 0.0408, 0.0000, 0.0408, 0.1020, 0.0816, 0.0000, 0.6939])\n",
      "Policy pred: tensor([0.0441, 0.0203, 0.0519, 0.0052, 0.0297, 0.0780, 0.0685, 0.0063, 0.6960],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.005050903186202049\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:58 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:37:58 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter6.pth.tar\n",
      "[I 200412 12:37:58 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter5.pth.tar\n",
      "[I 200412 12:37:58 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:37:58 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 9.9999982e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.5928100e-16 0.0000000e+00 1.7102667e-07 1.6310374e-13] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2198601e-15 1.2198601e-15\n",
      " 7.2031520e-11 0.0000000e+00 9.9983186e-01 1.6816809e-04] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4135136e-12\n",
      " 8.4154019e-03 0.0000000e+00 0.0000000e+00 9.9158460e-01] black = current\n",
      "Policy :  [1.3999064e-10 0.0000000e+00 2.3707540e-15 9.9739331e-01 0.0000000e+00\n",
      " 2.6066715e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 1. 0. 0.] black = current\n",
      "Policy :  [1.7215429e-04 0.0000000e+00 8.2354188e-01 0.0000000e+00 4.9373368e-14\n",
      " 1.7628600e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.9468002e-14 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9468002e-14\n",
      " 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:37:59 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 9.9999982e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.5928100e-16 0.0000000e+00 1.7102667e-07 1.6310374e-13] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2198601e-15 1.2198601e-15\n",
      " 7.2031520e-11 0.0000000e+00 9.9983186e-01 1.6816809e-04] white = best\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.199298e-11\n",
      " 7.030565e-04 0.000000e+00 0.000000e+00 9.992969e-01] black = current\n",
      "Policy :  [9.9814136e-11 0.0000000e+00 0.0000000e+00 9.9814141e-01 0.0000000e+00\n",
      " 1.8585718e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5402373e-17\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [7.2356453e-04 0.0000000e+00 2.5834635e-01 0.0000000e+00 7.2356455e-14\n",
      " 7.4093008e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [1.9468002e-14 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:00 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [7.9994706e-08 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9999994e-01\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = current\n",
      "current wins!\n",
      "Policy :  [4.8440745e-16 4.9603323e-13 4.8440745e-16 2.8603776e-11 4.8440745e-16\n",
      " 4.8440745e-16 4.8440745e-16 1.0000000e+00 4.7305417e-09] black = best\n",
      "Policy :  [2.2229449e-09 9.9986869e-01 2.2229449e-09 2.2229449e-09 3.9474450e-08\n",
      " 1.3126268e-04 3.8549268e-11 0.0000000e+00 2.2229449e-09] white = current\n",
      "Policy :  [4.1148368e-07 0.0000000e+00 7.4823829e-03 2.4297701e-02 4.1148368e-07\n",
      " 6.8051879e-05 7.4823829e-03 0.0000000e+00 9.6066862e-01] black = best\n",
      "Policy :  [9.3215477e-04 0.0000000e+00 3.2999519e-02 3.2226093e-05 1.1506221e-02\n",
      " 9.5452648e-01 3.4602504e-06 0.0000000e+00 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:01 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1.9330501e-04 0.0000000e+00 3.1969115e-02 1.1146941e-02 3.1969115e-02\n",
      " 0.0000000e+00 9.2472154e-01 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [9.7555615e-04 0.0000000e+00 9.9896950e-01 1.6521129e-08 5.4936998e-05\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 9.9999982e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.5928100e-16 0.0000000e+00 1.7102667e-07 1.6310374e-13] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2198601e-15 1.2198601e-15\n",
      " 7.2031520e-11 0.0000000e+00 9.9983186e-01 1.6816809e-04] white = best\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.199298e-11\n",
      " 7.030565e-04 0.000000e+00 0.000000e+00 9.992969e-01] black = current\n",
      "Policy :  [1.3999064e-10 0.0000000e+00 2.3707540e-15 9.9739331e-01 0.0000000e+00\n",
      " 2.6066715e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 9.9999994e-01 7.9994706e-08 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [2.1044656e-08 0.0000000e+00 1.0000000e+00 0.0000000e+00 7.4500882e-17\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [1.9468002e-14 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [9.999623e-01 0.000000e+00 0.000000e+00 0.000000e+00 3.770441e-05\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:02 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [3.6250961e-16 3.7120984e-13 3.6250961e-16 3.7120984e-13 3.6250961e-16\n",
      " 3.6250961e-16 3.6250961e-16 1.0000000e+00 3.5401331e-09] black = best\n",
      "Policy :  [2.2231461e-09 9.9995917e-01 2.2231461e-09 2.2231461e-09 3.6766772e-07\n",
      " 4.0425493e-05 3.8552755e-11 0.0000000e+00 2.2231461e-09] white = current\n",
      "Policy :  [3.4911911e-06 0.0000000e+00 3.2514250e-05 3.3294592e-02 1.9660125e-07\n",
      " 3.2514250e-05 3.5749797e-03 0.0000000e+00 9.6306169e-01] black = best\n",
      "Policy :  [9.3215477e-04 0.0000000e+00 3.2999519e-02 3.2226093e-05 1.1506221e-02\n",
      " 9.5452648e-01 3.4602504e-06 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [1.9330501e-04 0.0000000e+00 3.1969115e-02 1.1146941e-02 3.1969115e-02\n",
      " 0.0000000e+00 9.2472154e-01 0.0000000e+00 0.0000000e+00] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:03 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:38:03 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:38:03 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter6.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [4.0889598e-04 0.0000000e+00 9.9953598e-01 1.5395226e-07 5.4968150e-05\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Current_net wins ratio 0.60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:06 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:38:06 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:38:06 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_6/.\n",
      "[I 200412 12:38:06 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.4561009407043457\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 1,    32/ 27 points] total loss per batch: 2.456\n",
      "Policy (actual, predicted): 8 0\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0612, 0.2653, 0.0000, 0.0000, 0.6735])\n",
      "Policy pred: tensor([0.1831, 0.0763, 0.1199, 0.0963, 0.1108, 0.0905, 0.1400, 0.0250, 0.1583],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.19435304403305054\n",
      " \n",
      "2.4331777095794678\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 2,    32/ 27 points] total loss per batch: 2.433\n",
      "Policy (actual, predicted): 7 6\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0514, 0.1770, 0.0857, 0.0667, 0.0846, 0.0884, 0.1988, 0.0958, 0.1518],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.3855574131011963\n",
      " \n",
      "2.5630578994750977\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 3,    32/ 27 points] total loss per batch: 2.563\n",
      "Policy (actual, predicted): 8 3\n",
      "Policy data: tensor([0.0612, 0.0000, 0.1224, 0.1633, 0.0816, 0.1020, 0.1224, 0.0000, 0.3469])\n",
      "Policy pred: tensor([0.0436, 0.1328, 0.1575, 0.2806, 0.0563, 0.1186, 0.0176, 0.0311, 0.1619],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.013352621346712112\n",
      " \n",
      "2.06274151802063\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 4,    32/ 27 points] total loss per batch: 2.063\n",
      "Policy (actual, predicted): 7 6\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0928, 0.1137, 0.0559, 0.0470, 0.1601, 0.1189, 0.1622, 0.1499, 0.0996],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.12066802382469177\n",
      " \n",
      "2.001447916030884\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 5,    32/ 27 points] total loss per batch: 2.001\n",
      "Policy (actual, predicted): 3 6\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0000, 0.4490, 0.1429, 0.2653, 0.0612, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1415, 0.0704, 0.0394, 0.0839, 0.1233, 0.1478, 0.2464, 0.0797, 0.0676],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.029855629429221153\n",
      " \n",
      "1.712862491607666\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 6,    32/ 27 points] total loss per batch: 1.713\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0612, 0.0612, 0.0000, 0.4694, 0.0000, 0.0816])\n",
      "Policy pred: tensor([0.0747, 0.0219, 0.1517, 0.0663, 0.0543, 0.0895, 0.2628, 0.0408, 0.2381],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.6419351100921631\n",
      " \n",
      "1.6373234987258911\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 7,    32/ 27 points] total loss per batch: 1.637\n",
      "Policy (actual, predicted): 8 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0000, 0.0612, 0.1224, 0.3061, 0.0408, 0.0000, 0.3878])\n",
      "Policy pred: tensor([0.0478, 0.0998, 0.0510, 0.0465, 0.1150, 0.2616, 0.1621, 0.1375, 0.0789],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.08365613222122192\n",
      " \n",
      "1.4642837047576904\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 8,    32/ 27 points] total loss per batch: 1.464\n",
      "Policy (actual, predicted): 6 2\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0612, 0.0612, 0.0000, 0.4694, 0.0000, 0.0816])\n",
      "Policy pred: tensor([0.0837, 0.0289, 0.2190, 0.0551, 0.0854, 0.1449, 0.1501, 0.0666, 0.1663],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.8117286562919617\n",
      " \n",
      "1.3997416496276855\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 9,    32/ 27 points] total loss per batch: 1.400\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3673, 0.0000, 0.0000, 0.6327])\n",
      "Policy pred: tensor([0.0456, 0.0164, 0.0530, 0.0378, 0.0101, 0.1464, 0.0304, 0.0106, 0.6497],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.634799063205719\n",
      " \n",
      "1.3059203624725342\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 10,    32/ 27 points] total loss per batch: 1.306\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0459, 0.0756, 0.0921, 0.0305, 0.0384, 0.1040, 0.0625, 0.4307, 0.1203],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.20146524906158447\n",
      " \n",
      "1.2255022525787354\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 11,    32/ 27 points] total loss per batch: 1.226\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.1837, 0.0000, 0.0000, 0.0000, 0.1429, 0.5918, 0.0816, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1402, 0.0045, 0.0272, 0.0580, 0.1300, 0.4395, 0.1240, 0.0048, 0.0718],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.045585066080093384\n",
      " \n",
      "1.154224157333374\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 12,    32/ 27 points] total loss per batch: 1.154\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3673, 0.0000, 0.0000, 0.6327])\n",
      "Policy pred: tensor([0.0160, 0.0248, 0.0413, 0.0177, 0.0120, 0.3241, 0.0117, 0.0106, 0.5418],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.6133363842964172\n",
      " \n",
      "1.1095706224441528\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 13,    32/ 27 points] total loss per batch: 1.110\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0000, 0.4490, 0.1429, 0.2653, 0.0612, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.6806e-01, 1.0340e-03, 8.0674e-03, 2.6917e-01, 9.6144e-02, 1.5775e-01,\n",
      "        1.3526e-01, 2.0209e-04, 6.4309e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.06596212834119797\n",
      " \n",
      "1.0725892782211304\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 14,    32/ 27 points] total loss per batch: 1.073\n",
      "Policy (actual, predicted): 8 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0000, 0.0612, 0.1224, 0.3061, 0.0408, 0.0000, 0.3878])\n",
      "Policy pred: tensor([0.0799, 0.0170, 0.0204, 0.0674, 0.0879, 0.3904, 0.0285, 0.0073, 0.3012],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.016733447089791298\n",
      " \n",
      "1.0731626749038696\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 15,    32/ 27 points] total loss per batch: 1.073\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.9184, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000, 0.0612])\n",
      "Policy pred: tensor([2.3505e-03, 2.0399e-05, 7.5194e-01, 9.5524e-02, 5.9740e-02, 1.0856e-03,\n",
      "        3.3045e-03, 1.4117e-05, 8.6026e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.976028323173523\n",
      " \n",
      "1.0824519395828247\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 16,    32/ 27 points] total loss per batch: 1.082\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0484, 0.0131, 0.1279, 0.0529, 0.0285, 0.0837, 0.0734, 0.5043, 0.0678],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.5727559328079224\n",
      " \n",
      "1.0398972034454346\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 17,    32/ 27 points] total loss per batch: 1.040\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0612, 0.2653, 0.0000, 0.0000, 0.6735])\n",
      "Policy pred: tensor([0.0131, 0.0059, 0.0163, 0.0069, 0.0184, 0.1780, 0.0072, 0.0075, 0.7468],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.36889129877090454\n",
      " \n",
      "1.0183782577514648\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 18,    32/ 27 points] total loss per batch: 1.018\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.7755, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1633, 0.0408])\n",
      "Policy pred: tensor([0.0031, 0.7702, 0.0048, 0.0076, 0.0058, 0.0221, 0.0045, 0.1470, 0.0349],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.026876017451286316\n",
      " \n",
      "0.9970520734786987\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 19,    32/ 27 points] total loss per batch: 0.997\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0612, 0.4694, 0.0612, 0.0612, 0.0816, 0.1633, 0.0408, 0.0000, 0.0612])\n",
      "Policy pred: tensor([0.0511, 0.4460, 0.0424, 0.0544, 0.0812, 0.1911, 0.0409, 0.0173, 0.0755],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0022857782896608114\n",
      " \n",
      "0.9885596632957458\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 20,    32/ 27 points] total loss per batch: 0.989\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0408, 0.0612, 0.1224, 0.7755, 0.0000, 0.0000])\n",
      "Policy pred: tensor([7.0355e-03, 4.2304e-05, 1.6831e-02, 3.1581e-02, 1.2402e-01, 1.4918e-01,\n",
      "        6.7068e-01, 6.0158e-05, 5.7331e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.012722655199468136\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9754299521446228\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 21,    32/ 27 points] total loss per batch: 0.975\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0408, 0.0612, 0.1224, 0.7755, 0.0000, 0.0000])\n",
      "Policy pred: tensor([4.9260e-03, 3.0585e-05, 8.6639e-03, 1.8273e-02, 9.2563e-02, 9.1450e-02,\n",
      "        7.8376e-01, 4.0797e-05, 2.9124e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.005824768450111151\n",
      " \n",
      "0.9706348180770874\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 22,    32/ 27 points] total loss per batch: 0.971\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.1837, 0.0000, 0.0000, 0.0000, 0.1429, 0.5918, 0.0816, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.9436e-01, 4.2208e-05, 2.0401e-03, 7.5074e-03, 1.8245e-01, 3.9223e-01,\n",
      "        1.1741e-01, 5.6250e-06, 3.9503e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.005599283613264561\n",
      " \n",
      "0.9596533179283142\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 23,    32/ 27 points] total loss per batch: 0.960\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.7347, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.2041, 0.0408])\n",
      "Policy pred: tensor([6.4687e-04, 7.3180e-01, 1.8618e-03, 2.7155e-03, 3.1833e-03, 2.2371e-02,\n",
      "        2.5699e-03, 1.8265e-01, 5.2204e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.008167874999344349\n",
      " \n",
      "0.9624561071395874\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 24,    32/ 27 points] total loss per batch: 0.962\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0000, 0.0612, 0.1224, 0.3061, 0.0408, 0.0000, 0.3878])\n",
      "Policy pred: tensor([7.2343e-02, 3.2156e-03, 7.8117e-03, 5.2781e-02, 9.3323e-02, 3.6579e-01,\n",
      "        2.9073e-02, 3.2659e-04, 3.7534e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.00013949349522590637\n",
      " \n",
      "0.9575157165527344\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 25,    32/ 27 points] total loss per batch: 0.958\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3265, 0.0000, 0.6735, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.5005e-03, 9.9507e-08, 4.3557e-05, 3.3893e-03, 4.0793e-01, 4.1785e-03,\n",
      "        5.8295e-01, 1.6699e-08, 8.3516e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.002130043227225542\n",
      " \n",
      "0.9554783701896667\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 26,    32/ 27 points] total loss per batch: 0.955\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([3.7841e-09, 4.6362e-12, 1.7529e-03, 9.9816e-01, 7.4022e-05, 8.2533e-10,\n",
      "        1.2199e-07, 7.8900e-14, 1.1965e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9926205277442932\n",
      " \n",
      "0.9532329440116882\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 27,    32/ 27 points] total loss per batch: 0.953\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3265, 0.0000, 0.6735, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.0295e-03, 5.1331e-08, 2.1009e-05, 2.4140e-03, 4.4894e-01, 1.3743e-03,\n",
      "        5.4622e-01, 8.1260e-09, 4.8576e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.005049435421824455\n",
      " \n",
      "0.9491572380065918\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 28,    32/ 27 points] total loss per batch: 0.949\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0406, 0.0016, 0.0730, 0.0395, 0.0400, 0.0667, 0.0738, 0.5799, 0.0849],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.48776689171791077\n",
      " \n",
      "0.9492553472518921\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 29,    32/ 27 points] total loss per batch: 0.949\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.2245, 0.0000, 0.0000, 0.0000, 0.0000, 0.7755])\n",
      "Policy pred: tensor([3.3853e-03, 1.8010e-09, 8.6982e-03, 2.2958e-01, 5.6453e-04, 4.2301e-06,\n",
      "        3.6051e-05, 8.2232e-08, 7.5773e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9992768168449402\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:09 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9489728212356567\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 30,    32/ 27 points] total loss per batch: 0.949\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3673, 0.0000, 0.0000, 0.6327])\n",
      "Policy pred: tensor([7.6297e-04, 8.0002e-04, 1.6285e-03, 2.3366e-03, 2.7470e-02, 4.5627e-01,\n",
      "        7.6258e-04, 3.6408e-04, 5.0961e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.5025287866592407\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:09 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:38:09 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:38:09 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter6.pth.tar\n",
      "[I 200412 12:38:09 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:38:09 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 3.6768274e-17 0.0000000e+00 2.1711298e-12 3.6768274e-17] black = current\n",
      "Policy :  [1.7341530e-12 0.0000000e+00 9.9999994e-11 1.7341530e-12 1.7341530e-12\n",
      " 9.9999994e-11 9.9999994e-11 1.0000000e+00 1.7757726e-09] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6538173e-08] black = current\n",
      "Policy :  [2.5273936e-08 0.0000000e+00 1.4924007e-03 1.4232642e-09 1.4232642e-09\n",
      " 0.0000000e+00 9.9850750e-01 0.0000000e+00 2.5273936e-08] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9999642e-01 0.0000000e+00 3.6250833e-06\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8011752e-10] black = current\n",
      "Policy :  [7.6433551e-01 0.0000000e+00 0.0000000e+00 2.2985759e-04 2.3537417e-01\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.0469920e-05] white = best\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = current\n",
      "Policy :  [0.         0.         0.         0.         0.9897237  0.\n",
      " 0.         0.         0.01027631] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:10 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 0. 1.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 9.9999642e-01 0.0000000e+00 3.6250832e-16 0.0000000e+00\n",
      " 3.6250832e-16 0.0000000e+00 3.6250833e-06 3.7120852e-13] black = best\n",
      "Policy :  [9.9988816e-11 0.0000000e+00 9.5356768e-07 1.0238855e-07 1.0484587e-04\n",
      " 9.9988818e-01 9.9988816e-11 5.9042400e-06 9.9988816e-11] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 9.9999875e-01 1.2284384e-06] black = best\n",
      "Policy :  [2.9839981e-05 0.0000000e+00 4.9349871e-03 1.3940118e-04 9.9489045e-01\n",
      " 0.0000000e+00 4.8193233e-06 0.0000000e+00 5.1747094e-07] white = current\n",
      "Policy :  [0.         0.         0.600661   0.39933902 0.         0.\n",
      " 0.         0.         0.        ] black = best\n",
      "Policy :  [1.18931895e-02 0.00000000e+00 0.00000000e+00 6.32128045e-02\n",
      " 0.00000000e+00 0.00000000e+00 9.24401641e-01 0.00000000e+00\n",
      " 4.92347055e-04] white = current\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5402373e-17 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:11 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.         0.         0.         0.9897237  0.         0.\n",
      " 0.         0.         0.01027631] white = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.7341530e-12 2.9368032e-17] black = current\n",
      "Policy :  [1.7341530e-12 0.0000000e+00 9.9999994e-11 1.7341530e-12 1.7341530e-12\n",
      " 9.9999994e-11 9.9999994e-11 1.0000000e+00 1.7757726e-09] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6538173e-08] black = current\n",
      "Policy :  [2.5273936e-08 0.0000000e+00 1.4924007e-03 1.4232642e-09 1.4232642e-09\n",
      " 0.0000000e+00 9.9850750e-01 0.0000000e+00 2.5273936e-08] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9999642e-01 0.0000000e+00 3.6250833e-06\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8011752e-10] black = current\n",
      "Policy :  [7.6433551e-01 0.0000000e+00 0.0000000e+00 2.2985759e-04 2.3537417e-01\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.0469920e-05] white = best\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:12 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.         0.         0.         0.         0.97623837 0.\n",
      " 0.         0.         0.02376161] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 0. 1.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 3.6768274e-17 0.0000000e+00 2.1711298e-12 3.6768274e-17] black = current\n",
      "Policy :  [1.7341530e-12 0.0000000e+00 9.9999994e-11 1.7341530e-12 1.7341530e-12\n",
      " 9.9999994e-11 9.9999994e-11 1.0000000e+00 1.7757726e-09] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6538173e-08] black = current\n",
      "Policy :  [2.5273936e-08 0.0000000e+00 1.4924007e-03 1.4232642e-09 1.4232642e-09\n",
      " 0.0000000e+00 9.9850750e-01 0.0000000e+00 2.5273936e-08] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9999905e-01 0.0000000e+00 9.5367341e-07\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8679692e-10] black = current\n",
      "Policy :  [7.6433551e-01 0.0000000e+00 0.0000000e+00 2.2985759e-04 2.3537417e-01\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.0469920e-05] white = best\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = current\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7978726e-09] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:13 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 0. 1.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 3.6768274e-17 0.0000000e+00 2.1711298e-12 3.6768274e-17] black = current\n",
      "Policy :  [1.7341530e-12 0.0000000e+00 9.9999994e-11 1.7341530e-12 1.7341530e-12\n",
      " 9.9999994e-11 9.9999994e-11 1.0000000e+00 1.7757726e-09] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6538173e-08] black = current\n",
      "Policy :  [2.5273936e-08 0.0000000e+00 1.4924007e-03 1.4232642e-09 1.4232642e-09\n",
      " 0.0000000e+00 9.9850750e-01 0.0000000e+00 2.5273936e-08] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9999642e-01 0.0000000e+00 3.6250833e-06\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8011752e-10] black = current\n",
      "Policy :  [6.4663833e-01 0.0000000e+00 0.0000000e+00 3.4440710e-04 3.5267287e-01\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4440710e-04] white = best\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:14 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:38:14 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:38:14 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:38:14 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter6.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.         0.         0.         0.         0.9897237  0.\n",
      " 0.         0.         0.01027631] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 0. 1.] black = current\n",
      "None wins!\n",
      "Current_net wins ratio 0.20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:18 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:38:18 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:38:18 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_6/.\n",
      "[I 200412 12:38:18 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.614288568496704\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 1,    32/ 26 points] total loss per batch: 2.614\n",
      "Policy (actual, predicted): 7 6\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.1079, 0.1282, 0.1394, 0.1147, 0.1076, 0.0952, 0.1768, 0.0416, 0.0886],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 -0.010202858597040176\n",
      " \n",
      "2.3386244773864746\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 2,    32/ 26 points] total loss per batch: 2.339\n",
      "Policy (actual, predicted): 2 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.6735, 0.2449, 0.0204, 0.0000, 0.0000, 0.0000, 0.0612])\n",
      "Policy pred: tensor([0.0267, 0.1411, 0.0663, 0.1724, 0.2390, 0.1860, 0.0184, 0.1031, 0.0471],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.3708232641220093\n",
      " \n",
      "2.524780035018921\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 3,    32/ 26 points] total loss per batch: 2.525\n",
      "Policy (actual, predicted): 0 2\n",
      "Policy data: tensor([0.6531, 0.0000, 0.0000, 0.1633, 0.0816, 0.0000, 0.0000, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.1428, 0.0160, 0.2852, 0.1997, 0.0587, 0.0379, 0.1241, 0.0489, 0.0866],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.25999507308006287\n",
      " \n",
      "2.0323891639709473\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 4,    32/ 26 points] total loss per batch: 2.032\n",
      "Policy (actual, predicted): 4 5\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([0.1316, 0.0108, 0.0919, 0.0786, 0.2382, 0.2650, 0.0451, 0.0573, 0.0815],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.2865920066833496\n",
      " \n",
      "1.791198492050171\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 5,    32/ 26 points] total loss per batch: 1.791\n",
      "Policy (actual, predicted): 2 4\n",
      "Policy data: tensor([0.1837, 0.0000, 0.5510, 0.0612, 0.2041, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0865, 0.0212, 0.1677, 0.0899, 0.2829, 0.1490, 0.1021, 0.0577, 0.0429],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.17826540768146515\n",
      " \n",
      "1.6527001857757568\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 6,    32/ 26 points] total loss per batch: 1.653\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0369, 0.0924, 0.0822, 0.0207, 0.0320, 0.0787, 0.1087, 0.4073, 0.1411],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.2881527841091156\n",
      " \n",
      "1.4958237409591675\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 7,    32/ 26 points] total loss per batch: 1.496\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0222, 0.0385, 0.0948, 0.0098, 0.0239, 0.0526, 0.1313, 0.5626, 0.0644],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.3802071213722229\n",
      " \n",
      "1.3580113649368286\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 8,    32/ 26 points] total loss per batch: 1.358\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([2.6737e-02, 3.2283e-06, 5.3432e-03, 7.2499e-01, 2.2442e-01, 1.5303e-02,\n",
      "        5.7794e-04, 9.7960e-06, 2.6218e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.09661263972520828\n",
      " \n",
      "1.291967511177063\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 9,    32/ 26 points] total loss per batch: 1.292\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7959, 0.2041])\n",
      "Policy pred: tensor([0.0153, 0.1840, 0.0266, 0.0105, 0.0108, 0.0161, 0.0436, 0.5341, 0.1590],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.7642809152603149\n",
      " \n",
      "1.170317530632019\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 10,    32/ 26 points] total loss per batch: 1.170\n",
      "Policy (actual, predicted): 6 2\n",
      "Policy data: tensor([0.0612, 0.0000, 0.2857, 0.0612, 0.0612, 0.0000, 0.4286, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.1425, 0.0044, 0.2717, 0.0482, 0.0852, 0.0114, 0.2535, 0.0691, 0.1140],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.896379828453064\n",
      " \n",
      "1.1084548234939575\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 11,    32/ 26 points] total loss per batch: 1.108\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([1.3846e-01, 2.0346e-05, 3.8663e-02, 6.5595e-02, 6.3973e-01, 1.1059e-01,\n",
      "        2.3498e-03, 4.3502e-05, 4.5446e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.03650825843214989\n",
      " \n",
      "1.057616114616394\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 12,    32/ 26 points] total loss per batch: 1.058\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0612, 0.0000, 0.2857, 0.0612, 0.0612, 0.0000, 0.4286, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.1372, 0.0023, 0.2593, 0.0784, 0.0549, 0.0047, 0.2909, 0.1047, 0.0675],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.905126690864563\n",
      " \n",
      "1.0294339656829834\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 13,    32/ 26 points] total loss per batch: 1.029\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.6939, 0.0000, 0.0204, 0.0204, 0.0204, 0.0000, 0.2041, 0.0408])\n",
      "Policy pred: tensor([4.6864e-04, 7.9641e-01, 6.7010e-04, 3.4916e-03, 1.0653e-03, 2.2149e-02,\n",
      "        2.6438e-04, 1.2762e-01, 4.7865e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.02131587639451027\n",
      " \n",
      "0.9869874119758606\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 14,    32/ 26 points] total loss per batch: 0.987\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.7551, 0.0408, 0.0816, 0.1224, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.1367e-02, 4.3030e-05, 6.9473e-01, 1.5103e-02, 1.1353e-01, 6.6237e-02,\n",
      "        7.6642e-02, 5.3637e-04, 1.8114e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.03579235076904297\n",
      " \n",
      "0.9565328359603882\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 15,    32/ 26 points] total loss per batch: 0.957\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.6531, 0.0000, 0.0000, 0.1633, 0.0816, 0.0000, 0.0000, 0.0000, 0.1020])\n",
      "Policy pred: tensor([4.0360e-01, 5.9183e-05, 7.1195e-02, 3.5543e-01, 6.6911e-02, 2.1042e-04,\n",
      "        3.2037e-03, 2.9597e-04, 9.9094e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9765092730522156\n",
      " \n",
      "0.9614468216896057\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 16,    32/ 26 points] total loss per batch: 0.961\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.1837, 0.0000, 0.5510, 0.0612, 0.2041, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.2495e-01, 1.5264e-05, 5.6284e-01, 3.7634e-02, 2.4732e-01, 7.7806e-03,\n",
      "        1.8590e-02, 1.0447e-04, 7.7483e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.04520733281970024\n",
      " \n",
      "0.9362948536872864\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 17,    32/ 26 points] total loss per batch: 0.936\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0612, 0.1224, 0.1633, 0.3265, 0.0000, 0.0000])\n",
      "Policy pred: tensor([7.5475e-02, 3.2467e-04, 2.1133e-01, 6.5466e-02, 1.4416e-01, 1.4118e-01,\n",
      "        3.4383e-01, 6.3707e-03, 1.1860e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.00010278820991516113\n",
      " \n",
      "0.9227518439292908\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 18,    32/ 26 points] total loss per batch: 0.923\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0408, 0.0000, 0.1224, 0.0204, 0.0204, 0.0000, 0.7959, 0.0000, 0.0000])\n",
      "Policy pred: tensor([4.1400e-02, 5.6183e-05, 8.2875e-02, 1.5472e-02, 1.8160e-02, 7.5045e-03,\n",
      "        8.2221e-01, 3.3015e-03, 9.0226e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.03170938789844513\n",
      " \n",
      "0.9134761095046997\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 19,    32/ 26 points] total loss per batch: 0.913\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0612, 0.0000, 0.2857, 0.0612, 0.0612, 0.0000, 0.4286, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.0903, 0.0005, 0.3260, 0.0813, 0.0284, 0.0072, 0.3537, 0.0214, 0.0912],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9610049724578857\n",
      " \n",
      "0.9040049910545349\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 20,    32/ 26 points] total loss per batch: 0.904\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([4.8999e-04, 7.7338e-10, 3.8190e-08, 9.9446e-01, 4.8347e-03, 3.9493e-07,\n",
      "        6.9483e-11, 4.2196e-10, 2.1434e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.010693849995732307\n",
      " \n",
      "0.9067061543464661\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 21,    32/ 26 points] total loss per batch: 0.907\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.6531, 0.0000, 0.0000, 0.1633, 0.0816, 0.0000, 0.0000, 0.0000, 0.1020])\n",
      "Policy pred: tensor([6.4379e-01, 1.7109e-05, 1.3306e-02, 1.9973e-01, 4.8098e-02, 1.3050e-04,\n",
      "        7.0206e-04, 1.3799e-05, 9.4208e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9910464882850647\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9014418125152588\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 22,    32/ 26 points] total loss per batch: 0.901\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([1.4500e-04, 7.3145e-09, 1.5681e-04, 2.6480e-02, 9.6676e-01, 6.4601e-03,\n",
      "        1.1992e-08, 2.4539e-09, 1.8717e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.013639707118272781\n",
      " \n",
      "0.9027477502822876\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 23,    32/ 26 points] total loss per batch: 0.903\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.6939, 0.0000, 0.0204, 0.0204, 0.0204, 0.0000, 0.2041, 0.0408])\n",
      "Policy pred: tensor([1.1355e-04, 7.4950e-01, 6.1959e-04, 1.1002e-02, 8.0811e-03, 1.5837e-02,\n",
      "        2.5055e-05, 1.8036e-01, 3.4468e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.004550399724394083\n",
      " \n",
      "0.8981397151947021\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 24,    32/ 26 points] total loss per batch: 0.898\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.6939, 0.0000, 0.0204, 0.0204, 0.0204, 0.0000, 0.2041, 0.0408])\n",
      "Policy pred: tensor([9.7052e-05, 6.4721e-01, 5.8668e-04, 1.1927e-02, 9.0190e-03, 2.0038e-02,\n",
      "        2.4631e-05, 2.6903e-01, 4.2067e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.005315190646797419\n",
      " \n",
      "0.8971224427223206\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 25,    32/ 26 points] total loss per batch: 0.897\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.7143, 0.0000, 0.0204, 0.0000, 0.0204, 0.0000, 0.2041, 0.0408])\n",
      "Policy pred: tensor([8.6536e-05, 7.5222e-01, 5.1173e-04, 1.1507e-02, 9.5162e-03, 2.3064e-02,\n",
      "        1.9685e-05, 1.5478e-01, 4.8287e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.004453097004443407\n",
      " \n",
      "0.8937236070632935\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 26,    32/ 26 points] total loss per batch: 0.894\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.1429, 0.8571, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.3312e-03, 1.9479e-08, 1.9846e-04, 1.6615e-01, 8.3093e-01, 3.6869e-04,\n",
      "        5.4724e-08, 8.3791e-09, 2.5290e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0038766853976994753\n",
      " \n",
      "0.8906806707382202\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 27,    32/ 26 points] total loss per batch: 0.891\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.7551, 0.0408, 0.0816, 0.1224, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([7.5382e-03, 1.6487e-06, 7.9949e-01, 3.3388e-02, 5.6315e-02, 1.0159e-01,\n",
      "        1.6284e-03, 6.5980e-06, 3.5620e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.003529411042109132\n",
      " \n",
      "0.8895519971847534\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 28,    32/ 26 points] total loss per batch: 0.890\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0408, 0.0612, 0.8980, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([4.3063e-03, 4.4577e-06, 1.1480e-03, 2.6214e-02, 7.0919e-02, 8.9690e-01,\n",
      "        4.9574e-07, 4.7906e-07, 5.0962e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.006781764794141054\n",
      " \n",
      "0.8903968334197998\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 29,    32/ 26 points] total loss per batch: 0.890\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0376, 0.0012, 0.0616, 0.0418, 0.0476, 0.0692, 0.0587, 0.5909, 0.0914],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.31544890999794006\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:21 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8873463273048401\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 30,    32/ 26 points] total loss per batch: 0.887\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0408, 0.0612, 0.8980, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.3413e-03, 3.2954e-06, 9.6366e-04, 2.5046e-02, 6.3150e-02, 9.0810e-01,\n",
      "        3.0054e-07, 3.3474e-07, 3.9295e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.00666884146630764\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:21 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:38:21 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:38:21 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter6.pth.tar\n",
      "[I 200412 12:38:21 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:38:21 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 9.9999982e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.5928100e-16 0.0000000e+00 1.7102667e-07 1.6310374e-13] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 6.1958724e-01 0.0000000e+00 3.9884981e-05 3.8037282e-01] white = current\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 2.0796142e-16 2.0308732e-09 1.2574632e-08\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.7022416e-04 4.9991485e-01 0.0000000e+00\n",
      " 4.9991485e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [1.1397680e-02 0.0000000e+00 1.1130547e-05 9.8859119e-01 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 1. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9468002e-14\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:22 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 9.9999982e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.5928100e-16 0.0000000e+00 1.7102667e-07 1.6310374e-13] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 6.1958724e-01 0.0000000e+00 3.9884981e-05 3.8037282e-01] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 9.9999875e-01 1.2284384e-06] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 7.3767104e-08 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9992436e-01 0.0000000e+00 7.5537515e-05] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9981117e-01 1.8882476e-04 6.5279765e-16\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.5279765e-16] black = best\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5402373e-17] white = current\n",
      "Policy :  [0.         0.         0.         0.39933902 0.600661   0.\n",
      " 0.         0.         0.        ] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:23 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.         0.         0.         0.9982127  0.         0.\n",
      " 0.         0.         0.00178736] white = current\n",
      "current wins!\n",
      "Policy :  [2.0448617e-12 5.9148580e-01 2.0448617e-12 2.8190154e-01 2.0448617e-12\n",
      " 1.2074703e-07 2.0448617e-12 1.2074703e-07 1.2661244e-01] black = current\n",
      "Policy :  [1.7341530e-12 0.0000000e+00 9.9999994e-11 1.7341530e-12 1.7341530e-12\n",
      " 9.9999994e-11 9.9999994e-11 1.0000000e+00 1.7757726e-09] white = best\n",
      "Policy :  [7.0290559e-15 0.0000000e+00 7.0290559e-15 7.1977532e-12 7.1977532e-12\n",
      " 7.7285287e-03 7.0290559e-15 0.0000000e+00 9.9227148e-01] black = current\n",
      "Policy :  [6.2859378e-08 0.0000000e+00 9.9991500e-01 3.5398324e-09 3.6247884e-06\n",
      " 6.4368003e-05 1.6933649e-05 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [2.8603776e-11 0.0000000e+00 0.0000000e+00 5.0793802e-10 5.0793802e-10\n",
      " 1.0000000e+00 5.0793802e-10 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [1.4715281e-04 0.0000000e+00 0.0000000e+00 2.4920457e-09 2.4920457e-09\n",
      " 0.0000000e+00 9.9985290e-01 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.14621492 0.         0.         0.81531954 0.03846557 0.\n",
      " 0.         0.         0.        ] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:24 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.0794613e-11\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [2.6516810e-12 7.6701111e-01 2.6516810e-12 6.8777777e-02 2.6516810e-12\n",
      " 2.5895322e-05 2.6516810e-12 1.5657911e-07 1.6418509e-01] black = current\n",
      "Policy :  [1.7341530e-12 0.0000000e+00 9.9999994e-11 1.7341530e-12 1.7341530e-12\n",
      " 9.9999994e-11 9.9999994e-11 1.0000000e+00 1.7757726e-09] white = best\n",
      "Policy :  [7.0549849e-15 0.0000000e+00 7.0549849e-15 7.2243045e-12 4.1658979e-10\n",
      " 4.0682596e-03 7.0549849e-15 0.0000000e+00 9.9593174e-01] black = current\n",
      "Policy :  [9.0195095e-07 0.0000000e+00 5.3259302e-02 5.0792028e-08 5.2011037e-05\n",
      " 9.2359778e-04 9.4576412e-01 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [9.7559899e-04 0.0000000e+00 1.0624902e-05 3.6731996e-07 3.9440682e-08\n",
      " 9.9901336e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [1.5261731e-03 0.0000000e+00 9.8145390e-01 3.4744776e-09 1.7019913e-02\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.20956534 0.         0.         0.68052596 0.10990874 0.\n",
      " 0.         0.         0.        ] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6538173e-08 1.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:25 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 9.9999905e-01 0.0000000e+00 2.7351086e-16 0.0000000e+00\n",
      " 2.7351086e-16 0.0000000e+00 9.5367341e-07 2.8007512e-13] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 3.8037282e-01 0.0000000e+00 3.9884981e-05 6.1958724e-01] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 9.9999875e-01 1.2284384e-06] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.5367045e-07 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999595e-01 0.0000000e+00 3.0968743e-06] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9902421e-01 9.7560958e-04 3.3728395e-15\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.0394270e-07] black = best\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5402373e-17] white = current\n",
      "Policy :  [0.         0.         0.         0.8857397  0.11426028 0.\n",
      " 0.         0.         0.        ] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:26 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:38:26 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:38:26 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:38:26 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter6.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7978726e-09\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = best\n",
      "None wins!\n",
      "Current_net wins ratio 0.40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:29 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:38:29 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:38:29 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_6/.\n",
      "[I 200412 12:38:29 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "3.237311363220215\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 1,    32/ 26 points] total loss per batch: 3.237\n",
      "Policy (actual, predicted): 4 1\n",
      "Policy data: tensor([0.0816, 0.1837, 0.0612, 0.0816, 0.2449, 0.1633, 0.0612, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0804, 0.1416, 0.0852, 0.1100, 0.0978, 0.1231, 0.1404, 0.1072, 0.1143],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 0.08194777369499207\n",
      " \n",
      "2.7363333702087402\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 2,    32/ 26 points] total loss per batch: 2.736\n",
      "Policy (actual, predicted): 5 4\n",
      "Policy data: tensor([0.2857, 0.0000, 0.2449, 0.0408, 0.0408, 0.3878, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1353, 0.1925, 0.0893, 0.1448, 0.2158, 0.0717, 0.0662, 0.0678, 0.0166],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.20293036103248596\n",
      " \n",
      "2.346782922744751\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 3,    32/ 26 points] total loss per batch: 2.347\n",
      "Policy (actual, predicted): 2 0\n",
      "Policy data: tensor([0.2245, 0.0000, 0.4286, 0.0612, 0.2857, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.3654, 0.0095, 0.2404, 0.1861, 0.0916, 0.0139, 0.0510, 0.0166, 0.0254],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.7611725330352783\n",
      " \n",
      "2.38041090965271\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 4,    32/ 26 points] total loss per batch: 2.380\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([8.5243e-05, 7.5331e-06, 1.0905e-04, 9.4419e-01, 5.5465e-02, 1.1537e-04,\n",
      "        1.0435e-05, 1.0105e-05, 1.2302e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9945279955863953\n",
      " \n",
      "1.8676419258117676\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 5,    32/ 26 points] total loss per batch: 1.868\n",
      "Policy (actual, predicted): 3 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2449, 0.5918, 0.0000, 0.0000, 0.0000, 0.0000, 0.1633])\n",
      "Policy pred: tensor([0.0464, 0.1101, 0.0619, 0.0929, 0.2000, 0.0594, 0.1924, 0.1125, 0.1244],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.20731215178966522\n",
      " \n",
      "1.6163837909698486\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 6,    32/ 26 points] total loss per batch: 1.616\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0612, 0.0408, 0.0408, 0.0612, 0.0612, 0.6122, 0.0816])\n",
      "Policy pred: tensor([0.0409, 0.0754, 0.1030, 0.0354, 0.0588, 0.0747, 0.1843, 0.3533, 0.0742],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.26343807578086853\n",
      " \n",
      "1.4417444467544556\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 7,    32/ 26 points] total loss per batch: 1.442\n",
      "Policy (actual, predicted): 4 7\n",
      "Policy data: tensor([0.0816, 0.1837, 0.0612, 0.0816, 0.2449, 0.1633, 0.0612, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0930, 0.0858, 0.1502, 0.0443, 0.0956, 0.0897, 0.0901, 0.2680, 0.0833],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.8116075992584229\n",
      " \n",
      "1.3313418626785278\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 8,    32/ 26 points] total loss per batch: 1.331\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9184, 0.0000, 0.0816])\n",
      "Policy pred: tensor([5.8406e-03, 4.0914e-04, 2.6850e-02, 5.4601e-02, 2.4081e-01, 1.0824e-03,\n",
      "        6.3758e-01, 2.8347e-03, 2.9992e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.8190123438835144\n",
      " \n",
      "1.2601460218429565\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 9,    32/ 26 points] total loss per batch: 1.260\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0204, 0.8980, 0.0204, 0.0000, 0.0000, 0.0204, 0.0408, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.2243e-02, 7.3502e-01, 9.6225e-03, 2.0852e-03, 2.9224e-03, 1.0448e-01,\n",
      "        6.5548e-04, 4.3306e-02, 8.9670e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.8473520874977112\n",
      " \n",
      "1.245225429534912\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 10,    32/ 26 points] total loss per batch: 1.245\n",
      "Policy (actual, predicted): 6 2\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0612, 0.1224, 0.1633, 0.3265, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0984, 0.0048, 0.3167, 0.0438, 0.1787, 0.1632, 0.1675, 0.0093, 0.0176],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9580097794532776\n",
      " \n",
      "1.167982578277588\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 11,    32/ 26 points] total loss per batch: 1.168\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([4.2422e-02, 9.6961e-09, 1.0515e-05, 9.5429e-01, 3.2392e-03, 3.7231e-05,\n",
      "        4.3690e-07, 9.0730e-11, 7.1639e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9866673350334167\n",
      " \n",
      "1.143812656402588\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 12,    32/ 26 points] total loss per batch: 1.144\n",
      "Policy (actual, predicted): 6 2\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0612, 0.1224, 0.1633, 0.3265, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0975, 0.0036, 0.2696, 0.0893, 0.1653, 0.1016, 0.2453, 0.0078, 0.0200],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.8876475095748901\n",
      " \n",
      "1.091401219367981\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 13,    32/ 26 points] total loss per batch: 1.091\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.7755, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1633, 0.0408])\n",
      "Policy pred: tensor([7.6741e-04, 8.0292e-01, 8.1498e-04, 7.0585e-04, 3.5040e-04, 7.0765e-03,\n",
      "        1.1840e-04, 1.3199e-01, 5.5263e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.14595727622509003\n",
      " \n",
      "1.0750439167022705\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 14,    32/ 26 points] total loss per batch: 1.075\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.2245, 0.0000, 0.4286, 0.0612, 0.2857, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.7339e-01, 2.0345e-04, 4.2655e-01, 4.0270e-02, 2.7720e-01, 4.9501e-02,\n",
      "        3.1368e-02, 1.9595e-04, 1.3145e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9881537556648254\n",
      " \n",
      "1.0566747188568115\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 15,    32/ 26 points] total loss per batch: 1.057\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.2245, 0.0000, 0.4286, 0.0612, 0.2857, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.0171e-01, 1.4304e-04, 3.9118e-01, 3.7810e-02, 3.0085e-01, 3.7479e-02,\n",
      "        2.9928e-02, 1.0725e-04, 7.9155e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9920043349266052\n",
      " \n",
      "1.0251001119613647\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 16,    32/ 26 points] total loss per batch: 1.025\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2449, 0.5918, 0.0000, 0.0000, 0.0000, 0.0000, 0.1633])\n",
      "Policy pred: tensor([5.4349e-03, 5.6784e-04, 1.5964e-01, 6.1183e-01, 5.3712e-02, 8.4626e-04,\n",
      "        6.6566e-03, 9.0187e-03, 1.5230e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9897388219833374\n",
      " \n",
      "1.0216642618179321\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 17,    32/ 26 points] total loss per batch: 1.022\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0612, 0.1224, 0.1633, 0.3265, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0963, 0.0019, 0.2511, 0.0662, 0.1324, 0.0946, 0.3400, 0.0058, 0.0117],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9605993032455444\n",
      " \n",
      "1.0109318494796753\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 18,    32/ 26 points] total loss per batch: 1.011\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0612, 0.0000, 0.2857, 0.0612, 0.0612, 0.0000, 0.4286, 0.0000, 0.1020])\n",
      "Policy pred: tensor([0.0222, 0.0004, 0.2688, 0.1212, 0.0835, 0.0030, 0.3624, 0.0096, 0.1289],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9506661295890808\n",
      " \n",
      "0.9941447377204895\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 19,    32/ 26 points] total loss per batch: 0.994\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0816, 0.0000, 0.2449, 0.0612, 0.1224, 0.1633, 0.3265, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1008, 0.0018, 0.2180, 0.0675, 0.1296, 0.1802, 0.2866, 0.0052, 0.0102],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9645180106163025\n",
      " \n",
      "0.9905077815055847\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 20,    32/ 26 points] total loss per batch: 0.991\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9184, 0.0000, 0.0816])\n",
      "Policy pred: tensor([1.0506e-03, 5.4358e-07, 2.8621e-03, 4.3789e-03, 2.5223e-02, 9.3622e-06,\n",
      "        9.3442e-01, 7.8457e-06, 3.2052e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9861079454421997\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9823071956634521\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 21,    32/ 26 points] total loss per batch: 0.982\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.2245, 0.0000, 0.4286, 0.0612, 0.2857, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.0776e-01, 9.4918e-06, 4.4331e-01, 6.0664e-02, 2.7809e-01, 6.8406e-03,\n",
      "        3.2905e-03, 9.2679e-06, 2.1856e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9978716969490051\n",
      " \n",
      "0.9787963628768921\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 22,    32/ 26 points] total loss per batch: 0.979\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([6.8897e-04, 8.8632e-12, 5.3221e-08, 9.9405e-01, 5.2651e-03, 2.0129e-08,\n",
      "        3.4087e-09, 1.1285e-13, 3.2833e-09], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9988019466400146\n",
      " \n",
      "0.9745273590087891\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 23,    32/ 26 points] total loss per batch: 0.975\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.7755, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1633, 0.0408])\n",
      "Policy pred: tensor([0.0010, 0.7069, 0.0013, 0.0015, 0.0017, 0.0132, 0.0007, 0.2211, 0.0526],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.015697069466114044\n",
      " \n",
      "0.9730227589607239\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 24,    32/ 26 points] total loss per batch: 0.973\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.7755, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1633, 0.0408])\n",
      "Policy pred: tensor([8.0012e-04, 8.3204e-01, 9.7501e-04, 1.0764e-03, 1.2062e-03, 9.4731e-03,\n",
      "        5.8520e-04, 1.2271e-01, 3.1136e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.00314990128390491\n",
      " \n",
      "0.9780142307281494\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 25,    32/ 26 points] total loss per batch: 0.978\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.7959, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1633, 0.0408])\n",
      "Policy pred: tensor([6.9441e-04, 7.8814e-01, 9.1419e-04, 1.0895e-03, 1.1664e-03, 8.0530e-03,\n",
      "        6.6349e-04, 1.5756e-01, 4.1723e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.04551516845822334\n",
      " \n",
      "1.0109375715255737\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 26,    32/ 26 points] total loss per batch: 1.011\n",
      "Policy (actual, predicted): 4 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.8776, 0.0000, 0.1224, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.7492e-04, 4.4225e-09, 1.3285e-04, 4.6116e-03, 4.5828e-01, 7.4537e-08,\n",
      "        5.2196e-01, 8.7074e-09, 1.4740e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9989381432533264\n",
      " \n",
      "1.1131951808929443\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 27,    32/ 26 points] total loss per batch: 1.113\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.2245, 0.0000, 0.4286, 0.0612, 0.2857, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.6180e-01, 8.5630e-06, 6.0168e-01, 5.1735e-02, 1.7156e-01, 1.0758e-02,\n",
      "        2.4458e-03, 1.3547e-05, 6.6903e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9983406066894531\n",
      " \n",
      "1.0041863918304443\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 28,    32/ 26 points] total loss per batch: 1.004\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.9184, 0.0000, 0.0000, 0.0408, 0.0408, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([9.1446e-01, 2.2649e-06, 1.6376e-03, 4.8283e-02, 3.4914e-02, 6.8873e-04,\n",
      "        1.8135e-05, 1.3786e-08, 1.0808e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9993371963500977\n",
      " \n",
      "1.0288211107254028\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 29,    32/ 26 points] total loss per batch: 1.029\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.7755, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1633, 0.0408])\n",
      "Policy pred: tensor([0.0010, 0.7196, 0.0013, 0.0015, 0.0013, 0.0098, 0.0019, 0.1707, 0.0928],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.13215948641300201\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:32 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9985764622688293\n",
      "Update step size: 1\n",
      "[Iteration 6] Process ID: 11022 [Epoch: 30,    32/ 26 points] total loss per batch: 0.999\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.9184, 0.0000, 0.0000, 0.0408, 0.0408, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([9.1550e-01, 1.8962e-06, 2.5444e-03, 5.2139e-02, 2.8366e-02, 1.4207e-03,\n",
      "        2.3296e-05, 1.1252e-08, 1.3035e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9995741248130798\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:32 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:38:32 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:38:32 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter6.pth.tar\n",
      "[I 200412 12:38:32 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:38:32 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = current\n",
      "Policy :  [7.3594238e-06 2.4471974e-02 4.1443502e-07 4.1443502e-07 9.6755898e-01\n",
      " 7.5360499e-03 4.1443502e-07 4.2438146e-04 0.0000000e+00] white = best\n",
      "Policy :  [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 1.391984e-12 0.000000e+00 0.000000e+00 0.000000e+00] black = current\n",
      "Policy :  [2.8007539e-13 0.0000000e+00 1.6150558e-11 1.6150558e-11 0.0000000e+00\n",
      " 1.6150558e-11 2.8007539e-13 1.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [9.9972075e-01 0.0000000e+00 0.0000000e+00 2.7925576e-04 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.5263547e-07 1.5263547e-07 0.0000000e+00\n",
      " 9.0129720e-03 9.9098670e-01 0.0000000e+00 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:33 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = current\n",
      "Policy :  [0.000000e+00 0.000000e+00 9.999997e-01 0.000000e+00 0.000000e+00\n",
      " 3.325256e-07 0.000000e+00 0.000000e+00 0.000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 1. 0. 0. 0.] black = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 7.0305652e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.2192075e-15 0.0000000e+00 1.2484685e-12 9.9929690e-01] black = current\n",
      "Policy :  [7.4861568e-06 7.6658246e-03 4.2157180e-07 7.4861568e-06 9.8422080e-01\n",
      " 7.6658246e-03 4.2157180e-07 4.3168952e-04 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 9.9999875e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.2284384e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [2.8007539e-13 0.0000000e+00 1.6150558e-11 1.6150558e-11 0.0000000e+00\n",
      " 1.6150558e-11 2.8007539e-13 1.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [9.9972075e-01 0.0000000e+00 0.0000000e+00 2.7925576e-04 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.4934228e-07 1.5332408e-07 0.0000000e+00\n",
      " 4.5414097e-03 9.9545753e-01 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = current\n",
      "Policy :  [0.000000e+00 0.000000e+00 9.999997e-01 0.000000e+00 0.000000e+00\n",
      " 3.325256e-07 0.000000e+00 0.000000e+00 0.000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 1. 0. 0. 0.] black = current\n",
      "current wins!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:34 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 9.9999982e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.5928100e-16 0.0000000e+00 1.7102667e-07 1.6310374e-13] black = best\n",
      "Policy :  [1.7757726e-09 0.0000000e+00 1.6935088e-15 9.9999994e-11 1.6935088e-15\n",
      " 1.0000000e+00 1.7341530e-12 9.9999994e-11 1.6538172e-08] white = current\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 9.999997e-01 3.325256e-07] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.8384134e-07 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 4.9999991e-01 0.0000000e+00 4.9999991e-01] white = current\n",
      "Policy :  [0.         0.         0.00178736 0.9982127  0.         0.\n",
      " 0.         0.         0.        ] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 7.9994706e-08 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999994e-01 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [1.7341530e-12 0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9368032e-17\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [0.01027631 0.         0.         0.         0.9897237  0.\n",
      " 0.         0.         0.        ] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:35 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 9.9999642e-01 0.0000000e+00 3.6250832e-16 0.0000000e+00\n",
      " 3.6250832e-16 0.0000000e+00 3.6250833e-06 3.7120852e-13] black = best\n",
      "Policy :  [1.7757726e-09 0.0000000e+00 1.6935088e-15 9.9999994e-11 1.6935088e-15\n",
      " 1.0000000e+00 1.7341530e-12 9.9999994e-11 1.6538172e-08] white = current\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 9.999997e-01 3.325256e-07] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.8384134e-07 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 4.9999991e-01 0.0000000e+00 4.9999991e-01] white = current\n",
      "Policy :  [0.         0.         0.00178736 0.9982127  0.         0.\n",
      " 0.         0.         0.        ] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.9468002e-14 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [1.7341530e-12 0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9368032e-17\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:36 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [4.1313224e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9999589e-01\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 9.9999976e-01 0.0000000e+00 2.0796137e-16 0.0000000e+00\n",
      " 2.0796137e-16 0.0000000e+00 2.2329682e-07 2.1295244e-13] black = best\n",
      "Policy :  [1.7757726e-09 0.0000000e+00 1.6935088e-15 9.9999994e-11 1.6935088e-15\n",
      " 1.0000000e+00 1.7341530e-12 9.9999994e-11 1.6538172e-08] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 9.9999875e-01 1.2284384e-06] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.5554787e-09 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 9.6311146e-09] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9964726e-01 3.5278508e-04 1.2196349e-15\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.2018218e-11] black = best\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:37 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:38:37 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:38:37 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter7.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5402373e-17 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 1.391984e-12] white = current\n",
      "current wins!\n",
      "Current_net wins ratio 0.60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:40 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:38:40 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:38:40 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_7/.\n",
      "[I 200412 12:38:40 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.2510790824890137\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 1,    32/ 27 points] total loss per batch: 2.251\n",
      "Policy (actual, predicted): 7 6\n",
      "Policy data: tensor([0.1837, 0.0000, 0.0204, 0.0408, 0.0816, 0.0000, 0.1224, 0.2857, 0.2653])\n",
      "Policy pred: tensor([0.1101, 0.1202, 0.0750, 0.1099, 0.1101, 0.0701, 0.1804, 0.1191, 0.1050],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.2477322220802307\n",
      " \n",
      "2.1888065338134766\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 2,    32/ 27 points] total loss per batch: 2.189\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0204, 0.0612, 0.0204, 0.6122, 0.0408, 0.0612, 0.1020])\n",
      "Policy pred: tensor([0.0530, 0.1546, 0.0505, 0.0622, 0.0503, 0.3729, 0.0259, 0.0160, 0.2145],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.1635606288909912\n",
      " \n",
      "2.1191112995147705\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 3,    32/ 27 points] total loss per batch: 2.119\n",
      "Policy (actual, predicted): 1 6\n",
      "Policy data: tensor([0.0000, 0.9184, 0.0000, 0.0000, 0.0000, 0.0816, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0679, 0.0687, 0.0863, 0.1793, 0.1309, 0.0188, 0.2618, 0.0961, 0.0903],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.065970778465271\n",
      " \n",
      "1.9751869440078735\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 4,    32/ 27 points] total loss per batch: 1.975\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.1181, 0.0412, 0.1069, 0.0327, 0.0190, 0.4494, 0.0803, 0.0637, 0.0886],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.04098116233944893\n",
      " \n",
      "1.726805329322815\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 5,    32/ 27 points] total loss per batch: 1.727\n",
      "Policy (actual, predicted): 5 1\n",
      "Policy data: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "Policy pred: tensor([0.0206, 0.3301, 0.0404, 0.1157, 0.0539, 0.1431, 0.0932, 0.0792, 0.1238],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.07420799881219864\n",
      " \n",
      "1.5164306163787842\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 6,    32/ 27 points] total loss per batch: 1.516\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.2449, 0.0000, 0.0612, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6531])\n",
      "Policy pred: tensor([0.0716, 0.0601, 0.1012, 0.0475, 0.0525, 0.0707, 0.0674, 0.0923, 0.4366],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0455632284283638\n",
      " \n",
      "1.378928542137146\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 7,    32/ 27 points] total loss per batch: 1.379\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0408, 0.0408, 0.0000, 0.8367, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.1651, 0.0295, 0.1131, 0.0514, 0.0039, 0.4326, 0.1151, 0.0564, 0.0329],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.035241883248090744\n",
      " \n",
      "1.2030466794967651\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 8,    32/ 27 points] total loss per batch: 1.203\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.2449, 0.0000, 0.0612, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6531])\n",
      "Policy pred: tensor([0.1351, 0.0310, 0.0602, 0.0215, 0.0368, 0.0262, 0.0455, 0.0993, 0.5443],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.04468625783920288\n",
      " \n",
      "1.0972554683685303\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 9,    32/ 27 points] total loss per batch: 1.097\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0816, 0.2245, 0.1020, 0.1633, 0.1633, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0847, 0.2522, 0.0476, 0.1098, 0.0316, 0.0462, 0.0447, 0.1603, 0.2228],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.024625448510050774\n",
      " \n",
      "0.9891733527183533\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 10,    32/ 27 points] total loss per batch: 0.989\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.2393, 0.0185, 0.0222, 0.1162, 0.0094, 0.5010, 0.0306, 0.0368, 0.0259],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.04097602516412735\n",
      " \n",
      "0.9410409331321716\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 11,    32/ 27 points] total loss per batch: 0.941\n",
      "Policy (actual, predicted): 3 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.3061, 0.3061, 0.0000, 0.0000, 0.2041, 0.1837, 0.0000])\n",
      "Policy pred: tensor([0.0022, 0.0112, 0.1323, 0.1932, 0.0095, 0.0479, 0.3924, 0.2083, 0.0030],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.004005097784101963\n",
      " \n",
      "0.9539147615432739\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 12,    32/ 27 points] total loss per batch: 0.954\n",
      "Policy (actual, predicted): 1 7\n",
      "Policy data: tensor([0.0816, 0.2245, 0.1020, 0.1633, 0.1633, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0584, 0.0613, 0.1067, 0.1592, 0.1408, 0.0116, 0.1236, 0.1750, 0.1634],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.003777202917262912\n",
      " \n",
      "0.895578920841217\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 13,    32/ 27 points] total loss per batch: 0.896\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "Policy pred: tensor([1.0100e-03, 1.5970e-02, 2.4175e-03, 2.9597e-02, 9.3456e-04, 9.4143e-01,\n",
      "        3.4592e-03, 4.9991e-03, 1.7817e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.03017777018249035\n",
      " \n",
      "0.832542896270752\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 14,    32/ 27 points] total loss per batch: 0.833\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0408, 0.0408, 0.0000, 0.8367, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0203, 0.0155, 0.0369, 0.0369, 0.0083, 0.8352, 0.0150, 0.0285, 0.0035],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.010644897818565369\n",
      " \n",
      "0.8444594144821167\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 15,    32/ 27 points] total loss per batch: 0.844\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0612, 0.0000, 0.0000, 0.0408, 0.8980])\n",
      "Policy pred: tensor([1.3910e-03, 6.1151e-03, 7.0743e-03, 3.0254e-03, 4.0927e-02, 3.0668e-05,\n",
      "        1.0115e-03, 2.6967e-02, 9.1346e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.030373675748705864\n",
      " \n",
      "0.8100365996360779\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 16,    32/ 27 points] total loss per batch: 0.810\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0204, 0.0612, 0.0204, 0.6122, 0.0408, 0.0612, 0.1020])\n",
      "Policy pred: tensor([0.1006, 0.0315, 0.0357, 0.0400, 0.0259, 0.6151, 0.0311, 0.0565, 0.0635],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.01010778360068798\n",
      " \n",
      "0.7924888134002686\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 17,    32/ 27 points] total loss per batch: 0.792\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1837, 0.0000, 0.0204, 0.0408, 0.0816, 0.0000, 0.1224, 0.2857, 0.2653])\n",
      "Policy pred: tensor([0.1243, 0.0502, 0.0534, 0.1576, 0.0893, 0.0107, 0.0909, 0.2299, 0.1938],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.007355840411037207\n",
      " \n",
      "0.8014408946037292\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 18,    32/ 27 points] total loss per batch: 0.801\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([0.0030, 0.2150, 0.0032, 0.0055, 0.0055, 0.0032, 0.0023, 0.0493, 0.7131],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.012910796329379082\n",
      " \n",
      "0.7781051397323608\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 19,    32/ 27 points] total loss per batch: 0.778\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0750, 0.0316, 0.0334, 0.0411, 0.0376, 0.7053, 0.0307, 0.0313, 0.0141],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.001012012013234198\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7747229337692261\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 20,    32/ 27 points] total loss per batch: 0.775\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2653, 0.0408, 0.0000, 0.0000, 0.6939, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.4457e-03, 8.6054e-03, 2.0639e-01, 1.9118e-02, 1.0414e-03, 2.5610e-04,\n",
      "        7.5237e-01, 9.7052e-03, 6.0491e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.01906726323068142\n",
      " \n",
      "0.7716915607452393\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 21,    32/ 27 points] total loss per batch: 0.772\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2653, 0.0408, 0.0000, 0.0000, 0.6939, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.4850e-03, 8.1572e-03, 2.7800e-01, 4.1525e-02, 8.0986e-04, 3.0221e-04,\n",
      "        6.6111e-01, 8.5587e-03, 5.4990e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.030716506764292717\n",
      " \n",
      "0.7611488699913025\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 22,    32/ 27 points] total loss per batch: 0.761\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.3061, 0.3061, 0.0000, 0.0000, 0.2041, 0.1837, 0.0000])\n",
      "Policy pred: tensor([2.7779e-04, 1.6150e-03, 3.1441e-01, 3.3719e-01, 1.0033e-03, 5.6376e-03,\n",
      "        1.9437e-01, 1.4534e-01, 1.6644e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.010230601765215397\n",
      " \n",
      "0.7609689235687256\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 23,    32/ 27 points] total loss per batch: 0.761\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([0.0016, 0.3184, 0.0012, 0.0019, 0.0018, 0.0100, 0.0013, 0.0431, 0.6208],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.009952097199857235\n",
      " \n",
      "0.7579306364059448\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 24,    32/ 27 points] total loss per batch: 0.758\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0408, 0.0000, 0.0408, 0.0408, 0.0000, 0.8367, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0206, 0.0077, 0.0234, 0.0424, 0.0074, 0.8504, 0.0217, 0.0235, 0.0029],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.013293958269059658\n",
      " \n",
      "0.7521387934684753\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 25,    32/ 27 points] total loss per batch: 0.752\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4898, 0.5102, 0.0000])\n",
      "Policy pred: tensor([2.6563e-05, 2.3395e-05, 3.5543e-03, 3.0122e-03, 2.1227e-03, 3.0154e-05,\n",
      "        4.8281e-01, 5.0834e-01, 8.1229e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.008229680359363556\n",
      " \n",
      "0.7537396550178528\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 26,    32/ 27 points] total loss per batch: 0.754\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([1.2314e-09, 8.0866e-07, 4.1792e-06, 2.6815e-06, 9.9847e-01, 1.1663e-09,\n",
      "        5.9302e-04, 8.2859e-04, 1.0001e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.011871435679495335\n",
      " \n",
      "0.750224232673645\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 27,    32/ 27 points] total loss per batch: 0.750\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4898, 0.5102, 0.0000])\n",
      "Policy pred: tensor([1.6113e-05, 1.2083e-05, 2.4141e-03, 1.7937e-03, 1.4010e-03, 1.7588e-05,\n",
      "        4.3038e-01, 5.6391e-01, 5.3917e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0034171880688518286\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:43 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7479384541511536\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 28,    32/ 27 points] total loss per batch: 0.748\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0204, 0.0612, 0.0204, 0.6122, 0.0408, 0.0612, 0.1020])\n",
      "Policy pred: tensor([0.0802, 0.0105, 0.0164, 0.0463, 0.0180, 0.6675, 0.0283, 0.0559, 0.0769],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.003179286140948534\n",
      " \n",
      "0.7481439113616943\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 29,    32/ 27 points] total loss per batch: 0.748\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.9796, 0.0000, 0.0000, 0.0204, 0.0000])\n",
      "Policy pred: tensor([1.4600e-07, 3.6453e-06, 2.1227e-04, 2.3450e-05, 9.8346e-01, 3.2224e-08,\n",
      "        1.6685e-03, 1.3387e-02, 1.2453e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.00302199088037014\n",
      " \n",
      "0.7467918395996094\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 30,    32/ 27 points] total loss per batch: 0.747\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0816, 0.2245, 0.1020, 0.1633, 0.1633, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0654, 0.2309, 0.1085, 0.1768, 0.1380, 0.0013, 0.1545, 0.1200, 0.0046],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.008207427337765694\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:43 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:38:43 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter8.pth.tar\n",
      "[I 200412 12:38:43 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:38:43 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:38:43 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [7.0693183e-15 2.0448333e-03 7.0693183e-15 7.0693183e-15 7.0693183e-15\n",
      " 7.0693183e-15 7.0693183e-15 4.1743617e-10 9.9795514e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [9.8961997e-01 7.7078771e-03 7.0102731e-05 7.0102731e-05 7.0102731e-05\n",
      " 0.0000000e+00 2.0277542e-03 4.3405764e-04 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 2.8603776e-11 4.7305417e-09 2.8603776e-11\n",
      " 0.0000000e+00 2.8603776e-11 4.8440745e-16 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.6900456e-05 9.9795502e-01 7.8952580e-05\n",
      " 0.0000000e+00 9.7456545e-04 9.7456545e-04 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.4699728e-01 0.0000000e+00 3.6564294e-02\n",
      " 0.0000000e+00 1.6422382e-02 1.6037482e-05 0.0000000e+00] white = best\n",
      "Policy :  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 2.08995174e-04 0.00000000e+00 1.17692525e-05 9.99779284e-01\n",
      " 0.00000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.1737915e-04\n",
      " 0.0000000e+00 9.9928260e-01 0.0000000e+00 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:44 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [1.0454052e-14 3.0238833e-03 1.0454052e-14 1.0454052e-14 1.0454052e-14\n",
      " 1.0704949e-11 1.0454052e-14 6.1730132e-10 9.9697608e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [9.8961997e-01 7.7078771e-03 7.0102731e-05 7.0102731e-05 7.0102731e-05\n",
      " 0.0000000e+00 2.0277542e-03 4.3405764e-04 0.0000000e+00] black = current\n",
      "Policy :  [1.0000000e+00 1.2579225e-13 1.2579225e-13 1.2579225e-13 1.2579225e-13\n",
      " 0.0000000e+00 0.0000000e+00 1.2579225e-13 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 1.5898628e-06 1.7480727e-04 9.9814957e-01 4.5987526e-05\n",
      " 0.0000000e+00 0.0000000e+00 1.6280195e-03 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 2.7826142e-01 7.2173846e-01 0.0000000e+00 1.6045957e-09\n",
      " 0.0000000e+00 0.0000000e+00 2.8493970e-08 0.0000000e+00] white = best\n",
      "Policy :  [0.         0.00692366 0.         0.         0.00148207 0.\n",
      " 0.         0.99159425 0.        ] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:46 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.         0.02376161 0.         0.         0.97623837 0.\n",
      " 0.         0.         0.        ] white = best\n",
      "best wins!\n",
      "Policy :  [1.0454052e-14 3.0238833e-03 1.0454052e-14 1.0454052e-14 1.0454052e-14\n",
      " 1.0704949e-11 1.0454052e-14 6.1730132e-10 9.9697608e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [9.9083847e-01 7.7173673e-03 7.0189046e-05 4.3459208e-04 7.0189046e-05\n",
      " 0.0000000e+00 4.3459208e-04 4.3459208e-04 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 2.8603776e-11 4.7305417e-09 2.8603776e-11\n",
      " 0.0000000e+00 2.8603776e-11 4.8440745e-16 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.6900456e-05 9.9795502e-01 7.8952580e-05\n",
      " 0.0000000e+00 9.7456545e-04 9.7456545e-04 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.4699728e-01 0.0000000e+00 3.6564294e-02\n",
      " 0.0000000e+00 1.6422382e-02 1.6037482e-05 0.0000000e+00] white = best\n",
      "Policy :  [0.         0.         0.81531954 0.         0.         0.\n",
      " 0.03846557 0.14621492 0.        ] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:47 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.         0.         0.         0.         0.         0.\n",
      " 0.9897237  0.01027631 0.        ] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 9.7560976e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.7324611e-12 0.0000000e+00 1.7324611e-12 9.9902439e-01] black = best\n",
      "Policy :  [3.8554326e-11 6.6859105e-13 6.6859105e-13 3.8554326e-11 6.6859105e-13\n",
      " 1.0000000e+00 6.6859105e-13 6.6859105e-13 0.0000000e+00] white = current\n",
      "Policy :  [2.4649860e-01 6.3935387e-01 2.4072129e-04 2.6467586e-02 8.5948750e-02\n",
      " 0.0000000e+00 0.0000000e+00 1.4904828e-03 0.0000000e+00] black = best\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 1.1914700e-08 7.2043635e-11 1.2493469e-12\n",
      " 0.0000000e+00 7.2043635e-11 1.1914700e-08 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 6.0875464e-11 6.2336476e-08 8.1955083e-03\n",
      " 0.0000000e+00 9.9159718e-01 2.0728479e-04 0.0000000e+00] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.7414452e-01 2.2963869e-07 2.2963869e-07\n",
      " 0.0000000e+00 0.0000000e+00 2.5855018e-02 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9999726e-01 2.7351036e-06\n",
      " 0.0000000e+00 0.0000000e+00 1.6150513e-11 0.0000000e+00] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9999589e-01\n",
      " 0.0000000e+00 0.0000000e+00 4.1313224e-06 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:48 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "None wins!\n",
      "Policy :  [1.0454052e-14 3.0238833e-03 1.0454052e-14 1.0454052e-14 1.0454052e-14\n",
      " 1.0704949e-11 1.0454052e-14 6.1730132e-10 9.9697608e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [9.8961997e-01 7.7078771e-03 7.0102731e-05 7.0102731e-05 7.0102731e-05\n",
      " 0.0000000e+00 2.0277542e-03 4.3405764e-04 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 2.8603776e-11 4.7305417e-09 2.8603776e-11\n",
      " 0.0000000e+00 2.8603776e-11 4.8440745e-16 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.6881109e-05 9.9681270e-01 7.8862206e-05\n",
      " 0.0000000e+00 2.9977012e-04 2.7918268e-03 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.4699728e-01 0.0000000e+00 3.6564294e-02\n",
      " 0.0000000e+00 1.6422382e-02 1.6037482e-05 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.7557593e-05\n",
      " 0.0000000e+00 3.3757242e-05 9.9987870e-01 0.0000000e+00] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:49 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:38:49 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:38:49 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:38:49 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter7.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.1737915e-04\n",
      " 0.0000000e+00 9.9928260e-01 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Current_net wins ratio 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:52 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:38:52 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:38:52 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_7/.\n",
      "[I 200412 12:38:52 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.4273810386657715\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 1,    32/ 24 points] total loss per batch: 2.427\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.8980, 0.0000, 0.0408, 0.0408, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1860, 0.1379, 0.1318, 0.0843, 0.1189, 0.0901, 0.1161, 0.0414, 0.0937],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.02661740779876709\n",
      " \n",
      "2.2986364364624023\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 2,    32/ 24 points] total loss per batch: 2.299\n",
      "Policy (actual, predicted): 2 1\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([0.0679, 0.3144, 0.1927, 0.0135, 0.2433, 0.0781, 0.0312, 0.0070, 0.0519],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.46819183230400085\n",
      " \n",
      "2.2671549320220947\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 3,    32/ 24 points] total loss per batch: 2.267\n",
      "Policy (actual, predicted): 2 0\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([0.3376, 0.0020, 0.3366, 0.0496, 0.1691, 0.0034, 0.0057, 0.0078, 0.0883],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.2214844822883606\n",
      " \n",
      "2.202345609664917\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 4,    32/ 24 points] total loss per batch: 2.202\n",
      "Policy (actual, predicted): 8 4\n",
      "Policy data: tensor([0.1633, 0.0000, 0.1633, 0.2857, 0.0408, 0.0000, 0.0000, 0.0000, 0.3469])\n",
      "Policy pred: tensor([0.0323, 0.0231, 0.0701, 0.2472, 0.4267, 0.0950, 0.0210, 0.0551, 0.0295],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.13489103317260742\n",
      " \n",
      "1.8823703527450562\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 5,    32/ 24 points] total loss per batch: 1.882\n",
      "Policy (actual, predicted): 8 1\n",
      "Policy data: tensor([0.1633, 0.0000, 0.1633, 0.2857, 0.0408, 0.0000, 0.0000, 0.0000, 0.3469])\n",
      "Policy pred: tensor([0.0702, 0.1764, 0.0863, 0.1458, 0.0958, 0.0847, 0.0674, 0.1293, 0.1441],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.08667124807834625\n",
      " \n",
      "1.677356243133545\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 6,    32/ 24 points] total loss per batch: 1.677\n",
      "Policy (actual, predicted): 1 8\n",
      "Policy data: tensor([0.0000, 0.9796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0687, 0.2872, 0.0366, 0.0574, 0.0357, 0.0715, 0.0456, 0.0765, 0.3208],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.19540412724018097\n",
      " \n",
      "1.5003072023391724\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 7,    32/ 24 points] total loss per batch: 1.500\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([0.0899, 0.2570, 0.0290, 0.0595, 0.0233, 0.0943, 0.0369, 0.0506, 0.3596],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.10046584159135818\n",
      " \n",
      "1.360367774963379\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 8,    32/ 24 points] total loss per batch: 1.360\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.8367, 0.0000, 0.0408, 0.0000])\n",
      "Policy pred: tensor([0.0341, 0.0968, 0.0322, 0.0415, 0.0075, 0.7017, 0.0443, 0.0087, 0.0333],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.841353714466095\n",
      " \n",
      "1.2750376462936401\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 9,    32/ 24 points] total loss per batch: 1.275\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1020, 0.0000, 0.0000, 0.0000, 0.7755, 0.0000, 0.1224])\n",
      "Policy pred: tensor([0.1843, 0.1487, 0.0414, 0.1167, 0.0495, 0.0392, 0.2694, 0.0522, 0.0986],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.019041363149881363\n",
      " \n",
      "1.1697622537612915\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 10,    32/ 24 points] total loss per batch: 1.170\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.9184, 0.0000, 0.0204, 0.0000, 0.0612, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.3879, 0.0235, 0.0558, 0.2224, 0.2759, 0.0009, 0.0056, 0.0137, 0.0143],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.015444607473909855\n",
      " \n",
      "1.116072654724121\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 11,    32/ 24 points] total loss per batch: 1.116\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0204, 0.0612, 0.0204, 0.6122, 0.0408, 0.0612, 0.1020])\n",
      "Policy pred: tensor([0.0253, 0.1132, 0.0102, 0.0257, 0.0093, 0.6233, 0.1049, 0.0415, 0.0465],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.12493022531270981\n",
      " \n",
      "1.0377188920974731\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 12,    32/ 24 points] total loss per batch: 1.038\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([3.3173e-02, 6.8424e-05, 1.6506e-01, 1.3663e-02, 7.8792e-01, 1.1797e-06,\n",
      "        3.1581e-05, 6.4960e-05, 2.0121e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.032238904386758804\n",
      " \n",
      "0.9910008907318115\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 13,    32/ 24 points] total loss per batch: 0.991\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.9796, 0.0000, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.4097e-03, 8.5481e-05, 9.4800e-01, 1.0800e-02, 3.8578e-02, 1.2571e-04,\n",
      "        9.9132e-04, 1.2515e-05, 1.9711e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9849060773849487\n",
      " \n",
      "0.9471673369407654\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 14,    32/ 24 points] total loss per batch: 0.947\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0292, 0.0851, 0.0158, 0.0397, 0.0350, 0.7234, 0.0253, 0.0340, 0.0123],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.4739179015159607\n",
      " \n",
      "0.9231646656990051\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 15,    32/ 24 points] total loss per batch: 0.923\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.8980, 0.0000, 0.0408, 0.0408, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([8.0302e-01, 9.1723e-04, 8.0718e-02, 5.0172e-02, 4.1668e-02, 6.7795e-05,\n",
      "        1.4752e-02, 3.3230e-03, 5.3647e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.020958438515663147\n",
      " \n",
      "0.9090254902839661\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 16,    32/ 24 points] total loss per batch: 0.909\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([3.9416e-07, 2.0438e-09, 8.6790e-01, 1.0623e-04, 1.3200e-01, 1.1937e-11,\n",
      "        7.6206e-11, 1.2715e-08, 4.9278e-11], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.03618749976158142\n",
      " \n",
      "0.8869839310646057\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 17,    32/ 24 points] total loss per batch: 0.887\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([0.0072, 0.3495, 0.0025, 0.0053, 0.0022, 0.0097, 0.0008, 0.0312, 0.5915],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0036361839156597853\n",
      " \n",
      "0.8607059121131897\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 18,    32/ 24 points] total loss per batch: 0.861\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.8367, 0.0000, 0.0408, 0.0000])\n",
      "Policy pred: tensor([0.0036, 0.1626, 0.0043, 0.0204, 0.0140, 0.7710, 0.0057, 0.0161, 0.0023],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9815340042114258\n",
      " \n",
      "0.8553723692893982\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 19,    32/ 24 points] total loss per batch: 0.855\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.3061, 0.0000, 0.6939, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.7461e-04, 4.8705e-06, 3.4360e-01, 1.4174e-02, 6.4179e-01, 1.8802e-07,\n",
      "        2.7106e-06, 2.5327e-04, 6.9426e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.009597856551408768\n",
      " \n",
      "0.8668172955513\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 20,    32/ 24 points] total loss per batch: 0.867\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.9796, 0.0000, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([5.7995e-05, 3.2921e-05, 9.8180e-01, 2.9638e-03, 1.3421e-02, 2.2088e-05,\n",
      "        1.6870e-03, 1.1328e-05, 7.0220e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.990964412689209\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8974082469940186\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 21,    32/ 24 points] total loss per batch: 0.897\n",
      "Policy (actual, predicted): 8 3\n",
      "Policy data: tensor([0.1633, 0.0000, 0.1633, 0.2857, 0.0408, 0.0000, 0.0000, 0.0000, 0.3469])\n",
      "Policy pred: tensor([2.6489e-01, 1.3622e-03, 1.4998e-01, 2.9524e-01, 1.3258e-01, 7.4279e-05,\n",
      "        8.4914e-03, 3.4464e-02, 1.1291e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.00722105149179697\n",
      " \n",
      "0.8817372918128967\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 22,    32/ 24 points] total loss per batch: 0.882\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.9796, 0.0000, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.1454e-05, 5.9619e-05, 9.8293e-01, 3.2152e-03, 1.1523e-02, 3.5319e-05,\n",
      "        2.1996e-03, 8.3513e-06, 6.2164e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9916695356369019\n",
      " \n",
      "0.8479397892951965\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 23,    32/ 24 points] total loss per batch: 0.848\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([1.8865e-03, 4.3623e-07, 4.8031e-03, 2.6983e-03, 9.9061e-01, 1.7089e-09,\n",
      "        3.6692e-08, 5.9477e-06, 7.0859e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.004000362940132618\n",
      " \n",
      "0.8577840328216553\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 24,    32/ 24 points] total loss per batch: 0.858\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([4.4186e-09, 1.6516e-11, 9.9667e-01, 6.0952e-06, 3.3267e-03, 1.2333e-13,\n",
      "        1.5850e-12, 4.2656e-10, 6.9031e-13], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.009874927811324596\n",
      " \n",
      "0.8286058306694031\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 25,    32/ 24 points] total loss per batch: 0.829\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.4898, 0.2041, 0.0000, 0.1837, 0.0000, 0.0000, 0.0000, 0.1224, 0.0000])\n",
      "Policy pred: tensor([5.6291e-01, 1.2806e-01, 5.6543e-03, 1.7835e-01, 1.7805e-02, 2.5275e-04,\n",
      "        4.8269e-03, 9.2622e-02, 9.5284e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.017999881878495216\n",
      " \n",
      "0.8354058265686035\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 26,    32/ 24 points] total loss per batch: 0.835\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([2.3342e-03, 1.9622e-01, 4.1910e-04, 1.0061e-03, 4.3264e-04, 3.3172e-03,\n",
      "        2.2908e-04, 1.8570e-02, 7.7747e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0024107908830046654\n",
      " \n",
      "0.8221834301948547\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 27,    32/ 24 points] total loss per batch: 0.822\n",
      "Policy (actual, predicted): 4 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.4694, 0.5306, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([9.9333e-04, 1.2109e-03, 9.4430e-03, 5.4780e-01, 4.3172e-01, 3.8077e-06,\n",
      "        3.4726e-05, 8.6547e-03, 1.3444e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.02145390957593918\n",
      " \n",
      "0.8266501426696777\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 28,    32/ 24 points] total loss per batch: 0.827\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.8980, 0.0000, 0.0408, 0.0408, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([9.0034e-01, 3.6130e-05, 4.9536e-02, 2.4458e-02, 2.1441e-02, 3.6188e-06,\n",
      "        4.7718e-04, 2.1557e-04, 3.4922e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.005294627510011196\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:55 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8162891268730164\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 29,    32/ 24 points] total loss per batch: 0.816\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.4898, 0.2041, 0.0000, 0.1837, 0.0000, 0.0000, 0.0000, 0.1224, 0.0000])\n",
      "Policy pred: tensor([4.8407e-01, 2.0889e-01, 4.0197e-03, 1.6897e-01, 1.2286e-02, 2.3025e-04,\n",
      "        4.4538e-03, 1.1000e-01, 7.0809e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.010548096150159836\n",
      " \n",
      "0.81754070520401\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 30,    32/ 24 points] total loss per batch: 0.818\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.9796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000])\n",
      "Policy pred: tensor([7.3467e-04, 9.7861e-01, 1.7108e-04, 3.8546e-03, 8.4172e-04, 1.2917e-04,\n",
      "        9.0283e-05, 1.4254e-02, 1.3183e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.009878835640847683\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:56 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:38:56 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter8.pth.tar\n",
      "[I 200412 12:38:56 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:38:56 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:38:56 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [9.9999875e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.2284384e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 4.5047837e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.0000000e+00 0.0000000e+00 7.6288903e-14 0.0000000e+00] black = best\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] white = current\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 1. 0. 0.] white = current\n",
      "Policy :  [0.000000e+00 0.000000e+00 1.391984e-12 1.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00] black = best\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:57 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 1. 0. 0. 0. 0. 0. 0.] black = best\n",
      "None wins!\n",
      "Policy :  [5.8330767e-07 3.6983679e-03 6.1164240e-11 6.1164240e-11 3.5270384e-09\n",
      " 6.1164240e-11 5.9730703e-14 5.9730703e-14 9.9630105e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [5.9916475e-04 9.9939960e-01 3.5380079e-09 6.2826977e-08 5.8512182e-07\n",
      " 0.0000000e+00 5.9916475e-14 5.8512182e-07 0.0000000e+00] black = current\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 1.6310377e-13 1.5554787e-09 1.5928103e-16\n",
      " 0.0000000e+00 1.6310377e-13 1.5928103e-16 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 5.2445999e-11 9.5367341e-07 9.9999905e-01\n",
      " 0.0000000e+00 9.3132169e-10 9.0949383e-13 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 4.7382217e-14 2.7322975e-12 0.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 4.6271696e-17 0.0000000e+00] white = best\n",
      "Policy :  [0.         0.         0.74002737 0.24333298 0.         0.\n",
      " 0.         0.01663969 0.        ] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:58 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.5402373e-17 0.0000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [9.9999875e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.2284384e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 4.5047837e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.0000000e+00 0.0000000e+00 7.6288903e-14 0.0000000e+00] black = best\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] white = current\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 3.770441e-05 0.000000e+00\n",
      " 0.000000e+00 9.999623e-01 0.000000e+00 0.000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.9468002e-14 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:38:59 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [9.999997e-01 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 3.325256e-07 0.000000e+00 0.000000e+00 0.000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 4.5047837e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.0000000e+00 0.0000000e+00 7.6288903e-14 0.0000000e+00] black = best\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] white = current\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 1. 0. 0.] white = current\n",
      "Policy :  [0. 0. 1. 0. 0. 0. 0. 0. 0.] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:00 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7978726e-09\n",
      " 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 9.7560976e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.7324611e-12 0.0000000e+00 1.7324611e-12 9.9902439e-01] black = best\n",
      "Policy :  [9.999997e-01 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 3.325256e-07 0.000000e+00 0.000000e+00 0.000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 4.5047837e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.0000000e+00 0.0000000e+00 7.6288903e-14 0.0000000e+00] black = best\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] white = current\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:01 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:39:01 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:39:01 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:39:01 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter7.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2891805e-01\n",
      " 0.0000000e+00 8.7107605e-01 5.9396129e-06 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6538173e-08\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n",
      "best wins!\n",
      "Current_net wins ratio 0.40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:04 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:39:04 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:39:04 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_7/.\n",
      "[I 200412 12:39:04 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.448068141937256\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 1,    32/ 27 points] total loss per batch: 2.448\n",
      "Policy (actual, predicted): 1 2\n",
      "Policy data: tensor([0.0612, 0.2245, 0.1020, 0.1633, 0.1837, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.1100, 0.0602, 0.1682, 0.1321, 0.1220, 0.1041, 0.1066, 0.0786, 0.1182],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.1850147843360901\n",
      " \n",
      "2.4973974227905273\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 2,    32/ 27 points] total loss per batch: 2.497\n",
      "Policy (actual, predicted): 5 4\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.1730, 0.1813, 0.0334, 0.0392, 0.2357, 0.0670, 0.0722, 0.1779, 0.0203],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.25135329365730286\n",
      " \n",
      "2.3385584354400635\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 3,    32/ 27 points] total loss per batch: 2.339\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.5306, 0.2041, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.1974, 0.0953, 0.1188, 0.1866, 0.0334, 0.0879, 0.1782, 0.0295, 0.0730],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.011596644297242165\n",
      " \n",
      "1.9152638912200928\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 4,    32/ 27 points] total loss per batch: 1.915\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.1773, 0.1601, 0.0900, 0.0504, 0.0588, 0.2252, 0.1314, 0.0421, 0.0647],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.02754506841301918\n",
      " \n",
      "1.8086382150650024\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 5,    32/ 27 points] total loss per batch: 1.809\n",
      "Policy (actual, predicted): 6 1\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0408, 0.0816, 0.2653, 0.0000, 0.4286, 0.1837, 0.0000])\n",
      "Policy pred: tensor([0.0156, 0.3042, 0.0493, 0.0770, 0.2919, 0.0122, 0.0803, 0.1198, 0.0496],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0524309016764164\n",
      " \n",
      "1.6178152561187744\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 6,    32/ 27 points] total loss per batch: 1.618\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.6735, 0.0408, 0.0000, 0.1633, 0.0408, 0.0000, 0.0408, 0.0408, 0.0000])\n",
      "Policy pred: tensor([0.2658, 0.1470, 0.1040, 0.0312, 0.0820, 0.1572, 0.1235, 0.0308, 0.0586],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.004003406502306461\n",
      " \n",
      "1.5160092115402222\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 7,    32/ 27 points] total loss per batch: 1.516\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.7755, 0.0000, 0.0408, 0.1020, 0.0204, 0.0000, 0.0408, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.2951, 0.0830, 0.1100, 0.0327, 0.0860, 0.1891, 0.1516, 0.0278, 0.0245],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.03683972358703613\n",
      " \n",
      "1.3885935544967651\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 8,    32/ 27 points] total loss per batch: 1.389\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.6735, 0.0408, 0.0000, 0.1633, 0.0408, 0.0000, 0.0408, 0.0408, 0.0000])\n",
      "Policy pred: tensor([0.4804, 0.0697, 0.0469, 0.0270, 0.0356, 0.2347, 0.0748, 0.0204, 0.0105],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0069796801544725895\n",
      " \n",
      "1.3094308376312256\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 9,    32/ 27 points] total loss per batch: 1.309\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0612, 0.2245, 0.1020, 0.1837, 0.1633, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0908, 0.3464, 0.0407, 0.1086, 0.0968, 0.0091, 0.0999, 0.0751, 0.1326],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.04918709769845009\n",
      " \n",
      "1.2323156595230103\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 10,    32/ 27 points] total loss per batch: 1.232\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.2279, 0.0159, 0.0044, 0.0321, 0.0048, 0.6829, 0.0118, 0.0153, 0.0049],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.013500834815204144\n",
      " \n",
      "1.293060302734375\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 11,    32/ 27 points] total loss per batch: 1.293\n",
      "Policy (actual, predicted): 6 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0408, 0.0612, 0.0000, 0.0000, 0.8776, 0.0204, 0.0000])\n",
      "Policy pred: tensor([3.0994e-02, 2.3779e-01, 3.8743e-01, 4.4828e-02, 5.8393e-02, 1.0642e-03,\n",
      "        2.0233e-01, 3.7008e-02, 1.5588e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.013633308000862598\n",
      " \n",
      "1.1573270559310913\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 12,    32/ 27 points] total loss per batch: 1.157\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0612, 0.2245, 0.1020, 0.1837, 0.1633, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.1341, 0.2170, 0.0383, 0.2057, 0.1132, 0.0038, 0.1704, 0.0970, 0.0204],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.03027767688035965\n",
      " \n",
      "1.0958629846572876\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 13,    32/ 27 points] total loss per batch: 1.096\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0408, 0.0816, 0.2653, 0.0000, 0.4286, 0.1837, 0.0000])\n",
      "Policy pred: tensor([0.0335, 0.0892, 0.0675, 0.1839, 0.2132, 0.0006, 0.3334, 0.0779, 0.0009],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.027151187881827354\n",
      " \n",
      "1.046265959739685\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 14,    32/ 27 points] total loss per batch: 1.046\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.7755, 0.0000, 0.0408, 0.1020, 0.0204, 0.0000, 0.0408, 0.0204, 0.0000])\n",
      "Policy pred: tensor([7.9636e-01, 1.2350e-02, 2.6141e-02, 4.0709e-02, 2.0443e-02, 5.4086e-03,\n",
      "        7.1864e-02, 2.6369e-02, 3.6056e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.016686517745256424\n",
      " \n",
      "1.0259233713150024\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 15,    32/ 27 points] total loss per batch: 1.026\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0816, 0.0000, 0.0000, 0.8163, 0.0000, 0.0000, 0.1020, 0.0000])\n",
      "Policy pred: tensor([1.7130e-04, 9.8097e-02, 6.0368e-03, 1.6404e-01, 5.4666e-01, 3.8749e-05,\n",
      "        1.0441e-02, 1.7447e-01, 5.1019e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.03049870952963829\n",
      " \n",
      "0.9758185148239136\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 16,    32/ 27 points] total loss per batch: 0.976\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0315, 0.0212, 0.0082, 0.0263, 0.0205, 0.8534, 0.0132, 0.0231, 0.0028],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.009482565335929394\n",
      " \n",
      "0.9412829875946045\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 17,    32/ 27 points] total loss per batch: 0.941\n",
      "Policy (actual, predicted): 1 0\n",
      "Policy data: tensor([0.0612, 0.2245, 0.1020, 0.1633, 0.1837, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.2418, 0.1990, 0.0627, 0.1370, 0.1084, 0.0025, 0.0922, 0.1504, 0.0061],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.012026374228298664\n",
      " \n",
      "0.9357543587684631\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 18,    32/ 27 points] total loss per batch: 0.936\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([8.9291e-04, 1.9052e-01, 6.7609e-04, 1.9699e-03, 4.1268e-03, 1.7487e-02,\n",
      "        4.8034e-04, 2.0923e-02, 7.6293e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.017605965957045555\n",
      " \n",
      "0.9456691741943359\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 19,    32/ 27 points] total loss per batch: 0.946\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0426, 0.0653, 0.0255, 0.0616, 0.0607, 0.6792, 0.0148, 0.0430, 0.0073],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0049686916172504425\n",
      " \n",
      "0.9272204041481018\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 20,    32/ 27 points] total loss per batch: 0.927\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.4082, 0.4490, 0.0000, 0.0612, 0.0000, 0.0000, 0.0816, 0.0000])\n",
      "Policy pred: tensor([2.5218e-04, 2.8777e-01, 5.5589e-01, 6.0419e-03, 6.8038e-02, 3.1441e-05,\n",
      "        1.6239e-02, 6.5728e-02, 9.6238e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.029453394934535027\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9035129547119141\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 21,    32/ 27 points] total loss per batch: 0.904\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.4082, 0.4490, 0.0000, 0.0612, 0.0000, 0.0000, 0.0816, 0.0000])\n",
      "Policy pred: tensor([1.9008e-04, 1.9427e-01, 6.8276e-01, 3.1849e-03, 4.8771e-02, 3.7316e-05,\n",
      "        7.8670e-03, 6.2902e-02, 8.9434e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.03218069672584534\n",
      " \n",
      "0.8996151089668274\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 22,    32/ 27 points] total loss per batch: 0.900\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0408, 0.0612, 0.0000, 0.0000, 0.8776, 0.0204, 0.0000])\n",
      "Policy pred: tensor([6.0858e-03, 1.3399e-03, 3.4567e-02, 2.5110e-02, 1.1491e-02, 4.2321e-04,\n",
      "        8.9066e-01, 3.0324e-02, 2.5410e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0015631007263436913\n",
      " \n",
      "0.8854097723960876\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 23,    32/ 27 points] total loss per batch: 0.885\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0000, 0.0000, 0.0000, 0.0408, 0.0000, 0.0408, 0.6122])\n",
      "Policy pred: tensor([0.0019, 0.3309, 0.0014, 0.0016, 0.0025, 0.0224, 0.0007, 0.0400, 0.5986],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.005894752684980631\n",
      " \n",
      "0.891991376876831\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 24,    32/ 27 points] total loss per batch: 0.892\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.7755, 0.0000, 0.0408, 0.1020, 0.0204, 0.0000, 0.0408, 0.0204, 0.0000])\n",
      "Policy pred: tensor([7.5910e-01, 5.3086e-03, 3.7851e-02, 1.0402e-01, 2.8549e-02, 6.3531e-03,\n",
      "        3.7868e-02, 2.0860e-02, 8.7480e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.004460595548152924\n",
      " \n",
      "0.8896491527557373\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 25,    32/ 27 points] total loss per batch: 0.890\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.9796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000])\n",
      "Policy pred: tensor([1.2423e-06, 1.9612e-03, 9.7879e-01, 3.2957e-04, 7.8507e-04, 2.8840e-06,\n",
      "        5.7939e-04, 1.7552e-02, 1.0550e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.00542138796299696\n",
      " \n",
      "0.8876896500587463\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 26,    32/ 27 points] total loss per batch: 0.888\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Policy pred: tensor([4.5007e-12, 1.2876e-04, 3.4353e-08, 5.4018e-06, 2.0552e-03, 2.9407e-11,\n",
      "        1.1310e-10, 9.9781e-01, 1.1182e-11], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.00980687327682972\n",
      " \n",
      "0.8857669830322266\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 27,    32/ 27 points] total loss per batch: 0.886\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.9796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000])\n",
      "Policy pred: tensor([2.7104e-06, 6.7981e-03, 9.4912e-01, 1.6684e-03, 1.4131e-03, 6.4550e-06,\n",
      "        8.4025e-04, 4.0152e-02, 3.6364e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.004208785947412252\n",
      " \n",
      "0.8884896039962769\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 28,    32/ 27 points] total loss per batch: 0.888\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0857, 0.0399, 0.0254, 0.0558, 0.0416, 0.7109, 0.0200, 0.0194, 0.0014],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0028883065097033978\n",
      " \n",
      "0.8712799549102783\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 29,    32/ 27 points] total loss per batch: 0.871\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.9796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000])\n",
      "Policy pred: tensor([8.9063e-06, 9.7639e-01, 1.9083e-03, 3.0844e-04, 3.2178e-04, 7.2167e-06,\n",
      "        2.8315e-04, 2.0744e-02, 2.7793e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.00020362436771392822\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:07 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789814114570618\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 30,    32/ 27 points] total loss per batch: 0.879\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0612, 0.2245, 0.1020, 0.1837, 0.1633, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.1794, 0.2437, 0.0765, 0.1190, 0.1234, 0.0006, 0.1210, 0.1348, 0.0016],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.005771155469119549\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:07 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:39:07 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter8.pth.tar\n",
      "[I 200412 12:39:07 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:39:08 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:39:08 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [3.8554326e-11 6.6859105e-13 6.6859105e-13 6.6859105e-13 3.8554326e-11\n",
      " 1.0000000e+00 6.6859105e-13 6.6859105e-13 0.0000000e+00] white = current\n",
      "Policy :  [9.9318677e-01 4.9265367e-03 0.0000000e+00 1.7177771e-03 0.0000000e+00\n",
      " 0.0000000e+00 1.3916247e-04 2.9788882e-05 0.0000000e+00] black = best\n",
      "Policy :  [0.0000000e+00 1.0414411e-04 4.8652213e-04 9.9319559e-01 6.0054739e-03\n",
      " 0.0000000e+00 1.0414411e-04 1.0414411e-04 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.9468002e-14 0.0000000e+00] black = best\n",
      "Policy :  [0.         0.         0.04221265 0.         0.47075626 0.\n",
      " 0.47075626 0.0162748  0.        ] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:08 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 1. 0. 0. 0. 0. 0. 0.] black = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [3.8554326e-11 6.6859105e-13 6.6859105e-13 6.6859105e-13 3.8554326e-11\n",
      " 1.0000000e+00 6.6859105e-13 6.6859105e-13 0.0000000e+00] white = current\n",
      "Policy :  [9.7364062e-01 2.2968134e-02 0.0000000e+00 0.0000000e+00 3.0876207e-03\n",
      " 0.0000000e+00 2.5013776e-04 5.3544067e-05 0.0000000e+00] black = best\n",
      "Policy :  [0.0000000e+00 2.0174320e-04 9.4246841e-04 9.6508765e-01 3.3364635e-02\n",
      " 0.0000000e+00 2.0174320e-04 2.0174320e-04 0.0000000e+00] white = current\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 6.8051784e-05 1.3892221e-01 0.0000000e+00\n",
      " 0.0000000e+00 8.6016977e-01 8.4000954e-04 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:10 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9999875e-01 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2284384e-06 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "best wins!\n",
      "Policy :  [1.5700401e-14 4.5414148e-03 1.5700401e-14 1.5700401e-14 1.5700401e-14\n",
      " 9.2709301e-10 1.5700401e-14 9.2709301e-10 9.9545854e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [6.8978238e-01 3.0980685e-01 5.2466062e-06 4.8862828e-05 4.8862828e-05\n",
      " 0.0000000e+00 5.2466062e-06 3.0254576e-04 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 2.8603776e-11 4.7305417e-09 2.8603776e-11\n",
      " 0.0000000e+00 2.8603776e-11 4.8440745e-16 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 5.3621072e-05 9.7504085e-01 9.5218833e-04\n",
      " 0.0000000e+00 9.5218833e-04 2.3001164e-02 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.4699728e-01 0.0000000e+00 3.6564294e-02\n",
      " 0.0000000e+00 1.6422382e-02 1.6037482e-05 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:10 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9951041e-01\n",
      " 0.0000000e+00 1.6530073e-08 4.8961549e-04 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 2.7978726e-09 0.0000000e+00] white = best\n",
      "best wins!\n",
      "Policy :  [1.5737811e-14 2.1695909e-03 1.5737811e-14 1.5737811e-14 1.5737811e-14\n",
      " 1.6502291e-08 1.5737811e-14 9.2930197e-10 9.9783039e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [6.8978238e-01 3.0980685e-01 5.2466062e-06 4.8862828e-05 4.8862828e-05\n",
      " 0.0000000e+00 5.2466062e-06 3.0254576e-04 0.0000000e+00] black = current\n",
      "Policy :  [1.000000e+00 0.000000e+00 7.628890e-14 7.628890e-14 7.450088e-17\n",
      " 0.000000e+00 7.628890e-14 7.450088e-17 0.000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.0226469e-07 5.8970977e-06 9.7527209e-04\n",
      " 0.0000000e+00 9.9867862e-01 3.4005637e-04 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 5.7175126e-10 1.0000000e+00 5.8547328e-17\n",
      " 0.0000000e+00 0.0000000e+00 5.8547328e-17 0.0000000e+00] white = best\n",
      "Policy :  [0.00000000e+00 0.00000000e+00 1.02376084e-07 0.00000000e+00\n",
      " 9.99766529e-01 0.00000000e+00 0.00000000e+00 2.33410057e-04\n",
      " 0.00000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.5402373e-17 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:12 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [6.6859105e-13 6.6859105e-13 6.6859105e-13 3.8554326e-11 3.8554326e-11\n",
      " 1.0000000e+00 6.6859105e-13 6.6859105e-13 0.0000000e+00] white = current\n",
      "Policy :  [9.977937e-01 1.627439e-03 1.589296e-06 5.674529e-04 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 9.840502e-06 0.000000e+00] black = best\n",
      "Policy :  [0.0000000e+00 1.0302773e-04 1.0302773e-04 9.8254901e-01 1.7038902e-02\n",
      " 0.0000000e+00 1.0302773e-04 1.0302773e-04 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.9468002e-14 0.0000000e+00] black = best\n",
      "Policy :  [0.         0.         0.01128789 0.         0.3265075  0.\n",
      " 0.6509167  0.01128789 0.        ] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 3.5906517e-10 0.0000000e+00 1.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:12 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:39:12 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:39:12 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:39:12 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter7.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9989510e-01 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.0484661e-04 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Current_net wins ratio 0.20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:16 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:39:16 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:39:16 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_7/.\n",
      "[I 200412 12:39:16 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.6105692386627197\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 1,    32/ 27 points] total loss per batch: 2.611\n",
      "Policy (actual, predicted): 1 2\n",
      "Policy data: tensor([0.1837, 0.2245, 0.1020, 0.1837, 0.1837, 0.0000, 0.0000, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0737, 0.1098, 0.1965, 0.1235, 0.1431, 0.1149, 0.1029, 0.0501, 0.0854],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.03549456596374512\n",
      " \n",
      "2.4937589168548584\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 2,    32/ 27 points] total loss per batch: 2.494\n",
      "Policy (actual, predicted): 5 7\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0445, 0.1328, 0.0681, 0.0633, 0.0572, 0.1050, 0.1791, 0.2549, 0.0950],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.42544084787368774\n",
      " \n",
      "2.429293394088745\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 3,    32/ 27 points] total loss per batch: 2.429\n",
      "Policy (actual, predicted): 0 3\n",
      "Policy data: tensor([0.6735, 0.2041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0700, 0.1523, 0.0972, 0.2424, 0.0836, 0.0655, 0.1047, 0.0276, 0.1567],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.07924741506576538\n",
      " \n",
      "1.9295483827590942\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 4,    32/ 27 points] total loss per batch: 1.930\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0204, 0.0612, 0.0204, 0.6122, 0.0408, 0.0612, 0.1020])\n",
      "Policy pred: tensor([0.0417, 0.1891, 0.1222, 0.0475, 0.0962, 0.2180, 0.0858, 0.0334, 0.1660],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.06533943116664886\n",
      " \n",
      "1.9040390253067017\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 5,    32/ 27 points] total loss per batch: 1.904\n",
      "Policy (actual, predicted): 3 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0612, 0.7347, 0.1633, 0.0000, 0.0000, 0.0408, 0.0000])\n",
      "Policy pred: tensor([0.0887, 0.0598, 0.0537, 0.1367, 0.1169, 0.0173, 0.0786, 0.3601, 0.0881],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0848172977566719\n",
      " \n",
      "1.5549439191818237\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 6,    32/ 27 points] total loss per batch: 1.555\n",
      "Policy (actual, predicted): 1 5\n",
      "Policy data: tensor([0.0000, 0.6939, 0.0612, 0.1020, 0.0612, 0.0000, 0.0612, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0200, 0.2695, 0.0690, 0.0467, 0.0525, 0.2862, 0.1059, 0.0313, 0.1190],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.4135746657848358\n",
      " \n",
      "1.4424149990081787\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 7,    32/ 27 points] total loss per batch: 1.442\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.6939, 0.0612, 0.1020, 0.0612, 0.0000, 0.0612, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0297, 0.3192, 0.0630, 0.0587, 0.0794, 0.1985, 0.0896, 0.0293, 0.1327],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.00864145252853632\n",
      " \n",
      "1.3479490280151367\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 8,    32/ 27 points] total loss per batch: 1.348\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.6939, 0.0612, 0.1020, 0.0612, 0.0000, 0.0612, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0345, 0.4186, 0.0782, 0.0537, 0.1008, 0.1300, 0.0603, 0.0308, 0.0932],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.07881253212690353\n",
      " \n",
      "1.275097370147705\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 9,    32/ 27 points] total loss per batch: 1.275\n",
      "Policy (actual, predicted): 8 0\n",
      "Policy data: tensor([0.1633, 0.0000, 0.0204, 0.0408, 0.0816, 0.0000, 0.1224, 0.2857, 0.2857])\n",
      "Policy pred: tensor([0.3050, 0.0663, 0.0498, 0.1418, 0.0340, 0.0073, 0.0350, 0.0889, 0.2720],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.7038180828094482\n",
      " \n",
      "1.2692680358886719\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 10,    32/ 27 points] total loss per batch: 1.269\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0816, 0.0000, 0.0204, 0.0612, 0.0204, 0.6122, 0.0408, 0.0612, 0.1020])\n",
      "Policy pred: tensor([0.0214, 0.0583, 0.0083, 0.0302, 0.0081, 0.7428, 0.0086, 0.0144, 0.1078],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.64278244972229\n",
      " \n",
      "1.1621026992797852\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 11,    32/ 27 points] total loss per batch: 1.162\n",
      "Policy (actual, predicted): 2 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.3878, 0.0000, 0.2653, 0.0000, 0.2245, 0.1224, 0.0000])\n",
      "Policy pred: tensor([2.5679e-03, 1.4011e-02, 2.2969e-01, 4.6662e-02, 3.0324e-01, 2.7982e-04,\n",
      "        3.6386e-01, 3.9458e-02, 2.2610e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.039250168949365616\n",
      " \n",
      "1.06159508228302\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 12,    32/ 27 points] total loss per batch: 1.062\n",
      "Policy (actual, predicted): 8 0\n",
      "Policy data: tensor([0.1633, 0.0000, 0.0204, 0.0408, 0.0816, 0.0000, 0.1224, 0.2857, 0.2857])\n",
      "Policy pred: tensor([0.3800, 0.0348, 0.0293, 0.1558, 0.0280, 0.0023, 0.0321, 0.0736, 0.2640],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.8880535960197449\n",
      " \n",
      "1.0131525993347168\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 13,    32/ 27 points] total loss per batch: 1.013\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0612, 0.7347, 0.1633, 0.0000, 0.0000, 0.0408, 0.0000])\n",
      "Policy pred: tensor([3.0811e-02, 1.5345e-02, 1.9375e-02, 7.2495e-01, 1.4645e-01, 4.2812e-05,\n",
      "        9.4607e-03, 5.2921e-02, 6.5038e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.04319274425506592\n",
      " \n",
      "0.9357104301452637\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 14,    32/ 27 points] total loss per batch: 0.936\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.6939, 0.0612, 0.1020, 0.0612, 0.0000, 0.0612, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0058, 0.7633, 0.0476, 0.0576, 0.0670, 0.0047, 0.0236, 0.0278, 0.0026],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.08369765430688858\n",
      " \n",
      "0.9206073880195618\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 15,    32/ 27 points] total loss per batch: 0.921\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Policy pred: tensor([2.9878e-04, 3.2772e-07, 9.2277e-04, 1.7264e-03, 1.5659e-04, 1.1204e-08,\n",
      "        1.0446e-02, 9.8645e-01, 5.1283e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.024382898584008217\n",
      " \n",
      "0.8813657164573669\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 16,    32/ 27 points] total loss per batch: 0.881\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0377, 0.0388, 0.0211, 0.0260, 0.0316, 0.7600, 0.0255, 0.0480, 0.0113],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.022283999249339104\n",
      " \n",
      "0.8546367883682251\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 17,    32/ 27 points] total loss per batch: 0.855\n",
      "Policy (actual, predicted): 1 0\n",
      "Policy data: tensor([0.1837, 0.2245, 0.1020, 0.1837, 0.1837, 0.0000, 0.0000, 0.1224, 0.0000])\n",
      "Policy pred: tensor([4.2699e-01, 1.6772e-01, 1.7847e-02, 1.0548e-01, 1.1221e-01, 2.6621e-04,\n",
      "        4.4922e-03, 1.3774e-01, 2.7254e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.011506292037665844\n",
      " \n",
      "0.8496614098548889\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 18,    32/ 27 points] total loss per batch: 0.850\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([0.0113, 0.4232, 0.0024, 0.0018, 0.0092, 0.0024, 0.0012, 0.0310, 0.5175],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.019815804436802864\n",
      " \n",
      "0.8433502912521362\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 19,    32/ 27 points] total loss per batch: 0.843\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.1133, 0.0321, 0.0312, 0.0396, 0.0375, 0.6656, 0.0264, 0.0409, 0.0134],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.022087084129452705\n",
      " \n",
      "0.8361103534698486\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 20,    32/ 27 points] total loss per batch: 0.836\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0816, 0.4898, 0.0000, 0.0000, 0.4082, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0029, 0.0009, 0.1258, 0.4496, 0.0050, 0.0005, 0.3971, 0.0138, 0.0042],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9920455813407898\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8375332951545715\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 21,    32/ 27 points] total loss per batch: 0.838\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0816, 0.4898, 0.0000, 0.0000, 0.4082, 0.0204, 0.0000])\n",
      "Policy pred: tensor([1.5700e-03, 8.3669e-04, 1.1263e-01, 4.9293e-01, 3.7511e-03, 4.3874e-04,\n",
      "        3.7131e-01, 1.2790e-02, 3.7500e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9928422570228577\n",
      " \n",
      "0.8454270362854004\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 22,    32/ 27 points] total loss per batch: 0.845\n",
      "Policy (actual, predicted): 2 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.3878, 0.0000, 0.2653, 0.0000, 0.2245, 0.1224, 0.0000])\n",
      "Policy pred: tensor([3.1817e-04, 2.8095e-03, 2.8624e-01, 4.4142e-03, 3.5022e-01, 2.3972e-05,\n",
      "        2.9489e-01, 6.1072e-02, 1.5092e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.008352409116923809\n",
      " \n",
      "0.8220099806785583\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 23,    32/ 27 points] total loss per batch: 0.822\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([3.4119e-03, 2.1149e-01, 1.9590e-03, 4.1609e-04, 1.9355e-03, 6.4555e-04,\n",
      "        3.3832e-04, 2.8312e-02, 7.5149e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.010215504094958305\n",
      " \n",
      "0.8219971060752869\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 24,    32/ 27 points] total loss per batch: 0.822\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.6939, 0.0612, 0.1020, 0.0612, 0.0000, 0.0612, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0026, 0.7891, 0.0371, 0.0757, 0.0383, 0.0021, 0.0371, 0.0170, 0.0010],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0030548886861652136\n",
      " \n",
      "0.8189497590065002\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 25,    32/ 27 points] total loss per batch: 0.819\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3265, 0.0000, 0.6735, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.5538e-06, 1.2690e-04, 4.2950e-03, 2.0989e-05, 4.1440e-01, 1.1360e-06,\n",
      "        5.7887e-01, 2.2742e-03, 3.6949e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.008784421719610691\n",
      " \n",
      "0.8157504200935364\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 26,    32/ 27 points] total loss per batch: 0.816\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "Policy pred: tensor([1.9407e-06, 9.4075e-08, 1.8988e-03, 7.1164e-06, 4.3119e-03, 2.6766e-08,\n",
      "        9.8821e-01, 5.5712e-03, 4.0289e-09], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.014680158346891403\n",
      " \n",
      "0.808418333530426\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 27,    32/ 27 points] total loss per batch: 0.808\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3265, 0.0000, 0.6735, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.2998e-06, 8.1177e-05, 3.6959e-03, 1.1597e-05, 3.2985e-01, 7.5393e-07,\n",
      "        6.6389e-01, 2.4664e-03, 2.8676e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.00912473350763321\n",
      " \n",
      "0.8107901811599731\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 28,    32/ 27 points] total loss per batch: 0.811\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0598, 0.0457, 0.0195, 0.0375, 0.0418, 0.7566, 0.0163, 0.0191, 0.0037],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.004040789324790239\n",
      " \n",
      "0.8081004619598389\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 29,    32/ 27 points] total loss per batch: 0.808\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.4694, 0.0000, 0.0000, 0.0000, 0.5306, 0.0000, 0.0000])\n",
      "Policy pred: tensor([8.6779e-07, 9.6448e-07, 3.7784e-01, 2.3060e-05, 1.4683e-03, 2.1648e-07,\n",
      "        6.1762e-01, 3.0508e-03, 5.4090e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.001252542482689023\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:18 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8043351173400879\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 30,    32/ 27 points] total loss per batch: 0.804\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.1633, 0.0000, 0.0204, 0.0408, 0.0816, 0.0000, 0.1224, 0.2857, 0.2857])\n",
      "Policy pred: tensor([0.1647, 0.0074, 0.0318, 0.0361, 0.0541, 0.0005, 0.0865, 0.3036, 0.3154],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9843406081199646\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:19 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:39:19 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter8.pth.tar\n",
      "[I 200412 12:39:19 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:39:19 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:39:19 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 9.7560976e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.7324611e-12 0.0000000e+00 1.7324611e-12 9.9902439e-01] black = best\n",
      "Policy :  [3.4571204e-12 9.9998820e-01 3.4571204e-12 3.4571204e-12 3.4571204e-12\n",
      " 1.1771713e-05 3.4571204e-12 3.4571204e-12 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 9.9928260e-01 0.0000000e+00 7.1737915e-04 0.0000000e+00] black = best\n",
      "Policy :  [1.02821883e-04 0.00000000e+00 1.70048606e-02 1.00411995e-07\n",
      " 9.80585933e-01 0.00000000e+00 4.80345218e-04 1.82588294e-03\n",
      " 0.00000000e+00] white = current\n",
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9999344e-01 6.6858666e-13 0.0000000e+00\n",
      " 0.0000000e+00 6.5291665e-06 6.8463274e-10 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 9.999997e-01 3.325256e-07 0.000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:20 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 7.0305652e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.2192075e-15 0.0000000e+00 1.2484685e-12 9.9929690e-01] black = best\n",
      "Policy :  [2.4339903e-12 9.9999744e-01 2.4339903e-12 2.4339903e-12 2.4339903e-12\n",
      " 2.5522238e-06 2.4339903e-12 2.4339903e-12 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 9.9928260e-01 0.0000000e+00 7.1737915e-04 0.0000000e+00] black = best\n",
      "Policy :  [1.02821883e-04 0.00000000e+00 1.70048606e-02 1.00411995e-07\n",
      " 9.80585933e-01 0.00000000e+00 4.80345218e-04 1.82588294e-03\n",
      " 0.00000000e+00] white = current\n",
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9997699e-01 9.0947378e-13 0.0000000e+00\n",
      " 0.0000000e+00 2.3036531e-05 9.3130115e-10 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:21 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 9.999997e-01 3.325256e-07 0.000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [2.4339903e-12 9.9999744e-01 2.4339903e-12 2.4339903e-12 2.4339903e-12\n",
      " 2.5522238e-06 2.4339903e-12 2.4339903e-12 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 9.9928260e-01 0.0000000e+00 7.1737915e-04 0.0000000e+00] black = best\n",
      "Policy :  [1.0296025e-04 0.0000000e+00 1.7027743e-02 1.7854883e-06 9.8190552e-01\n",
      " 0.0000000e+00 4.8099164e-04 4.8099164e-04 0.0000000e+00] white = current\n",
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9997699e-01 9.0947378e-13 0.0000000e+00\n",
      " 0.0000000e+00 2.3036531e-05 9.3130115e-10 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 9.999997e-01 3.325256e-07 0.000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:22 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6538173e-08] black = current\n",
      "Policy :  [1.7757726e-09 0.0000000e+00 1.6935088e-15 9.9999994e-11 1.6935088e-15\n",
      " 1.0000000e+00 1.7341530e-12 9.9999994e-11 1.6538172e-08] white = best\n",
      "Policy :  [1.8443356e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 9.9999756e-01 2.2765889e-06] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.5554787e-09 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 9.6311146e-09] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 0. 1.] black = current\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 3.7650713e-14 3.7650713e-14 3.6768274e-17\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:23 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.000000e+00 0.000000e+00 9.999997e-01 3.325256e-07 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00] white = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [3.4571204e-12 9.9998820e-01 3.4571204e-12 3.4571204e-12 3.4571204e-12\n",
      " 1.1771713e-05 3.4571204e-12 3.4571204e-12 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 9.9928260e-01 0.0000000e+00 7.1737915e-04 0.0000000e+00] black = best\n",
      "Policy :  [1.0296025e-04 0.0000000e+00 1.7027743e-02 1.7854883e-06 9.8190552e-01\n",
      " 0.0000000e+00 4.8099164e-04 4.8099164e-04 0.0000000e+00] white = current\n",
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9997699e-01 9.0947378e-13 0.0000000e+00\n",
      " 0.0000000e+00 2.3036531e-05 9.3130115e-10 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 1. 0. 0. 0. 0. 0.] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:24 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:39:24 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:39:24 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:39:24 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter7.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999875e-01 1.2284384e-06 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = best\n",
      "best wins!\n",
      "Current_net wins ratio 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:27 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:39:27 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:39:27 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_7/.\n",
      "[I 200412 12:39:27 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.523023843765259\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 1,    32/ 26 points] total loss per batch: 2.523\n",
      "Policy (actual, predicted): 5 0\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.1731, 0.1237, 0.1535, 0.0707, 0.0989, 0.1056, 0.1519, 0.0497, 0.0730],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.08902260661125183\n",
      " \n",
      "2.467154026031494\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 2,    32/ 26 points] total loss per batch: 2.467\n",
      "Policy (actual, predicted): 3 1\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0408, 0.3265, 0.0000, 0.0000, 0.2041, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0453, 0.6085, 0.0978, 0.0257, 0.0300, 0.0230, 0.0324, 0.0861, 0.0513],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.3608260750770569\n",
      " \n",
      "2.314453601837158\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 3,    32/ 26 points] total loss per batch: 2.314\n",
      "Policy (actual, predicted): 6 2\n",
      "Policy data: tensor([0.0000, 0.1020, 0.1837, 0.0000, 0.0000, 0.0000, 0.6735, 0.0408, 0.0000])\n",
      "Policy pred: tensor([0.1654, 0.0320, 0.2431, 0.1084, 0.1094, 0.1222, 0.0991, 0.0780, 0.0424],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.17674903571605682\n",
      " \n",
      "2.181912422180176\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 4,    32/ 26 points] total loss per batch: 2.182\n",
      "Policy (actual, predicted): 2 4\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([0.2106, 0.0604, 0.0354, 0.0798, 0.2652, 0.0047, 0.0283, 0.1677, 0.1478],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.4946155250072479\n",
      " \n",
      "1.8124016523361206\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 5,    32/ 26 points] total loss per batch: 1.812\n",
      "Policy (actual, predicted): 0 1\n",
      "Policy data: tensor([0.7143, 0.0000, 0.0612, 0.1020, 0.0000, 0.0000, 0.0000, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0640, 0.2894, 0.1214, 0.1189, 0.1362, 0.0166, 0.0369, 0.1493, 0.0672],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.09113919734954834\n",
      " \n",
      "1.7557868957519531\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 6,    32/ 26 points] total loss per batch: 1.756\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.5510, 0.0408, 0.0408, 0.1020, 0.0408, 0.1633, 0.0612, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.4457, 0.1032, 0.0292, 0.0187, 0.0176, 0.2716, 0.0839, 0.0099, 0.0203],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.1899343878030777\n",
      " \n",
      "1.5910288095474243\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 7,    32/ 26 points] total loss per batch: 1.591\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.3278, 0.0668, 0.0262, 0.0294, 0.0116, 0.4613, 0.0581, 0.0066, 0.0122],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.2086174637079239\n",
      " \n",
      "1.4610176086425781\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 8,    32/ 26 points] total loss per batch: 1.461\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.7755, 0.0000, 0.0000, 0.0000, 0.0000, 0.2245, 0.0000])\n",
      "Policy pred: tensor([6.0783e-04, 1.2822e-03, 6.1994e-01, 2.0397e-02, 3.0948e-01, 5.6181e-06,\n",
      "        5.8311e-04, 4.7688e-02, 1.2550e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.8564467430114746\n",
      " \n",
      "1.344404935836792\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 9,    32/ 26 points] total loss per batch: 1.344\n",
      "Policy (actual, predicted): 1 0\n",
      "Policy data: tensor([0.1633, 0.2245, 0.0000, 0.1633, 0.1837, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.2872, 0.2811, 0.0381, 0.0833, 0.0322, 0.0700, 0.0890, 0.0263, 0.0928],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.315186470746994\n",
      " \n",
      "1.2501769065856934\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 10,    32/ 26 points] total loss per batch: 1.250\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.8367, 0.0408, 0.0204, 0.0408, 0.0000, 0.0000, 0.0408, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.6877, 0.0513, 0.0420, 0.0440, 0.0160, 0.0700, 0.0709, 0.0127, 0.0054],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.059092216193675995\n",
      " \n",
      "1.155822992324829\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 11,    32/ 26 points] total loss per batch: 1.156\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([2.4258e-04, 4.6706e-04, 7.2624e-01, 2.2093e-02, 1.5850e-01, 4.2953e-07,\n",
      "        3.3535e-04, 9.2118e-02, 1.6993e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.14500872790813446\n",
      " \n",
      "1.0656462907791138\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 12,    32/ 26 points] total loss per batch: 1.066\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.8367, 0.0408, 0.0204, 0.0408, 0.0000, 0.0000, 0.0408, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.7269, 0.0398, 0.0571, 0.0570, 0.0263, 0.0101, 0.0538, 0.0265, 0.0025],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.02463715337216854\n",
      " \n",
      "1.0367026329040527\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 13,    32/ 26 points] total loss per batch: 1.037\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([1.2372e-03, 2.4064e-01, 5.8522e-04, 2.0497e-03, 2.8931e-03, 1.8524e-04,\n",
      "        6.3757e-04, 5.1491e-02, 7.0028e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.005042234435677528\n",
      " \n",
      "0.9966188669204712\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 14,    32/ 26 points] total loss per batch: 0.997\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2449, 0.0000, 0.1837, 0.0000, 0.5714, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.0130e-02, 6.1086e-03, 1.0465e-01, 4.0570e-02, 1.8065e-01, 4.6608e-04,\n",
      "        6.4608e-01, 1.1904e-03, 1.4793e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.01004995871335268\n",
      " \n",
      "0.9912401437759399\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 15,    32/ 26 points] total loss per batch: 0.991\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.1020, 0.1837, 0.0000, 0.0000, 0.0000, 0.6735, 0.0408, 0.0000])\n",
      "Policy pred: tensor([9.4928e-03, 1.5954e-01, 1.9438e-01, 6.7342e-03, 1.9900e-02, 1.7189e-04,\n",
      "        5.9721e-01, 1.2331e-02, 2.3990e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.01022620964795351\n",
      " \n",
      "0.9565333127975464\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 16,    32/ 26 points] total loss per batch: 0.957\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.7143, 0.0000, 0.0612, 0.1020, 0.0000, 0.0000, 0.0000, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.6960, 0.0249, 0.0355, 0.1237, 0.0597, 0.0060, 0.0065, 0.0466, 0.0010],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9215070009231567\n",
      " \n",
      "0.94868403673172\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 17,    32/ 26 points] total loss per batch: 0.949\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.8163, 0.0000, 0.0816, 0.0408, 0.0204, 0.0000, 0.0408, 0.0000, 0.0000])\n",
      "Policy pred: tensor([8.6849e-01, 4.0070e-03, 4.8284e-02, 2.9133e-02, 1.2326e-02, 3.9882e-03,\n",
      "        2.9220e-02, 4.1272e-03, 4.2902e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.022876037284731865\n",
      " \n",
      "0.9388093948364258\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 18,    32/ 26 points] total loss per batch: 0.939\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0000, 0.0612, 0.0408, 0.0612, 0.7551, 0.0000, 0.0204, 0.0000])\n",
      "Policy pred: tensor([4.1996e-02, 1.3803e-02, 4.3366e-02, 4.3574e-02, 3.2681e-02, 8.0163e-01,\n",
      "        1.3219e-03, 2.1599e-02, 3.3431e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9746620059013367\n",
      " \n",
      "0.9298830032348633\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 19,    32/ 26 points] total loss per batch: 0.930\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.8367, 0.0408, 0.0204, 0.0408, 0.0000, 0.0000, 0.0408, 0.0204, 0.0000])\n",
      "Policy pred: tensor([8.0885e-01, 2.3723e-02, 4.3999e-02, 2.9356e-02, 6.7818e-03, 1.8569e-03,\n",
      "        5.7471e-02, 2.7360e-02, 5.9721e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.007995684631168842\n",
      " \n",
      "0.9208996891975403\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 20,    32/ 26 points] total loss per batch: 0.921\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.7755, 0.0000, 0.0000, 0.0000, 0.0000, 0.2245, 0.0000])\n",
      "Policy pred: tensor([6.0203e-07, 1.2347e-04, 7.5442e-01, 9.4605e-04, 4.4296e-03, 7.4693e-09,\n",
      "        1.2558e-07, 2.4008e-01, 4.5650e-09], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9820199012756348\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168729782104492\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 21,    32/ 26 points] total loss per batch: 0.917\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.1020, 0.1837, 0.0000, 0.0000, 0.0000, 0.6735, 0.0408, 0.0000])\n",
      "Policy pred: tensor([9.9691e-04, 1.0311e-01, 1.5414e-01, 2.1266e-03, 8.1167e-03, 5.0018e-05,\n",
      "        7.0118e-01, 3.0253e-02, 2.7037e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0009013337548822165\n",
      " \n",
      "0.9113776683807373\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 22,    32/ 26 points] total loss per batch: 0.911\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([4.4465e-10, 1.6883e-06, 9.9909e-01, 4.2472e-05, 4.7189e-04, 1.3403e-11,\n",
      "        7.2463e-09, 3.9731e-04, 1.6822e-10], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.020704539492726326\n",
      " \n",
      "0.9087892174720764\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 23,    32/ 26 points] total loss per batch: 0.909\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([7.5726e-04, 3.5381e-01, 3.0030e-04, 1.5864e-04, 1.3622e-03, 7.0565e-03,\n",
      "        5.3666e-04, 2.6388e-02, 6.0963e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.003386395052075386\n",
      " \n",
      "0.9071118235588074\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 24,    32/ 26 points] total loss per batch: 0.907\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([8.3183e-04, 3.0658e-01, 2.9062e-04, 1.5080e-04, 1.3468e-03, 9.3853e-03,\n",
      "        5.9159e-04, 2.8518e-02, 6.5230e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0005154050304554403\n",
      " \n",
      "0.9122815728187561\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 25,    32/ 26 points] total loss per batch: 0.912\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([5.9485e-04, 3.2399e-01, 2.2648e-04, 1.0605e-04, 9.7009e-04, 8.7226e-03,\n",
      "        3.9306e-04, 3.5973e-02, 6.2903e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.002603774657472968\n",
      " \n",
      "0.9526217579841614\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 26,    32/ 26 points] total loss per batch: 0.953\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1020, 0.0000, 0.8980, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.9525e-06, 4.8982e-06, 1.1497e-01, 3.1919e-03, 8.8116e-01, 3.2367e-08,\n",
      "        6.1458e-08, 6.6902e-04, 6.1701e-09], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.999583899974823\n",
      " \n",
      "0.985541045665741\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 27,    32/ 26 points] total loss per batch: 0.986\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2449, 0.0000, 0.1837, 0.0000, 0.5714, 0.0000, 0.0000])\n",
      "Policy pred: tensor([9.9379e-05, 1.4081e-03, 1.6222e-01, 1.1751e-03, 2.1688e-01, 1.5321e-05,\n",
      "        6.1787e-01, 3.3553e-04, 7.0699e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.00036661323974840343\n",
      " \n",
      "0.9737882018089294\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 28,    32/ 26 points] total loss per batch: 0.974\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1837, 0.7143, 0.1020, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([7.9629e-05, 2.8213e-03, 1.2115e-01, 8.1166e-01, 6.1325e-02, 5.0310e-06,\n",
      "        1.0104e-03, 1.9411e-03, 8.2863e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.027673538774251938\n",
      " \n",
      "0.9623711109161377\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 29,    32/ 26 points] total loss per batch: 0.962\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([4.0999e-04, 2.5279e-01, 3.7741e-04, 1.7337e-04, 7.4405e-04, 4.1352e-03,\n",
      "        5.5677e-04, 2.8310e-02, 7.1250e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.010978391394019127\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:30 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9612259268760681\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 30,    32/ 26 points] total loss per batch: 0.961\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1837, 0.7143, 0.1020, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.6316e-05, 4.7690e-03, 2.4915e-01, 6.6139e-01, 8.2888e-02, 2.6254e-06,\n",
      "        7.0245e-04, 1.0576e-03, 6.0562e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.010131698101758957\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:30 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:39:30 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter8.pth.tar\n",
      "[I 200412 12:39:30 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:39:30 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:39:30 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [9.6975176e-14 6.0044471e-03 9.6975176e-14 9.6975176e-14 9.9302580e-11\n",
      " 9.6975176e-14 9.6975176e-14 9.6975174e-04 9.9302578e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [2.4550707e-06 4.1576838e-01 2.4550707e-06 1.4496948e-01 4.1576838e-01\n",
      " 0.0000000e+00 1.1744428e-02 1.1744428e-02 0.0000000e+00] black = current\n",
      "Policy :  [1.000000e+00 7.628890e-14 7.450088e-17 7.628890e-14 0.000000e+00\n",
      " 0.000000e+00 7.628890e-14 7.450088e-17 0.000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 6.3806069e-01 6.2310614e-04 3.0409911e-01 0.0000000e+00\n",
      " 0.0000000e+00 5.7214808e-02 2.3130312e-06 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 5.3704586e-08 3.0968772e-06 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999690e-01 9.0949188e-13 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 1.4079826e-02 9.8591977e-01 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 4.2229951e-07 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.5402373e-17 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:31 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [9.6975176e-14 6.0044471e-03 9.6975176e-14 9.6975176e-14 9.9302580e-11\n",
      " 9.6975176e-14 9.6975176e-14 9.6975174e-04 9.9302578e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [2.4550707e-06 4.1576838e-01 2.4550707e-06 4.1576838e-01 1.4496948e-01\n",
      " 0.0000000e+00 1.1744428e-02 1.1744428e-02 0.0000000e+00] black = current\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 1.6310377e-13 1.5554787e-09 1.5928103e-16\n",
      " 0.0000000e+00 1.6310377e-13 1.5928103e-16 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.1771713e-05 9.9998820e-01 3.2969670e-08\n",
      " 0.0000000e+00 3.5400913e-09 1.9935498e-10 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.6538172e-08 0.0000000e+00 2.8679720e-10\n",
      " 0.0000000e+00 1.0000000e+00 1.6150558e-11 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9999994e-01 0.0000000e+00 3.4700388e-08\n",
      " 0.0000000e+00 0.0000000e+00 7.2538143e-12 0.0000000e+00] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:33 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.5402373e-17 0.0000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [9.6975176e-14 6.0044471e-03 9.6975176e-14 9.6975176e-14 9.9302580e-11\n",
      " 9.6975176e-14 9.6975176e-14 9.6975174e-04 9.9302578e-01] black = current\n",
      "Policy :  [1.2279914e-11 2.1295250e-13 2.0796142e-16 2.1295250e-13 2.1295250e-13\n",
      " 1.0000000e+00 2.0796142e-16 2.0796142e-16 0.0000000e+00] white = best\n",
      "Policy :  [3.3666051e-06 1.9879468e-01 5.9783255e-05 1.9879468e-01 5.7013756e-01\n",
      " 0.0000000e+00 1.6104976e-02 1.6104976e-02 0.0000000e+00] black = current\n",
      "Policy :  [1.0000000e+00 0.0000000e+00 1.6310377e-13 1.5554787e-09 1.5928103e-16\n",
      " 0.0000000e+00 1.6310377e-13 1.5928103e-16 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.1771713e-05 9.9998820e-01 3.5400913e-09\n",
      " 0.0000000e+00 3.2969670e-08 1.9935498e-10 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.6538172e-08 0.0000000e+00 2.8679720e-10\n",
      " 0.0000000e+00 1.0000000e+00 1.6150558e-11 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 9.9999994e-01 0.0000000e+00 3.4700388e-08\n",
      " 0.0000000e+00 0.0000000e+00 7.2538143e-12 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.5402373e-17 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:34 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 0. 1. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [2.4915960e-09 2.3761712e-15 2.3761712e-15 9.9967241e-01 2.3761712e-15\n",
      " 3.2757537e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 9.9995959e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 4.0425508e-05 0.0000000e+00 6.8460954e-10 0.0000000e+00] black = best\n",
      "Policy :  [0.00389442 0.         0.3230707  0.         0.0289697  0.64406514\n",
      " 0.         0.         0.        ] white = current\n",
      "Policy :  [9.9999982e-01 0.0000000e+00 1.2200651e-15 0.0000000e+00 7.3772668e-08\n",
      " 0.0000000e+00 1.1914698e-08 7.3772668e-08 0.0000000e+00] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:35 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.        0.        0.2268782 0.        0.7731218 0.        0.\n",
      " 0.        0.       ] white = current\n",
      "Policy :  [0. 0. 1. 0. 0. 0. 0. 0. 0.] black = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [2.4915960e-09 2.3761712e-15 2.3761712e-15 9.9967241e-01 2.3761712e-15\n",
      " 3.2757537e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 9.9995959e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 4.0425508e-05 0.0000000e+00 6.8460954e-10 0.0000000e+00] black = best\n",
      "Policy :  [0.00389442 0.         0.3230707  0.         0.0289697  0.64406514\n",
      " 0.         0.         0.        ] white = current\n",
      "Policy :  [1.000000e+00 0.000000e+00 7.450088e-17 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 7.275477e-10 7.628890e-14 0.000000e+00] black = best\n",
      "Policy :  [0.        0.        0.2268782 0.        0.7731218 0.        0.\n",
      " 0.        0.       ] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:36 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:39:36 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:39:36 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:39:36 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter7.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 1. 0. 0. 0. 0. 0. 0.] black = best\n",
      "best wins!\n",
      "Current_net wins ratio 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:39 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:39:39 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:39:39 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_7/.\n",
      "[I 200412 12:39:39 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.627345085144043\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 1,    32/ 27 points] total loss per batch: 2.627\n",
      "Policy (actual, predicted): 1 4\n",
      "Policy data: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([0.0952, 0.0673, 0.1287, 0.0981, 0.1558, 0.0831, 0.1538, 0.0951, 0.1229],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.038994453847408295\n",
      " \n",
      "2.206432580947876\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 2,    32/ 27 points] total loss per batch: 2.206\n",
      "Policy (actual, predicted): 5 1\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0665, 0.2009, 0.1660, 0.1340, 0.1007, 0.1263, 0.0985, 0.0534, 0.0535],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.25035911798477173\n",
      " \n",
      "2.4108073711395264\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 3,    32/ 27 points] total loss per batch: 2.411\n",
      "Policy (actual, predicted): 0 1\n",
      "Policy data: tensor([0.6939, 0.2653, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.0000])\n",
      "Policy pred: tensor([0.1394, 0.3980, 0.0316, 0.0271, 0.0663, 0.0756, 0.0283, 0.1198, 0.1138],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.05414164811372757\n",
      " \n",
      "1.9124614000320435\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 4,    32/ 27 points] total loss per batch: 1.912\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.1448, 0.1656, 0.0614, 0.0489, 0.0909, 0.2767, 0.0931, 0.0486, 0.0700],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.11240048706531525\n",
      " \n",
      "1.6927014589309692\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 5,    32/ 27 points] total loss per batch: 1.693\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([0.0702, 0.4023, 0.0548, 0.0691, 0.0822, 0.0416, 0.0293, 0.0526, 0.1979],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.07109677791595459\n",
      " \n",
      "1.5457544326782227\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 6,    32/ 27 points] total loss per batch: 1.546\n",
      "Policy (actual, predicted): 2 1\n",
      "Policy data: tensor([0.1429, 0.0000, 0.2449, 0.1429, 0.1633, 0.1633, 0.1429, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1568, 0.2280, 0.0643, 0.1318, 0.0616, 0.2000, 0.0614, 0.0519, 0.0442],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.11786312609910965\n",
      " \n",
      "1.4169764518737793\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 7,    32/ 27 points] total loss per batch: 1.417\n",
      "Policy (actual, predicted): 1 0\n",
      "Policy data: tensor([0.0000, 0.3061, 0.1224, 0.1837, 0.1429, 0.0000, 0.1429, 0.1020, 0.0000])\n",
      "Policy pred: tensor([0.2009, 0.1942, 0.0375, 0.0903, 0.0988, 0.1366, 0.1236, 0.0940, 0.0242],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.2073121964931488\n",
      " \n",
      "1.3049577474594116\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 8,    32/ 27 points] total loss per batch: 1.305\n",
      "Policy (actual, predicted): 2 3\n",
      "Policy data: tensor([0.1429, 0.0000, 0.2449, 0.1429, 0.1633, 0.1633, 0.1429, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1311, 0.0541, 0.0936, 0.2685, 0.1102, 0.0485, 0.2145, 0.0732, 0.0062],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.019966518506407738\n",
      " \n",
      "1.2425038814544678\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 9,    32/ 27 points] total loss per batch: 1.243\n",
      "Policy (actual, predicted): 0 2\n",
      "Policy data: tensor([0.2449, 0.2245, 0.1020, 0.1633, 0.0000, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0433, 0.1614, 0.5260, 0.1558, 0.0567, 0.0012, 0.0155, 0.0208, 0.0192],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.8075016736984253\n",
      " \n",
      "1.1467570066452026\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 10,    32/ 27 points] total loss per batch: 1.147\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0585, 0.0945, 0.0131, 0.0215, 0.0102, 0.7727, 0.0134, 0.0087, 0.0073],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.36708152294158936\n",
      " \n",
      "1.0315439701080322\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 11,    32/ 27 points] total loss per batch: 1.032\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.2449, 0.4286, 0.0000, 0.1837, 0.1429, 0.0000])\n",
      "Policy pred: tensor([3.8525e-04, 9.3473e-04, 6.6948e-03, 8.1459e-02, 3.1782e-01, 2.2839e-05,\n",
      "        3.0836e-01, 2.8429e-01, 3.2778e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.027554746717214584\n",
      " \n",
      "0.9646639823913574\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 12,    32/ 27 points] total loss per batch: 0.965\n",
      "Policy (actual, predicted): 0 2\n",
      "Policy data: tensor([0.2449, 0.2245, 0.1020, 0.1633, 0.0000, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0513, 0.1612, 0.4927, 0.1700, 0.0447, 0.0014, 0.0247, 0.0457, 0.0083],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9515792727470398\n",
      " \n",
      "0.8924721479415894\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 13,    32/ 27 points] total loss per batch: 0.892\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([4.5626e-02, 9.2005e-01, 4.4247e-03, 1.3716e-03, 7.7717e-03, 1.7144e-04,\n",
      "        1.7627e-03, 1.2449e-02, 6.3702e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.04457951709628105\n",
      " \n",
      "0.8679561614990234\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 14,    32/ 27 points] total loss per batch: 0.868\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.3061, 0.1224, 0.1837, 0.1429, 0.0000, 0.1429, 0.1020, 0.0000])\n",
      "Policy pred: tensor([0.1443, 0.3437, 0.0707, 0.1267, 0.1345, 0.0045, 0.1116, 0.0614, 0.0025],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0022845526691526175\n",
      " \n",
      "0.8289047479629517\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 15,    32/ 27 points] total loss per batch: 0.829\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0408, 0.9388, 0.0000, 0.0204, 0.0000, 0.0000])\n",
      "Policy pred: tensor([7.2320e-05, 2.0000e-05, 1.9854e-02, 5.2633e-03, 9.6686e-01, 6.0106e-09,\n",
      "        7.4173e-03, 5.1723e-04, 2.9641e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.09880883246660233\n",
      " \n",
      "0.8245797157287598\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 16,    32/ 27 points] total loss per batch: 0.825\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0319, 0.0513, 0.0297, 0.0618, 0.0338, 0.7064, 0.0464, 0.0329, 0.0057],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.6479925513267517\n",
      " \n",
      "0.8116751313209534\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 17,    32/ 27 points] total loss per batch: 0.812\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([2.1225e-03, 9.8214e-01, 1.2366e-02, 9.7974e-05, 3.6747e-04, 3.6748e-05,\n",
      "        2.9472e-04, 1.5254e-03, 1.0517e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0023558319080621004\n",
      " \n",
      "0.7902801632881165\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 18,    32/ 27 points] total loss per batch: 0.790\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0408, 0.6327])\n",
      "Policy pred: tensor([4.9302e-03, 2.8850e-01, 2.8418e-04, 1.3585e-03, 1.9773e-04, 8.8906e-03,\n",
      "        1.2730e-03, 3.0531e-02, 6.6403e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.001795923220925033\n",
      " \n",
      "0.7878214716911316\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 19,    32/ 27 points] total loss per batch: 0.788\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.3265, 0.2857, 0.0408, 0.0612, 0.0612, 0.0000, 0.0612, 0.0816, 0.0816])\n",
      "Policy pred: tensor([0.3940, 0.2927, 0.0253, 0.0466, 0.0457, 0.0126, 0.0537, 0.0824, 0.0471],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.015140598639845848\n",
      " \n",
      "0.7862589955329895\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 20,    32/ 27 points] total loss per batch: 0.786\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.3878, 0.0000, 0.2653, 0.0000, 0.2245, 0.1224, 0.0000])\n",
      "Policy pred: tensor([2.3202e-04, 4.7720e-03, 4.3678e-01, 1.1030e-02, 2.1469e-01, 2.6662e-05,\n",
      "        1.7561e-01, 1.5678e-01, 7.2843e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9980565905570984\n",
      " \n",
      "0.7779335379600525\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 21,    32/ 27 points] total loss per batch: 0.778\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.0000, 0.3878, 0.0000, 0.2653, 0.0000, 0.2245, 0.1224, 0.0000])\n",
      "Policy pred: tensor([1.6080e-04, 4.4939e-03, 5.4601e-01, 7.6610e-03, 1.5255e-01, 1.6786e-05,\n",
      "        1.8685e-01, 1.0221e-01, 5.3742e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9983331561088562\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782956600189209\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 22,    32/ 27 points] total loss per batch: 0.783\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.2449, 0.4286, 0.0000, 0.1837, 0.1429, 0.0000])\n",
      "Policy pred: tensor([3.7177e-04, 4.0477e-03, 1.2427e-02, 2.5775e-01, 3.4367e-01, 1.0651e-04,\n",
      "        2.7119e-01, 1.1021e-01, 2.3221e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.02301076613366604\n",
      " \n",
      "0.7730065584182739\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 23,    32/ 27 points] total loss per batch: 0.773\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.3061, 0.0000, 0.0000, 0.0000, 0.0408, 0.0000, 0.0408, 0.6122])\n",
      "Policy pred: tensor([1.9967e-03, 3.5519e-01, 2.2011e-04, 9.4119e-04, 1.3490e-04, 1.5035e-02,\n",
      "        1.2861e-03, 4.7140e-02, 5.7806e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.009950137697160244\n",
      " \n",
      "0.7694835662841797\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 24,    32/ 27 points] total loss per batch: 0.769\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.3061, 0.1224, 0.1837, 0.1429, 0.0000, 0.1429, 0.1020, 0.0000])\n",
      "Policy pred: tensor([0.0032, 0.3504, 0.1269, 0.1486, 0.1549, 0.0010, 0.1236, 0.0888, 0.0026],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.015961602330207825\n",
      " \n",
      "0.7722027897834778\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 25,    32/ 27 points] total loss per batch: 0.772\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5714, 0.4286, 0.0000])\n",
      "Policy pred: tensor([6.4119e-09, 1.5716e-07, 3.4513e-05, 2.1853e-03, 1.7486e-03, 1.8614e-08,\n",
      "        6.4002e-01, 3.5601e-01, 3.5091e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.007912448607385159\n",
      " \n",
      "0.7667466402053833\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 26,    32/ 27 points] total loss per batch: 0.767\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([1.9791e-04, 1.5877e-06, 9.3114e-05, 9.9563e-01, 1.8711e-04, 5.8142e-06,\n",
      "        3.0436e-03, 8.4051e-04, 1.9773e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.002982816193252802\n",
      " \n",
      "0.7644187211990356\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 27,    32/ 27 points] total loss per batch: 0.764\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5714, 0.4286, 0.0000])\n",
      "Policy pred: tensor([4.3279e-09, 1.6092e-07, 2.7178e-05, 3.0092e-03, 1.0714e-03, 2.0232e-08,\n",
      "        5.0769e-01, 4.8820e-01, 3.2276e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.004509872756898403\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:42 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7645788788795471\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 28,    32/ 27 points] total loss per batch: 0.765\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0612, 0.0408, 0.0204, 0.0408, 0.0408, 0.7551, 0.0204, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0647, 0.0410, 0.0208, 0.0378, 0.0363, 0.7549, 0.0242, 0.0147, 0.0057],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.5242260098457336\n",
      " \n",
      "0.7618901133537292\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 29,    32/ 27 points] total loss per batch: 0.762\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.7143, 0.0000, 0.0000, 0.2857, 0.0000, 0.0000])\n",
      "Policy pred: tensor([6.3700e-04, 3.3552e-06, 1.8631e-04, 7.3730e-01, 3.5061e-03, 2.9629e-05,\n",
      "        2.5758e-01, 7.5325e-04, 8.1422e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.00647477712482214\n",
      " \n",
      "0.7619786262512207\n",
      "Update step size: 1\n",
      "[Iteration 7] Process ID: 11022 [Epoch: 30,    32/ 27 points] total loss per batch: 0.762\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.2449, 0.2245, 0.1020, 0.1633, 0.0000, 0.0000, 0.1429, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.2058, 0.2011, 0.1185, 0.1909, 0.0039, 0.0006, 0.1506, 0.1269, 0.0018],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9709389805793762\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:42 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:39:42 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter8.pth.tar\n",
      "[I 200412 12:39:42 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter7.pth.tar\n",
      "[I 200412 12:39:43 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:39:43 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 7.0305652e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.2192075e-15 0.0000000e+00 1.2484685e-12 9.9929690e-01] black = best\n",
      "Policy :  [3.3000682e-02 3.3000681e-12 1.9486572e-07 1.1506626e-02 3.3792698e-09\n",
      " 9.3218760e-04 1.9486572e-07 9.5456010e-01 0.0000000e+00] white = current\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [2.5655099e-04 0.0000000e+00 9.7519875e-04 5.4916869e-05 5.4916869e-05\n",
      " 9.9860352e-01 5.4916869e-05 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = best\n",
      "Policy :  [3.3104053e-04 0.0000000e+00 5.7407492e-06 9.7220093e-01 0.0000000e+00\n",
      " 0.0000000e+00 2.7462270e-02 0.0000000e+00 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:43 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.7731218 0.        0.2268782 0.        0.        0.        0.\n",
      " 0.        0.       ] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 4.1313224e-06 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999589e-01 0.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 9.7560976e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.7324611e-12 0.0000000e+00 1.7324611e-12 9.9902439e-01] black = best\n",
      "Policy :  [3.3000682e-02 3.3000681e-12 1.9486572e-07 1.1506626e-02 3.3792698e-09\n",
      " 9.3218760e-04 1.9486572e-07 9.5456010e-01 0.0000000e+00] white = current\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [2.5655099e-04 0.0000000e+00 9.7519875e-04 5.4916869e-05 5.4916869e-05\n",
      " 9.9860352e-01 5.4916869e-05 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = best\n",
      "Policy :  [3.3104053e-04 0.0000000e+00 5.7407492e-06 9.7220093e-01 0.0000000e+00\n",
      " 0.0000000e+00 2.7462270e-02 0.0000000e+00 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:45 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.94668865 0.         0.05331136 0.         0.         0.\n",
      " 0.         0.         0.        ] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.2284384e-06 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999875e-01 0.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 7.0305652e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 1.2192075e-15 0.0000000e+00 1.2484685e-12 9.9929690e-01] black = best\n",
      "Policy :  [5.9714769e-03 1.7126028e-12 1.0112748e-07 5.9714769e-03 1.7537053e-09\n",
      " 4.8376792e-04 1.0112748e-07 9.8757309e-01 0.0000000e+00] white = current\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [2.5668368e-04 0.0000000e+00 2.5668368e-04 5.4945274e-05 2.5668368e-04\n",
      " 9.9912006e-01 5.4945274e-05 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = best\n",
      "Policy :  [2.0543570e-04 0.0000000e+00 5.7537483e-07 9.8275161e-01 0.0000000e+00\n",
      " 0.0000000e+00 1.7042417e-02 0.0000000e+00 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:45 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.7731218 0.        0.2268782 0.        0.        0.        0.\n",
      " 0.        0.       ] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 4.1313224e-06 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999589e-01 0.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [5.9714769e-03 1.7126028e-12 1.0112748e-07 5.9714769e-03 1.7537053e-09\n",
      " 4.8376792e-04 1.0112748e-07 9.8757309e-01 0.0000000e+00] white = current\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [2.5655099e-04 0.0000000e+00 9.7519875e-04 5.4916869e-05 5.4916869e-05\n",
      " 9.9860352e-01 5.4916869e-05 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = best\n",
      "Policy :  [3.3104053e-04 0.0000000e+00 5.7407492e-06 9.7220093e-01 0.0000000e+00\n",
      " 0.0000000e+00 2.7462270e-02 0.0000000e+00 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:46 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.9897237  0.         0.01027631 0.         0.         0.\n",
      " 0.         0.         0.        ] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.2876258e-05 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9998719e-01 0.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Policy :  [0.0000000e+00 1.3396788e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.2476731e-12 9.9866033e-01] black = best\n",
      "Policy :  [3.3000682e-02 3.3000681e-12 1.9486572e-07 1.1506626e-02 3.3792698e-09\n",
      " 9.3218760e-04 1.9486572e-07 9.5456010e-01 0.0000000e+00] white = current\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "Policy :  [2.5655099e-04 0.0000000e+00 9.7519875e-04 5.4916869e-05 5.4916869e-05\n",
      " 9.9860352e-01 5.4916869e-05 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = best\n",
      "Policy :  [3.2224928e-04 0.0000000e+00 9.0254093e-07 9.4638276e-01 0.0000000e+00\n",
      " 0.0000000e+00 5.3294141e-02 0.0000000e+00 0.0000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:47 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:39:47 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:39:47 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter8.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.94668865 0.         0.05331136 0.         0.         0.\n",
      " 0.         0.         0.        ] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 4.1313224e-06 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999589e-01 0.0000000e+00 0.0000000e+00] white = current\n",
      "current wins!\n",
      "Current_net wins ratio 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:50 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:39:50 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:39:50 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_8/.\n",
      "[I 200412 12:39:51 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.5999515056610107\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 1,    32/ 26 points] total loss per batch: 2.600\n",
      "Policy (actual, predicted): 7 6\n",
      "Policy data: tensor([0.2041, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.2857, 0.0000])\n",
      "Policy pred: tensor([0.1550, 0.1003, 0.1497, 0.0987, 0.1026, 0.0876, 0.1762, 0.0600, 0.0700],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.08258834481239319\n",
      " \n",
      "2.5645124912261963\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 2,    32/ 26 points] total loss per batch: 2.565\n",
      "Policy (actual, predicted): 3 1\n",
      "Policy data: tensor([0.2041, 0.0000, 0.2245, 0.3061, 0.1429, 0.1224, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0108, 0.3789, 0.1699, 0.0482, 0.0691, 0.2428, 0.0182, 0.0372, 0.0249],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.3546757996082306\n",
      " \n",
      "2.5427052974700928\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 3,    32/ 26 points] total loss per batch: 2.543\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.2857, 0.0000, 0.0000, 0.3061, 0.1429, 0.2653, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1508, 0.0162, 0.0493, 0.3403, 0.1478, 0.0524, 0.0698, 0.0956, 0.0780],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.26849815249443054\n",
      " \n",
      "2.1169934272766113\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 4,    32/ 26 points] total loss per batch: 2.117\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([0.0477, 0.1585, 0.6131, 0.0050, 0.0610, 0.0493, 0.0358, 0.0136, 0.0160],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.07490864396095276\n",
      " \n",
      "1.9978258609771729\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 5,    32/ 26 points] total loss per batch: 1.998\n",
      "Policy (actual, predicted): 6 5\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2653, 0.0000, 0.2653, 0.2041, 0.2653, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0515, 0.0361, 0.1474, 0.0790, 0.1501, 0.4433, 0.0666, 0.0148, 0.0111],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.47855451703071594\n",
      " \n",
      "1.8978372812271118\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 6,    32/ 26 points] total loss per batch: 1.898\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.2041, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.2857, 0.0000])\n",
      "Policy pred: tensor([0.1495, 0.0360, 0.0724, 0.1345, 0.0752, 0.1123, 0.0667, 0.3123, 0.0413],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.38885438442230225\n",
      " \n",
      "1.7452454566955566\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 7,    32/ 26 points] total loss per batch: 1.745\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.2041, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.2857, 0.0000])\n",
      "Policy pred: tensor([0.1907, 0.0297, 0.0716, 0.1328, 0.0501, 0.0752, 0.0467, 0.3641, 0.0391],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.33610793948173523\n",
      " \n",
      "1.6299521923065186\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 8,    32/ 26 points] total loss per batch: 1.630\n",
      "Policy (actual, predicted): 4 2\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([3.7791e-03, 3.8225e-02, 4.2634e-01, 9.1868e-03, 2.4284e-01, 2.2556e-01,\n",
      "        5.3794e-02, 8.8242e-05, 1.8983e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.14208745956420898\n",
      " \n",
      "1.5446655750274658\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 9,    32/ 26 points] total loss per batch: 1.545\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.1837, 0.4694, 0.0000, 0.1633, 0.0000, 0.0204, 0.1633, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1734, 0.3248, 0.0432, 0.1332, 0.0535, 0.0372, 0.0893, 0.0795, 0.0660],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.6717180013656616\n",
      " \n",
      "1.4696934223175049\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 10,    32/ 26 points] total loss per batch: 1.470\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.1429, 0.0000, 0.1633, 0.1020, 0.1429, 0.3265, 0.1224, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1562, 0.0110, 0.0959, 0.1728, 0.2092, 0.2789, 0.0726, 0.0029, 0.0005],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.8512011170387268\n",
      " \n",
      "1.4333572387695312\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 11,    32/ 26 points] total loss per batch: 1.433\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([2.3909e-03, 2.3337e-02, 6.1758e-01, 1.0039e-03, 2.4084e-01, 7.8360e-02,\n",
      "        3.6451e-02, 2.3340e-05, 1.4539e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.04946313053369522\n",
      " \n",
      "1.377625823020935\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 12,    32/ 26 points] total loss per batch: 1.378\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.1429, 0.0000, 0.1633, 0.1020, 0.1429, 0.3265, 0.1224, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.2587e-01, 6.5037e-03, 9.9237e-02, 8.6677e-02, 2.4248e-01, 2.7187e-01,\n",
      "        6.6393e-02, 8.7799e-04, 1.0039e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9055263996124268\n",
      " \n",
      "1.3542951345443726\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 13,    32/ 26 points] total loss per batch: 1.354\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1837, 0.6735])\n",
      "Policy pred: tensor([0.0034, 0.1403, 0.0018, 0.0440, 0.0006, 0.0039, 0.0025, 0.2575, 0.5461],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.010279211215674877\n",
      " \n",
      "1.3608654737472534\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 14,    32/ 26 points] total loss per batch: 1.361\n",
      "Policy (actual, predicted): 5 2\n",
      "Policy data: tensor([0.3265, 0.0408, 0.2449, 0.0000, 0.0000, 0.3878, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.4146e-01, 2.9454e-02, 3.5460e-01, 1.9448e-02, 5.3658e-02, 2.2452e-01,\n",
      "        7.1787e-02, 4.7226e-03, 3.4447e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.031840503215789795\n",
      " \n",
      "1.3707714080810547\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 15,    32/ 26 points] total loss per batch: 1.371\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.2857, 0.0000, 0.0000, 0.3061, 0.1429, 0.2653, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.5429e-01, 4.2984e-03, 3.2903e-02, 4.1274e-01, 1.4877e-01, 1.3437e-01,\n",
      "        1.1275e-02, 1.3075e-03, 5.6289e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9784717559814453\n",
      " \n",
      "1.2830442190170288\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 16,    32/ 26 points] total loss per batch: 1.283\n",
      "Policy (actual, predicted): 6 5\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2653, 0.0000, 0.2653, 0.2041, 0.2653, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.7731e-02, 6.1891e-03, 1.1375e-01, 2.2151e-03, 1.2758e-01, 5.3850e-01,\n",
      "        1.9361e-01, 3.9576e-04, 2.0589e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.028237463906407356\n",
      " \n",
      "1.2935105562210083\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 17,    32/ 26 points] total loss per batch: 1.294\n",
      "Policy (actual, predicted): 4 2\n",
      "Policy data: tensor([0.2449, 0.0408, 0.2041, 0.0000, 0.2857, 0.0816, 0.1429, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.8096e-01, 2.2682e-02, 2.8107e-01, 1.4545e-02, 1.7174e-01, 1.4138e-01,\n",
      "        1.7868e-01, 8.8275e-03, 1.2424e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.08956657350063324\n",
      " \n",
      "1.246591329574585\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 18,    32/ 26 points] total loss per batch: 1.247\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.6122, 0.0408, 0.0000, 0.0612, 0.0408, 0.2449, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0397, 0.5011, 0.0475, 0.0178, 0.0135, 0.0590, 0.3170, 0.0032, 0.0012],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.024202797561883926\n",
      " \n",
      "1.249693512916565\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 19,    32/ 26 points] total loss per batch: 1.250\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.1429, 0.0000, 0.1633, 0.1020, 0.1429, 0.3265, 0.1224, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.0669e-01, 4.9746e-03, 1.0762e-01, 5.4127e-02, 1.7898e-01, 3.9269e-01,\n",
      "        5.3051e-02, 1.8470e-03, 1.6367e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.8793994188308716\n",
      " \n",
      "1.2444729804992676\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 20,    32/ 26 points] total loss per batch: 1.244\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([5.8399e-07, 2.6857e-05, 3.4237e-03, 1.0945e-06, 9.8000e-01, 1.6357e-02,\n",
      "        1.9574e-04, 3.3725e-09, 3.5060e-10], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.029824767261743546\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2135281562805176\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 21,    32/ 26 points] total loss per batch: 1.214\n",
      "Policy (actual, predicted): 3 0\n",
      "Policy data: tensor([0.2857, 0.0000, 0.0000, 0.3061, 0.1429, 0.2653, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.1927e-01, 2.3099e-03, 4.7565e-02, 2.4974e-01, 1.5423e-01, 2.0897e-01,\n",
      "        1.4699e-02, 3.2044e-03, 1.2901e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9713460206985474\n",
      " \n",
      "1.217167615890503\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 22,    32/ 26 points] total loss per batch: 1.217\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([1.5838e-04, 1.0810e-02, 9.7944e-01, 1.1673e-06, 5.4238e-03, 4.0300e-03,\n",
      "        1.3217e-04, 1.3198e-07, 4.6445e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0054219504818320274\n",
      " \n",
      "1.1986565589904785\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 23,    32/ 26 points] total loss per batch: 1.199\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1837, 0.6735])\n",
      "Policy pred: tensor([6.1856e-04, 1.1852e-01, 3.2632e-04, 6.8350e-03, 2.1063e-04, 1.8059e-02,\n",
      "        1.1796e-03, 2.1818e-01, 6.3607e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0016589522128924727\n",
      " \n",
      "1.194629192352295\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 24,    32/ 26 points] total loss per batch: 1.195\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1837, 0.6735])\n",
      "Policy pred: tensor([4.3630e-04, 1.2095e-01, 2.2669e-04, 4.0743e-03, 1.4628e-04, 1.4955e-02,\n",
      "        7.8941e-04, 1.3452e-01, 7.2390e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0018070990918204188\n",
      " \n",
      "1.1834607124328613\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 25,    32/ 26 points] total loss per batch: 1.183\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1837, 0.6735])\n",
      "Policy pred: tensor([3.4945e-04, 1.1177e-01, 1.9919e-04, 3.8517e-03, 1.3533e-04, 1.6180e-02,\n",
      "        5.9302e-04, 1.4604e-01, 7.2089e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.007594379596412182\n",
      " \n",
      "1.1787350177764893\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 26,    32/ 26 points] total loss per batch: 1.179\n",
      "Policy (actual, predicted): 5 5\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3265, 0.6735, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.1340e-05, 2.9427e-05, 1.6876e-03, 1.2486e-04, 3.3840e-01, 6.5634e-01,\n",
      "        3.4122e-03, 9.6894e-07, 6.0470e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.005404337774962187\n",
      " \n",
      "1.1771514415740967\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 27,    32/ 26 points] total loss per batch: 1.177\n",
      "Policy (actual, predicted): 5 0\n",
      "Policy data: tensor([0.3265, 0.0408, 0.2449, 0.0000, 0.0000, 0.3878, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.9661e-01, 3.3421e-02, 2.5896e-01, 2.8651e-03, 1.1367e-02, 2.8992e-01,\n",
      "        6.3963e-03, 4.3421e-04, 2.1462e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.004664476495236158\n",
      " \n",
      "1.172119379043579\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 28,    32/ 26 points] total loss per batch: 1.172\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.8163, 0.1429, 0.0000, 0.0000, 0.0408, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.6063e-03, 8.1206e-01, 1.3151e-01, 1.0026e-04, 4.7485e-03, 3.4855e-02,\n",
      "        1.3656e-02, 2.1784e-04, 2.4958e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0032878622878342867\n",
      " \n",
      "1.1725661754608154\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 29,    32/ 26 points] total loss per batch: 1.173\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.1997, 0.0224, 0.0614, 0.1702, 0.0452, 0.1690, 0.0702, 0.2616, 0.0003],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.29214614629745483\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:53 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1688385009765625\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 30,    32/ 26 points] total loss per batch: 1.169\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0000, 0.8163, 0.1429, 0.0000, 0.0000, 0.0408, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.6262e-03, 8.0680e-01, 1.4712e-01, 5.8466e-05, 3.2727e-03, 3.4058e-02,\n",
      "        6.6119e-03, 2.3828e-04, 2.1724e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.000854231184348464\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:53 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:39:53 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter9.pth.tar\n",
      "[I 200412 12:39:53 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter8.pth.tar\n",
      "[I 200412 12:39:53 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:39:53 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [2.9154454e-09 1.6811927e-07 1.6811927e-07 1.7215413e-04 1.6811927e-07\n",
      " 1.6811927e-07 1.6811927e-07 1.7628583e-01 8.2354122e-01] black = current\n",
      "Policy :  [2.9358617e-07 2.8670524e-10 1.6929658e-05 2.7342344e-06 1.6532869e-08\n",
      " 3.0063224e-04 1.6532869e-08 0.0000000e+00 9.9967939e-01] white = best\n",
      "Policy :  [1.4253542e-09 2.5311049e-08 2.3572753e-07 2.5918514e-05 1.4253542e-09\n",
      " 9.9997377e-01 1.4253542e-09 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [9.9125826e-01 3.6796483e-03 3.6796483e-03 9.6802565e-04 2.0721393e-04\n",
      " 0.0000000e+00 2.0721393e-04 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 9.9842685e-01 6.2765821e-08 1.5525663e-03 3.6193935e-06\n",
      " 0.0000000e+00 1.6908447e-05 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 2.9367487e-07 9.9998140e-01 1.6934773e-05\n",
      " 0.0000000e+00 1.3719387e-06 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 5.3703509e-08 0.0000000e+00 9.9997687e-01\n",
      " 0.0000000e+00 2.3036528e-05 0.0000000e+00 0.0000000e+00] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:54 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.000000e+00 0.000000e+00 3.325256e-07 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 9.999997e-01 0.000000e+00 0.000000e+00] white = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 5.3704273e-08 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 8.8817048e-16 0.0000000e+00 8.8817051e-06 9.9999106e-01] black = best\n",
      "Policy :  [4.7382217e-14 0.0000000e+00 0.0000000e+00 4.7382217e-14 0.0000000e+00\n",
      " 4.7382217e-14 0.0000000e+00 1.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [6.7960450e-06 9.9667645e-01 0.0000000e+00 3.3167242e-03 2.4058904e-14\n",
      " 2.4058904e-14 2.5227589e-08 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [4.8554690e-15 0.0000000e+00 1.3715498e-06 3.0063785e-04 4.9720002e-12\n",
      " 9.9969804e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [2.8679720e-10 0.0000000e+00 1.0000000e+00 2.8679720e-10 2.8007539e-13\n",
      " 0.0000000e+00 1.6150558e-11 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [2.1806330e-10 0.0000000e+00 0.0000000e+00 9.9999976e-01 2.2329682e-07\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [7.811984e-11 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00\n",
      " 0.000000e+00 7.811984e-11 0.000000e+00 0.000000e+00] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:55 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] white = current\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 1. 0. 0.] black = best\n",
      "None wins!\n",
      "Policy :  [2.9154454e-09 1.6811927e-07 1.6811927e-07 1.7215413e-04 1.6811927e-07\n",
      " 1.6811927e-07 1.6811927e-07 1.7628583e-01 8.2354122e-01] black = current\n",
      "Policy :  [1.7099475e-07 1.6698706e-10 9.8604196e-06 1.5925128e-06 9.6293160e-09\n",
      " 1.7509863e-04 1.6698706e-10 0.0000000e+00 9.9981326e-01] white = best\n",
      "Policy :  [1.4253542e-09 2.5311049e-08 2.3572753e-07 2.5918514e-05 1.4253542e-09\n",
      " 9.9997377e-01 1.4253542e-09 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [9.9125826e-01 3.6796483e-03 3.6796483e-03 9.6802565e-04 2.0721393e-04\n",
      " 0.0000000e+00 2.0721393e-04 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 9.9842685e-01 6.2765821e-08 1.5525663e-03 3.6193935e-06\n",
      " 0.0000000e+00 1.6908447e-05 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 2.9367487e-07 9.9998140e-01 1.6934773e-05\n",
      " 0.0000000e+00 1.3719387e-06 0.0000000e+00 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:56 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 7.3767104e-08 0.0000000e+00 9.9992436e-01\n",
      " 0.0000000e+00 7.5537515e-05 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 1.2284384e-06 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 9.9999875e-01 0.0000000e+00 0.0000000e+00] white = best\n",
      "best wins!\n",
      "Policy :  [0.0000000e+00 3.9479541e-08 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 6.5291947e-16 0.0000000e+00 2.2765894e-06 9.9999774e-01] black = best\n",
      "Policy :  [4.7382217e-14 0.0000000e+00 0.0000000e+00 4.7382217e-14 0.0000000e+00\n",
      " 4.7382217e-14 0.0000000e+00 1.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [0.0000000e+00 9.9659967e-01 1.4205448e-09 3.3164688e-03 2.4057050e-14\n",
      " 0.0000000e+00 8.3881750e-05 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [4.8554690e-15 0.0000000e+00 1.3715498e-06 3.0063785e-04 4.9720002e-12\n",
      " 9.9969804e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [2.8679720e-10 0.0000000e+00 1.0000000e+00 2.8679720e-10 2.8007539e-13\n",
      " 0.0000000e+00 1.6150558e-11 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [2.1806330e-10 0.0000000e+00 0.0000000e+00 9.9999976e-01 2.2329682e-07\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [7.811984e-11 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00\n",
      " 0.000000e+00 7.811984e-11 0.000000e+00 0.000000e+00] black = best\n",
      "Policy :  [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 1.391984e-12 0.000000e+00 0.000000e+00] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:57 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 0. 1. 0. 0.] black = best\n",
      "None wins!\n",
      "Policy :  [2.9154454e-09 1.6811927e-07 1.6811927e-07 1.7215413e-04 1.6811927e-07\n",
      " 1.6811927e-07 1.6811927e-07 1.7628583e-01 8.2354122e-01] black = current\n",
      "Policy :  [3.3000682e-02 3.3000681e-12 1.9486572e-07 1.1506626e-02 3.3792698e-09\n",
      " 9.3218760e-04 1.9486572e-07 9.5456010e-01 0.0000000e+00] white = best\n",
      "Policy :  [1.6150484e-11 1.5402301e-07 1.6538095e-08 9.9999535e-01 9.3131824e-10\n",
      " 4.4551875e-06 1.6538095e-08 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [9.5458783e-02 1.5787137e-09 1.5417126e-02 0.0000000e+00 8.8902920e-01\n",
      " 1.6166028e-06 9.3221468e-05 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [6.2427887e-07 2.0758875e-03 5.8140499e-06 0.0000000e+00 0.0000000e+00\n",
      " 6.5460384e-01 3.4331384e-01 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [1.5077303e-01 1.4041833e-10 8.4905596e-03 0.0000000e+00 0.0000000e+00\n",
      " 8.4073639e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [1.000000e-10 9.313226e-10 1.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:39:58 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:39:58 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:39:58 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:39:58 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter8.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1.0000000e+00 1.9468002e-14 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Current_net wins ratio 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:01 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:40:01 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:40:01 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_8/.\n",
      "[I 200412 12:40:01 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "3.078748941421509\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 1,    32/ 26 points] total loss per batch: 3.079\n",
      "Policy (actual, predicted): 7 1\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.0821, 0.1519, 0.0936, 0.1128, 0.1073, 0.1120, 0.1356, 0.1097, 0.0949],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.13594377040863037\n",
      " \n",
      "2.617912769317627\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 2,    32/ 26 points] total loss per batch: 2.618\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.2245, 0.5102, 0.0000, 0.0000, 0.0816, 0.0204, 0.1633, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0626, 0.3403, 0.1766, 0.0125, 0.1420, 0.0240, 0.1869, 0.0132, 0.0420],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 -0.4425124526023865\n",
      " \n",
      "3.20455002784729\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 3,    32/ 26 points] total loss per batch: 3.205\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0816, 0.0000, 0.0000, 0.5918, 0.0204, 0.3061, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1199, 0.1165, 0.0788, 0.0954, 0.1645, 0.0841, 0.1462, 0.0919, 0.1028],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.04677712917327881\n",
      " \n",
      "2.2808663845062256\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 4,    32/ 26 points] total loss per batch: 2.281\n",
      "Policy (actual, predicted): 4 2\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([9.4698e-04, 2.7683e-04, 3.8767e-01, 1.9525e-04, 2.4619e-01, 1.9466e-04,\n",
      "        3.3242e-01, 1.5574e-04, 3.1955e-02], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9755162000656128\n",
      " \n",
      "1.9716540575027466\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 5,    32/ 26 points] total loss per batch: 1.972\n",
      "Policy (actual, predicted): 4 6\n",
      "Policy data: tensor([0.1633, 0.0000, 0.2245, 0.0000, 0.3673, 0.0000, 0.2449, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0551, 0.0748, 0.1857, 0.0209, 0.2131, 0.0739, 0.2866, 0.0204, 0.0695],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.7809594869613647\n",
      " \n",
      "1.816111445426941\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 6,    32/ 26 points] total loss per batch: 1.816\n",
      "Policy (actual, predicted): 8 1\n",
      "Policy data: tensor([0.0816, 0.0408, 0.1224, 0.1020, 0.0612, 0.1633, 0.0612, 0.0000, 0.3673])\n",
      "Policy pred: tensor([0.1502, 0.1663, 0.1234, 0.0473, 0.0851, 0.1030, 0.1243, 0.0649, 0.1355],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.7792667746543884\n",
      " \n",
      "1.6543891429901123\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 7,    32/ 26 points] total loss per batch: 1.654\n",
      "Policy (actual, predicted): 7 0\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.2087, 0.1299, 0.0687, 0.0845, 0.0787, 0.1048, 0.0810, 0.1303, 0.1133],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.15239141881465912\n",
      " \n",
      "1.59514319896698\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 8,    32/ 26 points] total loss per batch: 1.595\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([2.8943e-03, 1.3851e-02, 5.6401e-01, 3.3698e-05, 3.5975e-01, 4.1006e-03,\n",
      "        5.5203e-02, 2.6863e-05, 1.3547e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9514841437339783\n",
      " \n",
      "1.5378152132034302\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 9,    32/ 26 points] total loss per batch: 1.538\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.1837, 0.4490, 0.0000, 0.2449, 0.0204, 0.0000, 0.1020, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1487, 0.4536, 0.0320, 0.0653, 0.0851, 0.0230, 0.1502, 0.0306, 0.0115],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.48092368245124817\n",
      " \n",
      "1.5505281686782837\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 10,    32/ 26 points] total loss per batch: 1.551\n",
      "Policy (actual, predicted): 4 0\n",
      "Policy data: tensor([0.2449, 0.0408, 0.2041, 0.0000, 0.2857, 0.0816, 0.1429, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.2040, 0.1177, 0.0887, 0.1058, 0.1888, 0.1142, 0.1415, 0.0308, 0.0084],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.5874741673469543\n",
      " \n",
      "1.4266324043273926\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 11,    32/ 26 points] total loss per batch: 1.427\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([2.2419e-06, 4.7484e-05, 3.3158e-02, 8.5477e-10, 9.4627e-01, 1.6610e-05,\n",
      "        2.0507e-02, 6.0240e-11, 2.6632e-08], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9987640380859375\n",
      " \n",
      "1.3975334167480469\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 12,    32/ 26 points] total loss per batch: 1.398\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.2449, 0.0408, 0.2041, 0.0000, 0.2857, 0.0816, 0.1429, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1667, 0.1457, 0.0731, 0.0422, 0.3221, 0.0883, 0.1544, 0.0060, 0.0016],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.8433120846748352\n",
      " \n",
      "1.3720976114273071\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 13,    32/ 26 points] total loss per batch: 1.372\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1837, 0.6735])\n",
      "Policy pred: tensor([0.0082, 0.1395, 0.0065, 0.0236, 0.0035, 0.0125, 0.0063, 0.1903, 0.6095],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.020661743357777596\n",
      " \n",
      "1.3736298084259033\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 14,    32/ 26 points] total loss per batch: 1.374\n",
      "Policy (actual, predicted): 4 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2857, 0.0000, 0.3265, 0.2041, 0.1837, 0.0000, 0.0000])\n",
      "Policy pred: tensor([4.0858e-02, 1.2197e-02, 2.4399e-01, 9.3920e-04, 2.4467e-01, 1.9438e-01,\n",
      "        2.6143e-01, 9.4211e-05, 1.4535e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9896982908248901\n",
      " \n",
      "1.3190792798995972\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 15,    32/ 26 points] total loss per batch: 1.319\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0816, 0.0000, 0.0000, 0.5918, 0.0204, 0.3061, 0.0000, 0.0000])\n",
      "Policy pred: tensor([4.6712e-02, 1.1938e-01, 4.3256e-02, 5.6214e-03, 5.5018e-01, 8.3589e-02,\n",
      "        1.5088e-01, 2.1829e-04, 1.5900e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9355816841125488\n",
      " \n",
      "1.3109530210494995\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 16,    32/ 26 points] total loss per batch: 1.311\n",
      "Policy (actual, predicted): 4 6\n",
      "Policy data: tensor([0.1633, 0.0000, 0.2245, 0.0000, 0.3673, 0.0000, 0.2449, 0.0000, 0.0000])\n",
      "Policy pred: tensor([5.2707e-02, 1.9630e-02, 2.0376e-01, 1.4556e-03, 2.1761e-01, 4.1619e-03,\n",
      "        5.0061e-01, 7.1149e-06, 6.2153e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9901753664016724\n",
      " \n",
      "1.3841135501861572\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 17,    32/ 26 points] total loss per batch: 1.384\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.3469, 0.0408, 0.2857, 0.0000, 0.0816, 0.1633, 0.0816, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.3983, 0.0246, 0.2871, 0.0259, 0.0793, 0.1137, 0.0426, 0.0016, 0.0268],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9879992604255676\n",
      " \n",
      "1.293438196182251\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 18,    32/ 26 points] total loss per batch: 1.293\n",
      "Policy (actual, predicted): 1 1\n",
      "Policy data: tensor([0.0612, 0.7959, 0.0408, 0.0000, 0.0408, 0.0000, 0.0612, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.1173e-02, 8.8722e-01, 3.4130e-02, 2.6441e-03, 6.8833e-03, 1.1161e-03,\n",
      "        4.6260e-02, 2.5496e-05, 5.5143e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.954369306564331\n",
      " \n",
      "1.333052396774292\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 19,    32/ 26 points] total loss per batch: 1.333\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.2449, 0.0408, 0.2041, 0.0000, 0.2857, 0.0816, 0.1429, 0.0000, 0.0000])\n",
      "Policy pred: tensor([1.9612e-01, 2.7878e-02, 1.3009e-01, 9.5600e-03, 3.1818e-01, 2.9687e-02,\n",
      "        2.8775e-01, 6.0970e-04, 1.3160e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.8698760271072388\n",
      " \n",
      "1.2693428993225098\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 20,    32/ 26 points] total loss per batch: 1.269\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([4.5632e-04, 8.0645e-04, 9.7247e-01, 9.9222e-07, 1.5709e-02, 2.7376e-05,\n",
      "        1.0528e-02, 5.8116e-09, 5.8589e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9890521764755249\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2856945991516113\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 21,    32/ 26 points] total loss per batch: 1.286\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0816, 0.0000, 0.0000, 0.5918, 0.0204, 0.3061, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.0423e-02, 1.4663e-01, 2.7630e-02, 2.0806e-04, 5.7666e-01, 7.7465e-02,\n",
      "        1.5092e-01, 3.5810e-05, 2.4775e-05], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.968100905418396\n",
      " \n",
      "1.2617754936218262\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 22,    32/ 26 points] total loss per batch: 1.262\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Policy pred: tensor([3.4190e-09, 2.0112e-06, 1.2345e-02, 2.2936e-14, 9.6927e-01, 8.6421e-07,\n",
      "        1.8382e-02, 9.3791e-15, 2.8789e-10], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9999686479568481\n",
      " \n",
      "1.2535043954849243\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 23,    32/ 26 points] total loss per batch: 1.254\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1837, 0.6735])\n",
      "Policy pred: tensor([1.7652e-03, 8.2600e-02, 1.1161e-03, 2.6785e-03, 5.2067e-04, 1.2723e-02,\n",
      "        3.2791e-03, 2.0696e-01, 6.8836e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.0211753249168396\n",
      " \n",
      "1.2550188302993774\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 24,    32/ 26 points] total loss per batch: 1.255\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.1837, 0.6735])\n",
      "Policy pred: tensor([1.3157e-03, 8.1189e-02, 9.6132e-04, 2.3248e-03, 4.6622e-04, 1.2952e-02,\n",
      "        2.5734e-03, 2.8321e-01, 6.1500e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.03351188451051712\n",
      " \n",
      "1.2399359941482544\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 25,    32/ 26 points] total loss per batch: 1.240\n",
      "Policy (actual, predicted): 8 8\n",
      "Policy data: tensor([0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000, 0.2041, 0.6531])\n",
      "Policy pred: tensor([1.4719e-03, 1.0654e-01, 1.0789e-03, 2.4427e-03, 5.2453e-04, 1.4518e-02,\n",
      "        2.8061e-03, 1.8224e-01, 6.8837e-01], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.01419162843376398\n",
      " \n",
      "1.2395082712173462\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 26,    32/ 26 points] total loss per batch: 1.240\n",
      "Policy (actual, predicted): 6 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2857, 0.0000, 0.0000, 0.0000, 0.7143, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.2947e-03, 9.9047e-04, 2.6982e-01, 1.2822e-05, 3.0997e-02, 1.8011e-03,\n",
      "        6.9308e-01, 5.1665e-08, 8.5600e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9931201338768005\n",
      " \n",
      "1.2346923351287842\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 27,    32/ 26 points] total loss per batch: 1.235\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.2857, 0.0000, 0.3265, 0.2041, 0.1837, 0.0000, 0.0000])\n",
      "Policy pred: tensor([6.7214e-03, 2.9143e-03, 2.7155e-01, 4.5573e-05, 3.4391e-01, 1.7708e-01,\n",
      "        1.9741e-01, 4.5799e-06, 3.5924e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9989151358604431\n",
      " \n",
      "1.2307261228561401\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 28,    32/ 26 points] total loss per batch: 1.231\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1837, 0.0000, 0.5918, 0.0000, 0.2245, 0.0000, 0.0000])\n",
      "Policy pred: tensor([4.4976e-05, 3.7283e-03, 2.4335e-01, 1.8446e-08, 5.6253e-01, 5.8355e-04,\n",
      "        1.8976e-01, 6.4107e-09, 3.6039e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9971124529838562\n",
      " \n",
      "1.2287200689315796\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 29,    32/ 26 points] total loss per batch: 1.229\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.2158, 0.0208, 0.0682, 0.2060, 0.0317, 0.1322, 0.0560, 0.2658, 0.0035],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 0.033529672771692276\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:04 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2262077331542969\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 30,    32/ 26 points] total loss per batch: 1.226\n",
      "Policy (actual, predicted): 4 4\n",
      "Policy data: tensor([0.0000, 0.0000, 0.1837, 0.0000, 0.5918, 0.0000, 0.2245, 0.0000, 0.0000])\n",
      "Policy pred: tensor([3.0988e-05, 4.9616e-03, 1.5472e-01, 1.7742e-08, 6.2371e-01, 5.6987e-04,\n",
      "        2.1600e-01, 7.4932e-09, 4.4274e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): -1.0 -0.9968595504760742\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:04 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:40:04 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter9.pth.tar\n",
      "[I 200412 12:40:04 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter8.pth.tar\n",
      "[I 200412 12:40:04 <ipython-input-20-615e5088c2dc>:53] [CPU 0]: Starting games...\n",
      "[I 200412 12:40:04 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 3.9479541e-08 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 6.5291947e-16 0.0000000e+00 2.2765894e-06 9.9999774e-01] black = best\n",
      "Policy :  [3.7735055e-11 3.7735055e-11 3.7735055e-11 3.7735055e-11 2.1249935e-02\n",
      " 3.7735055e-11 3.7735055e-11 9.7875011e-01 0.0000000e+00] white = current\n",
      "Policy :  [5.4936998e-05 9.9896950e-01 9.3036284e-10 9.7555615e-04 0.0000000e+00\n",
      " 1.5755777e-14 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [1.2284398e-16 0.0000000e+00 9.9999994e-01 1.2284398e-16 3.4700388e-08\n",
      " 0.0000000e+00 1.2284398e-16 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [1.6348971e-02 0.0000000e+00 0.0000000e+00 2.9032055e-01 4.5789442e-05\n",
      " 4.6888389e-02 6.4639628e-01 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [1.901172e-17 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00\n",
      " 1.901172e-17 0.000000e+00 0.000000e+00 0.000000e+00] white = current\n",
      "Policy :  [6.1647646e-05 0.0000000e+00 0.0000000e+00 9.9993008e-01 0.0000000e+00\n",
      " 8.2873321e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [0. 0. 0. 0. 0. 1. 0. 0. 0.] white = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:05 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 5.3704273e-08 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 8.8817048e-16 0.0000000e+00 8.8817051e-06 9.9999106e-01] black = best\n",
      "Policy :  [3.7735055e-11 3.7735055e-11 3.7735055e-11 3.7735055e-11 2.1249935e-02\n",
      " 3.7735055e-11 3.7735055e-11 9.7875011e-01 0.0000000e+00] white = current\n",
      "Policy :  [2.5832554e-05 9.9665731e-01 0.0000000e+00 3.3166606e-03 0.0000000e+00\n",
      " 0.0000000e+00 2.3494572e-07 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [1.2284398e-16 0.0000000e+00 9.9999994e-01 1.2284398e-16 3.4700388e-08\n",
      " 0.0000000e+00 1.2284398e-16 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [1.6859854e-02 0.0000000e+00 0.0000000e+00 2.9939267e-01 2.9237565e-04\n",
      " 1.6859854e-02 6.6659528e-01 0.0000000e+00 0.0000000e+00] black = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      " 0.0000000e+00 3.0794613e-11 0.0000000e+00 0.0000000e+00] white = current\n",
      "Policy :  [4.5395661e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 4.0706221e-04 9.9505335e-01 0.0000000e+00 0.0000000e+00] black = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:06 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 0. 1. 0. 0. 0.] white = current\n",
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = best\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 1.5554787e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.0000000e+00 9.6311146e-09] black = current\n",
      "Policy :  [1.7099475e-07 1.6698706e-10 9.8604196e-06 1.5925128e-06 9.6293160e-09\n",
      " 1.7509863e-04 1.6698706e-10 0.0000000e+00 9.9981326e-01] white = best\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = current\n",
      "Policy :  [1.68016791e-01 0.00000000e+00 1.68016791e-01 1.68016791e-01\n",
      " 1.36115635e-02 4.81867462e-01 4.70573694e-04 0.00000000e+00\n",
      " 0.00000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 1. 0. 0.] black = current\n",
      "Policy :  [9.99348104e-01 0.00000000e+00 1.02333246e-07 6.48827350e-04\n",
      " 2.96003691e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00] white = best\n",
      "Policy :  [0.         0.         0.97623837 0.         0.02376161 0.\n",
      " 0.         0.         0.        ] black = current\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9999994e-01 7.9994706e-08\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:07 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 1.5554787e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.0000000e+00 9.6311146e-09] black = current\n",
      "Policy :  [1.7099475e-07 1.6698706e-10 9.8604196e-06 1.5925128e-06 9.6293160e-09\n",
      " 1.7509863e-04 1.6698706e-10 0.0000000e+00 9.9981326e-01] white = best\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = current\n",
      "Policy :  [1.68016791e-01 0.00000000e+00 1.68016791e-01 1.68016791e-01\n",
      " 1.36115635e-02 4.81867462e-01 4.70573694e-04 0.00000000e+00\n",
      " 0.00000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 0. 0. 1. 0. 0.] black = current\n",
      "Policy :  [9.9667501e-01 0.0000000e+00 1.4547478e-06 3.3167196e-03 6.7960354e-06\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.         0.         0.9982127  0.         0.00178736 0.\n",
      " 0.         0.         0.        ] black = current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:08 <ipython-input-20-615e5088c2dc>:8] starting game round...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9999994e-01 7.9994706e-08\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0. 0. 0. 0. 1. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Policy :  [0.0000000e+00 1.5554787e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.0000000e+00 9.6311146e-09] black = current\n",
      "Policy :  [1.7099475e-07 1.6698706e-10 9.8604196e-06 1.5925128e-06 9.6293160e-09\n",
      " 1.7509863e-04 1.6698706e-10 0.0000000e+00 9.9981326e-01] white = best\n",
      "Policy :  [0. 1. 0. 0. 0. 0. 0. 0. 0.] black = current\n",
      "Policy :  [0.1694153  0.         0.1694153  0.1694153  0.00293792 0.4858783\n",
      " 0.00293792 0.         0.        ] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6256260e-08 1.5139823e-07\n",
      " 9.8295391e-01 1.7045924e-02 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [9.0096555e-06 0.0000000e+00 0.0000000e+00 9.1354871e-01 8.5922778e-02\n",
      " 0.0000000e+00 5.1954214e-04 0.0000000e+00 0.0000000e+00] white = best\n",
      "Policy :  [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9468002e-14\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00] black = current\n",
      "Policy :  [3.325256e-07 0.000000e+00 0.000000e+00 0.000000e+00 9.999997e-01\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00] white = best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:09 <ipython-input-20-615e5088c2dc>:65] [CPU 0]: Finished arena games!\n",
      "[I 200412 12:40:09 <ipython-input-25-45b1ebfad181>:36] Trained net didn't perform better, generating more MCTS games for retraining...\n",
      "[I 200412 12:40:09 <ipython-input-18-8fbaffc5cd76>:9] Preparing model for MCTS...\n",
      "[I 200412 12:40:09 <ipython-input-18-8fbaffc5cd76>:17] Loaded ./model_data/tictactoe_iter8.pth.tar model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy :  [1. 0. 0. 0. 0. 0. 0. 0. 0.] black = current\n",
      "None wins!\n",
      "Current_net wins ratio 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:12 <ipython-input-18-8fbaffc5cd76>:24] Finished MCTS!\n",
      "[I 200412 12:40:12 <ipython-input-19-601145dc3411>:84] Loading training data...\n",
      "[I 200412 12:40:12 <ipython-input-19-601145dc3411>:92] Loaded data from ./datasets/iter_8/.\n",
      "[I 200412 12:40:12 <ipython-input-19-601145dc3411>:12] Starting training process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the train loader is1\n",
      "Update step size: 1\n",
      "2.6539390087127686\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 1,    32/ 23 points] total loss per batch: 2.654\n",
      "Policy (actual, predicted): 1 6\n",
      "Policy data: tensor([0.2245, 0.4286, 0.0000, 0.0000, 0.0204, 0.0204, 0.3061, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0839, 0.1200, 0.0721, 0.0716, 0.1134, 0.0718, 0.2079, 0.1720, 0.0874],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 -0.09754529595375061\n",
      " \n",
      "2.439531087875366\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 2,    32/ 23 points] total loss per batch: 2.440\n",
      "Policy (actual, predicted): 7 3\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.1828, 0.0817, 0.0348, 0.3660, 0.0202, 0.0758, 0.0524, 0.1564, 0.0299],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.3421141803264618\n",
      " \n",
      "2.8703229427337646\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 3,    32/ 23 points] total loss per batch: 2.870\n",
      "Policy (actual, predicted): 0 7\n",
      "Policy data: tensor([0.5918, 0.0612, 0.0000, 0.0000, 0.0000, 0.0000, 0.2245, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.0223, 0.2545, 0.0627, 0.0043, 0.0117, 0.0120, 0.0881, 0.4918, 0.0527],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.4814053773880005\n",
      " \n",
      "2.513172149658203\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 4,    32/ 23 points] total loss per batch: 2.513\n",
      "Policy (actual, predicted): 0 6\n",
      "Policy data: tensor([0.6531, 0.0816, 0.0000, 0.0000, 0.1429, 0.1224, 0.0000, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.1674, 0.0515, 0.1416, 0.0630, 0.0653, 0.1391, 0.1712, 0.1495, 0.0515],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.7155534029006958\n",
      " \n",
      "2.1369385719299316\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 5,    32/ 23 points] total loss per batch: 2.137\n",
      "Policy (actual, predicted): 2 7\n",
      "Policy data: tensor([0.0000, 0.1633, 0.3061, 0.1837, 0.0408, 0.0000, 0.1224, 0.1837, 0.0000])\n",
      "Policy pred: tensor([0.1719, 0.0394, 0.1119, 0.1041, 0.0543, 0.1126, 0.1222, 0.2343, 0.0492],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.6024692058563232\n",
      " \n",
      "2.0722241401672363\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 6,    32/ 23 points] total loss per batch: 2.072\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0408, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9592, 0.0000])\n",
      "Policy pred: tensor([0.1283, 0.0506, 0.0888, 0.1606, 0.0548, 0.0553, 0.1533, 0.2635, 0.0448],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.18629954755306244\n",
      " \n",
      "1.8966330289840698\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 7,    32/ 23 points] total loss per batch: 1.897\n",
      "Policy (actual, predicted): 7 3\n",
      "Policy data: tensor([0.2041, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.2857, 0.0000])\n",
      "Policy pred: tensor([0.1230, 0.0378, 0.1191, 0.2254, 0.0411, 0.0776, 0.1390, 0.2129, 0.0241],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.5272854566574097\n",
      " \n",
      "1.752975583076477\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 8,    32/ 23 points] total loss per batch: 1.753\n",
      "Policy (actual, predicted): 3 7\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.9796, 0.0000, 0.0000, 0.0000, 0.0204, 0.0000])\n",
      "Policy pred: tensor([0.0874, 0.1371, 0.0475, 0.1281, 0.0317, 0.0287, 0.0760, 0.3386, 0.1249],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.23178790509700775\n",
      " \n",
      "1.650148630142212\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 9,    32/ 23 points] total loss per batch: 1.650\n",
      "Policy (actual, predicted): 3 6\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.4082, 0.3673, 0.0000, 0.2245, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.0486, 0.0966, 0.0324, 0.2066, 0.2421, 0.0239, 0.2780, 0.0653, 0.0064],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.966724693775177\n",
      " \n",
      "1.5317175388336182\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 10,    32/ 23 points] total loss per batch: 1.532\n",
      "Policy (actual, predicted): 2 3\n",
      "Policy data: tensor([0.0000, 0.0408, 0.3878, 0.2449, 0.0408, 0.1633, 0.1224, 0.0000, 0.0000])\n",
      "Policy pred: tensor([0.2184, 0.0339, 0.1397, 0.2401, 0.0674, 0.1731, 0.0862, 0.0372, 0.0041],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9705665707588196\n",
      " \n",
      "1.438584327697754\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 11,    32/ 23 points] total loss per batch: 1.439\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0408, 0.2245, 0.0000, 0.0000, 0.0000, 0.0000, 0.7347, 0.0000])\n",
      "Policy pred: tensor([0.0131, 0.1245, 0.0508, 0.1932, 0.0089, 0.0044, 0.0714, 0.5190, 0.0147],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.0589083656668663\n",
      " \n",
      "1.3717994689941406\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 12,    32/ 23 points] total loss per batch: 1.372\n",
      "Policy (actual, predicted): 2 6\n",
      "Policy data: tensor([0.0000, 0.1633, 0.3061, 0.1837, 0.0408, 0.0000, 0.1224, 0.1837, 0.0000])\n",
      "Policy pred: tensor([0.0750, 0.0724, 0.1540, 0.1234, 0.0267, 0.0662, 0.2691, 0.2100, 0.0034],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.07579230517148972\n",
      " \n",
      "1.3251161575317383\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 13,    32/ 23 points] total loss per batch: 1.325\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0408, 0.2245, 0.0000, 0.0000, 0.0000, 0.0000, 0.7347, 0.0000])\n",
      "Policy pred: tensor([0.0051, 0.0921, 0.0756, 0.1511, 0.0035, 0.0025, 0.0336, 0.6336, 0.0029],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.09011846035718918\n",
      " \n",
      "1.3051297664642334\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 14,    32/ 23 points] total loss per batch: 1.305\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.4082, 0.3673, 0.0000, 0.2245, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.0561e-03, 6.9175e-03, 3.5595e-03, 4.8968e-01, 2.4092e-01, 1.3098e-02,\n",
      "        2.4287e-01, 9.0384e-04, 3.6929e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9980544447898865\n",
      " \n",
      "1.2762166261672974\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 15,    32/ 23 points] total loss per batch: 1.276\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.1776, 0.0665, 0.1245, 0.1202, 0.0278, 0.1103, 0.0741, 0.2964, 0.0027],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.6294375061988831\n",
      " \n",
      "1.235060691833496\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 16,    32/ 23 points] total loss per batch: 1.235\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.0408, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9592, 0.0000])\n",
      "Policy pred: tensor([8.4935e-05, 1.9662e-02, 1.7909e-02, 5.2518e-03, 9.4607e-06, 5.7524e-05,\n",
      "        6.1952e-04, 9.5617e-01, 2.3327e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.10760952532291412\n",
      " \n",
      "1.2161033153533936\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 17,    32/ 23 points] total loss per batch: 1.216\n",
      "Policy (actual, predicted): 1 6\n",
      "Policy data: tensor([0.0000, 0.4082, 0.0000, 0.2449, 0.0204, 0.0204, 0.3061, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.0151e-02, 2.9898e-01, 1.6412e-03, 1.6770e-01, 1.0180e-01, 2.9364e-02,\n",
      "        3.7787e-01, 2.4878e-03, 6.3979e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9995272755622864\n",
      " \n",
      "1.1958316564559937\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 18,    32/ 23 points] total loss per batch: 1.196\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.7143, 0.0000, 0.0000, 0.2857, 0.0000, 0.0000])\n",
      "Policy pred: tensor([2.8525e-04, 3.4275e-03, 2.1567e-03, 7.4770e-01, 6.8334e-02, 1.4691e-02,\n",
      "        1.6285e-01, 5.5218e-04, 1.1057e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9939837455749512\n",
      " \n",
      "1.1820658445358276\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 19,    32/ 23 points] total loss per batch: 1.182\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.5918, 0.0612, 0.0000, 0.0000, 0.0000, 0.0000, 0.2245, 0.1224, 0.0000])\n",
      "Policy pred: tensor([0.4959, 0.1438, 0.0157, 0.0695, 0.0094, 0.0268, 0.0721, 0.1433, 0.0236],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.024274950847029686\n",
      " \n",
      "1.1765568256378174\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 20,    32/ 23 points] total loss per batch: 1.177\n",
      "Policy (actual, predicted): 1 6\n",
      "Policy data: tensor([0.0000, 0.4082, 0.0000, 0.2449, 0.0204, 0.0204, 0.3061, 0.0000, 0.0000])\n",
      "Policy pred: tensor([6.6467e-03, 2.2835e-01, 5.4447e-04, 1.9731e-01, 5.8779e-02, 1.0140e-02,\n",
      "        4.9665e-01, 1.5827e-03, 2.6909e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9997559189796448\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1475825309753418\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 21,    32/ 23 points] total loss per batch: 1.148\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.0000, 0.2245, 0.0000, 0.2653, 0.0000, 0.0204, 0.0000, 0.4898, 0.0000])\n",
      "Policy pred: tensor([0.0034, 0.1819, 0.0274, 0.2996, 0.0026, 0.0277, 0.0174, 0.4362, 0.0037],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9621239900588989\n",
      " \n",
      "1.1561479568481445\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 22,    32/ 23 points] total loss per batch: 1.156\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.2130, 0.0312, 0.0554, 0.1592, 0.0421, 0.1267, 0.0556, 0.3159, 0.0010],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.7592387199401855\n",
      " \n",
      "1.135990858078003\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 23,    32/ 23 points] total loss per batch: 1.136\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.2144, 0.0266, 0.0523, 0.1586, 0.0442, 0.1456, 0.0567, 0.3004, 0.0012],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.7471541166305542\n",
      " \n",
      "1.1286710500717163\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 24,    32/ 23 points] total loss per batch: 1.129\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.1952, 0.0230, 0.0483, 0.1955, 0.0447, 0.1247, 0.0507, 0.3164, 0.0015],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 0.6259851455688477\n",
      " \n",
      "1.1251899003982544\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 25,    32/ 23 points] total loss per batch: 1.125\n",
      "Policy (actual, predicted): 2 2\n",
      "Policy data: tensor([0.0000, 0.1633, 0.3061, 0.1837, 0.0408, 0.0000, 0.1224, 0.1837, 0.0000])\n",
      "Policy pred: tensor([2.8579e-02, 1.2394e-01, 3.2156e-01, 1.6219e-01, 1.2973e-02, 8.7185e-03,\n",
      "        1.3009e-01, 2.1180e-01, 1.3265e-04], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 0.0 -0.005806261673569679\n",
      " \n",
      "1.1243020296096802\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 26,    32/ 23 points] total loss per batch: 1.124\n",
      "Policy (actual, predicted): 3 3\n",
      "Policy data: tensor([0.0000, 0.0000, 0.0000, 0.3878, 0.2449, 0.2245, 0.1429, 0.0000, 0.0000])\n",
      "Policy pred: tensor([5.3790e-04, 6.9503e-03, 2.5851e-03, 3.1602e-01, 2.2918e-01, 2.5842e-01,\n",
      "        1.8608e-01, 2.2423e-04, 5.1159e-07], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9990645051002502\n",
      " \n",
      "1.119624137878418\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 27,    32/ 23 points] total loss per batch: 1.120\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1020, 0.3878, 0.0816, 0.0000, 0.0000, 0.0204, 0.0000, 0.4082, 0.0000])\n",
      "Policy pred: tensor([1.2960e-01, 3.5568e-01, 7.0474e-02, 8.7927e-03, 3.6608e-04, 2.5652e-02,\n",
      "        1.5574e-02, 3.8902e-01, 4.8370e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9835134148597717\n",
      " \n",
      "1.121759057044983\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 28,    32/ 23 points] total loss per batch: 1.122\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1837, 0.0204, 0.0612, 0.1837, 0.0408, 0.1429, 0.0612, 0.3061, 0.0000])\n",
      "Policy pred: tensor([0.1798, 0.0276, 0.0595, 0.1707, 0.0407, 0.1103, 0.0777, 0.3323, 0.0015],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.645458459854126\n",
      " \n",
      "1.1410658359527588\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 29,    32/ 23 points] total loss per batch: 1.141\n",
      "Policy (actual, predicted): 7 7\n",
      "Policy data: tensor([0.1020, 0.3878, 0.0816, 0.0000, 0.0000, 0.0204, 0.0000, 0.4082, 0.0000])\n",
      "Policy pred: tensor([1.2073e-01, 3.6119e-01, 4.6839e-02, 5.6537e-03, 2.0970e-04, 2.8849e-02,\n",
      "        1.3000e-02, 4.1777e-01, 5.7619e-03], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9853368997573853\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:14 <ipython-input-19-601145dc3411>:65] Finished Training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1772570610046387\n",
      "Update step size: 1\n",
      "[Iteration 8] Process ID: 11022 [Epoch: 30,    32/ 23 points] total loss per batch: 1.177\n",
      "Policy (actual, predicted): 0 0\n",
      "Policy data: tensor([0.3673, 0.0408, 0.2857, 0.0000, 0.0612, 0.1633, 0.0816, 0.0000, 0.0000])\n",
      "Policy pred: tensor([4.0164e-01, 4.7254e-02, 2.4292e-01, 3.2748e-03, 6.6163e-02, 1.4805e-01,\n",
      "        8.7876e-02, 2.8152e-03, 6.2203e-06], grad_fn=<SelectBackward>)\n",
      "Value (actual, predicted): 1.0 0.9929296970367432\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200412 12:40:14 <ipython-input-20-615e5088c2dc>:71] Loading nets...\n",
      "[I 200412 12:40:14 <ipython-input-20-615e5088c2dc>:77] Current net: tictactoe_iter9.pth.tar\n",
      "[I 200412 12:40:14 <ipython-input-20-615e5088c2dc>:78] Previous (Best net: tictactoe_iter8.pth.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "\n",
    "args = {\"iteration\":0,\n",
    "       \"total_iterations\":100,\n",
    "       \"MCTS_num_processes\":1,\n",
    "       \"num_games_per_MCTS_process\":50,\n",
    "       \"temperature_MCTS\":1.1,\n",
    "       \"num_evaluator_games\":10,\n",
    "       \"neural_net_name\":\"tictactoe\",\n",
    "       \"batch_size\":32,\n",
    "       \"num_epochs\":30,\n",
    "       \"lr\":0.01,\n",
    "       \"gradient_acc_steps\":1.,\n",
    "       \"max_norm\":1.}\n",
    "\n",
    "class Dict2Obj(object):\n",
    "    \"\"\"\n",
    "    Turns a dictionary into a class\n",
    "    \"\"\"\n",
    "    #----------------------------------------------------------------------\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])\n",
    "\n",
    "args = Dict2Obj(args)\n",
    "\n",
    "for i in range(args.iteration, args.total_iterations):\n",
    "    run_MCTS(args, 0, i)\n",
    "    train_connectnet(args, iteration=i, new_optim_state=True)\n",
    "    if i >= 1:\n",
    "        winner = evaluate_nets(args, iteration_1=i, iteration_2=i + 1)\n",
    "        counts = 0\n",
    "        while (winner != (i+1)):\n",
    "            logger.info(\"Trained net didn't perform better, generating more MCTS games for retraining...\")\n",
    "            run_MCTS(args, start_idx=(counts + 1)*args.num_games_per_MCTS_process, iteration=i)\n",
    "            counts += 1\n",
    "            train_connectnet(args, iteration=i, new_optim_state=True)\n",
    "            winner = evaluate_nets(args,iteration_1=i, iteration_2=i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser la loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_results(iteration):\n",
    "    losses_path = \"./model_data/losses_per_epoch_iter%d.pkl\" % iteration\n",
    "    if os.path.exists(losses_path):\n",
    "        losses_per_epoch = load_pickle(\"losses_per_epoch_iter%d.pkl\" % iteration)\n",
    "        logger.info(\"Loaded results buffer\")\n",
    "    else:\n",
    "        losses_per_epoch = []\n",
    "    return losses_per_epoch\n",
    "\n",
    "losses_per_epoch = load_results(1 + 1)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter([e for e in range(0, (len(losses_per_epoch) ))], )\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss per batch')\n",
    "\n",
    "ax.set_title('loss vs Epoch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jouer contre l'ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "\n",
    "def play_game(net):\n",
    "    # Asks human what he/she wanna play as\n",
    "    white = None;\n",
    "    black = None\n",
    "    while (True):\n",
    "        play_as = input(\"What do you wanna play as? (\\\"O\\\"/\\\"X\\\")? Note: \\\"O\\\" starts first, \\\"X\\\" starts second\\n\")\n",
    "        if play_as == \"O\":\n",
    "            black = net;\n",
    "            break\n",
    "        elif play_as == \"X\":\n",
    "            white = net;\n",
    "            break\n",
    "        else:\n",
    "            print(\"I didn't get that.\")\n",
    "    current_board = Morpion_board()\n",
    "    checkmate = False\n",
    "    dataset = []\n",
    "    value = 0;\n",
    "    t = 0.1;\n",
    "    moves_count = 0\n",
    "    while checkmate == False and current_board.actions() != []:\n",
    "        if moves_count <= 5:\n",
    "            t = 1\n",
    "        else:\n",
    "            t = 0.1\n",
    "        moves_count += 1\n",
    "        dataset.append(copy.deepcopy(encode_board(current_board)))\n",
    "        print(current_board.board);\n",
    "        print(\" \")\n",
    "        if current_board.player == 0:\n",
    "            if white != None:\n",
    "                print(\"AI is thinking........\")\n",
    "                root = UCT_search(current_board, 777, white, t)\n",
    "                policy = get_policy(root, t)\n",
    "                current_board = do_decode_n_move_pieces(current_board, \\\n",
    "                                                        int(np.random.choice(np.array(range(9)), \\\n",
    "                                                                             p=policy)))\n",
    "            else:\n",
    "                while (True):\n",
    "                    row, col = ast.literal_eval(\n",
    "                        input(\"Which tuple do you wanna drop your piece? (Enter row, column as [(1,3),(1,3)])\\n\"))\n",
    "                    current_board.draw_sign([int(row) - 1, int(col) - 1])\n",
    "                    break\n",
    "        elif current_board.player == 1:\n",
    "            if black != None:\n",
    "                print(\"AI is thinking.............\")\n",
    "                root = UCT_search(current_board, 333, black, t)\n",
    "                policy = get_policy(root, t)\n",
    "                current_board = do_decode_n_move_pieces(current_board, \\\n",
    "                                                        int(np.random.choice(np.array(range(9)), \\\n",
    "                                                                             p=policy)))\n",
    "            else:\n",
    "                while (True):\n",
    "                    row, col = ast.literal_eval(\n",
    "                        input(\"Which tuple do you wanna drop your piece? (Enter row, column as [(1,3),(1,3)])\\n\"))\n",
    "                    current_board.draw_sign([int(row) - 1, int(col) - 1])\n",
    "                    break\n",
    "        # decode move and move piece(s)\n",
    "        if current_board.check_winner() == True:  # someone wins\n",
    "            if current_board.player == 0:  # black wins\n",
    "                value = -1\n",
    "            elif current_board.player == 1:  # white wins\n",
    "                value = 1\n",
    "            checkmate = True\n",
    "    dataset.append(encode_board(current_board))\n",
    "    print(current_board.board);\n",
    "    print(\" \")\n",
    "    if value == 1:\n",
    "        if play_as == \"O\":\n",
    "            dataset.append(f\"AI as black wins\");\n",
    "            print(\"YOU LOSE!!!!!!!\")\n",
    "        else:\n",
    "            dataset.append(f\"Human as black wins\");\n",
    "            print(\"YOU WIN!!!!!!!\")\n",
    "        return \"black\", dataset\n",
    "    elif value == -1:\n",
    "        if play_as == \"O\":\n",
    "            dataset.append(f\"Human as white wins\");\n",
    "            print(\"YOU WIN!!!!!!!!!!!\")\n",
    "        else:\n",
    "            dataset.append(f\"AI as white wins\");\n",
    "            print(\"YOU LOSE!!!!!!!\")\n",
    "        return \"white\", dataset\n",
    "    else:\n",
    "        dataset.append(\"Nobody wins\");\n",
    "        print(\"DRAW!!!!!\")\n",
    "        return None, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = \"tictactoe_iter5.pth.tar\"\n",
    "best_net_filename = os.path.join(\"./model_data/\", \\\n",
    "                                 best_net)\n",
    "best_cnet = ConnectNet()\n",
    "# cuda = torch.cuda.is_available()\n",
    "# if cuda:\n",
    "#     best_cnet.cuda()\n",
    "best_cnet.eval()\n",
    "checkpoint = torch.load(best_net_filename)\n",
    "best_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "play_again = True\n",
    "while (play_again == True):\n",
    "    play_game(best_cnet)\n",
    "    while (True):\n",
    "        again = input(\"Do you wanna play again? (Y/N)\\n\")\n",
    "        if again.lower() in [\"y\", \"n\"]:\n",
    "            if again.lower() == \"n\":\n",
    "                play_again = False;\n",
    "                break\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
